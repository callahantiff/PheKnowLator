{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<img width='700' src=\"https://user-images.githubusercontent.com/8030363/108961534-b9a66980-7634-11eb-96e2-cc46589dcb8c.png\" style=\"vertical-align:middle\">\n",
    "\n",
    "## Pre-Knowledge Graph Build Data Preparation\n",
    "***\n",
    "\n",
    "**Author:** [TJCallahan](https://mail.google.com/mail/u/0/?view=cm&fs=1&tf=1&to=callahantiff@gmail.com)  \n",
    "**GitHub Repository:** [PheKnowLator](https://github.com/callahantiff/PheKnowLator/wiki)  \n",
    "**Release:** **[`v4.0.0`](https://github.com/callahantiff/PheKnowLator/wiki/v4.0.0)**\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "**Purpose:** This notebook serves as a script to download and process data in order to generate mapping and filtering data needed to build edges for the PheKnowLator knowledge graph. For more information on the data sources utilize within this script, please see the [Data Sources](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources) Wiki page.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Assumptions:**   \n",
    "- Raw data downloads ➞ `./resources/processed_data/unprocessed_data`    \n",
    "- Processed data write location ➞ `./resources/processed_data`  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dependencies:**   \n",
    "- **Scripts**: This notebook utilizes several helper functions, which are stored in the [`data_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/data_utils.py) and [`kg_utils.py`](https://github.com/callahantiff/PheKnowLator/blob/master/pkt_kg/utils/kg_utils.py) scripts.  \n",
    "- **Data**: Hyperlinks to all downloaded and generated data sources are provided through [this](https://console.cloud.google.com/storage/browser/pheknowlator/release_v4.0.0?project=pheknowlator) dedicated Google Cloud Storage Bucket. <u>This notebook will download everything that is needed for you</u>.  \n",
    "_____\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "***\n",
    "\n",
    "### [Identifier Maps ](#create-identifier-maps)  \n",
    "- [HUMAN TRANSCRIPT, GENE, AND PROTEIN IDENTIFIER MAPPING](#human-transcript,-gene,-and-protein-identifier-mapping)\n",
    "  - [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "  - [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "  - [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "  - [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "  - [STRING-Protein Ontology](#string-proteinontology)  \n",
    "  - [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)  \n",
    "  - [Uniprot Accession-Entrez Gene](#uniprotaccession-entrezgene)\n",
    "  \n",
    "\n",
    "- [OTHER IDENTIFIER MAPPING](#other-identifier-mapping) \n",
    "  - [ChEBI Identifiers](#mesh-chebi) \n",
    "  - [Human Disease and Phenotype Identifiers](#disease-identifiers)\n",
    "  - [Human Protein Atlas Tissue and Cell Types](#hpa-uberon)  \n",
    "  - [Reactome Pathways - Pathway Ontology](#reactome-pw)  \n",
    "  - [Genomic Identifiers - Sequence Ontology](#genomic-soo)  \n",
    "\n",
    "\n",
    "### [Edge Datasets](#create-edge-datasets)\n",
    "- [ONTOLOGIES](#ontologies)  \n",
    "  - [Protein Ontology](#protein-ontology)  \n",
    "  - [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "\n",
    "- [LINKED DATA](#linked-data)  \n",
    "  - [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant)\n",
    "  - [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "\n",
    "### [Node and Relation Metadata](#node-relation-metadata)  \n",
    "- [CTD_chem_gene_ixns.tsv](#chemical-gene)  \n",
    "- [CTD_chem_go_enriched.tsv](#chemical-go)    \n",
    "- [CTD_chemicals_diseases.tsv](#chemical-disease)  \n",
    "- [CTD_genes_pathways.tsv](#gene-pathway)     \n",
    "- [goa_human.gaf](#goa)     \n",
    "- [COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt](#gene-gene)    \n",
    "- [phenotype.hpoa](#phenotype-disease) \n",
    "- [ChEBI2Reactome_All_Levels.txt](#chemical-pathway)   \n",
    "- [gene_association.reactome](#reactome-goa)    \n",
    "- [UniProt2Reactome_All_Levels.txt](#uniprot-react)     \n",
    "- [CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt](#variant-disease)    \n",
    "- [CLINVAR_VARIANT_GENE_EDGES.txt](#variant-gene)   \n",
    "- [HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt](#hpa) \n",
    "- [UNIPROT_PROTEIN_CATALYST.txt](#uniprot-catalyst)  \n",
    "- [UNIPROT_PROTEIN_COFACTOR.txt](#uniprot-cofactor)  \n",
    "- [9606.protein.links.v11.0.txt.gz](#protein-protein)  \n",
    "- [curated_gene_disease_associations.tsv](#gene-phen)  \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up Environment\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment and run to install any required modules from notebooks/requirements.txt\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if running a local version of pkt_kg, uncomment the code below\n",
    "# import sys\n",
    "# sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import datetime\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import networkx\n",
    "import numpy\n",
    "import os\n",
    "import openpyxl\n",
    "import pandas\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from rdflib import Graph, Namespace, URIRef, BNode, Literal\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from reactome2py import content\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "from pkt_kg.utils import *  # import pkt_kg utility script containing helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to use for processing data\n",
    "unprocessed_data_location = '../resources/processed_data/unprocessed_data/'\n",
    "processed_data_location = '../resources/processed_data/'\n",
    "\n",
    "# directory to write relations data to\n",
    "relations_data_location = '../resources/relations_data/'\n",
    "\n",
    "# directory to write node metadata to\n",
    "metadata_location = '../resources/metadata/'\n",
    "\n",
    "# directory to write kg construction approach dictionary to\n",
    "construction_approach_location = '../resources/construction_approach/'\n",
    "\n",
    "# directory to write ontology data to\n",
    "ontology_data_location = '../resources/ontologies/'\n",
    "\n",
    "# owltools location\n",
    "owltools_location = '../pkt_kg/libs/owltools'\n",
    "\n",
    "# obo spacespace\n",
    "obo = Namespace('http://purl.obolibrary.org/obo/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### CREATE MAPPING DATASETS  <a class=\"anchor\" id=\"create-identifier-maps\"></a>\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Transcript, Gene, and Protein Identifier Mapping  <a class=\"anchor\" id=\"human-transcript,-gene,-and-protein-identifier-mapping\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Pages:**   \n",
    "- [Ensembl](https://uswest.ensembl.org/)  \n",
    "- [Uniprot Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#universal-protein-resource-knowledgebase)  \n",
    "- [HGNC](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt) \n",
    "- [NCBI Gene](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#national-center-for-biotechnology-information-gene) \n",
    "- [Protein Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources/#protein-ontology)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** To map create `protein-coding gene`-`protein` edges and mappings between the identifiers types listed below. The edges types produced from each of these mappings will be further described within each of the subsequent identifier mapping sections:  \n",
    "- [Entrez Gene-Ensembl Transcript](#entrezgene-ensembltranscript)  \n",
    "- [Entrez Gene-Protein Ontology](#entrezgene-proteinontology)  \n",
    "- [Ensembl Gene-Entrez Gene](#ensemblgene-entrezgene)\n",
    "- [Gene Symbol-Ensembl Transcript](#genesymbol-ensembltranscript)  \n",
    "- [STRING-Protein Ontology](#string-proteinontology)  \n",
    "- [Uniprot Accession-Protein Ontology](#uniprotaccession-proteinontology)\n",
    "- [Uniprot Accession-Entrez Gene](#uniprotaccession-entrezgene)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Gene and Transcript Types:** The transcript and gene/locus types were reviewed by a PhD Molecular biologist to confirm whether or not they should be classified as `protein-coding` or not, which is useful for creating `genomic`-`rna`, `genomic`-`protein`, and `rna`-`protein` edges in the knowledge graph. For more information on this classification, please see the table below. Definitions of concepts in the table have been taken from [HGNC](https://www.genenames.org/help/symbol-report/), [Ensembl](https://uswest.ensembl.org/info/genome/genebuild/biotypes.html), [NCBI](https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/objects/entrezgene/entrezgene.asn), and Wikipedia.\n",
    "\n",
    "<table>\n",
    "<th align=\"center\">Gene and Transcript Type</th>  \n",
    "<th align=\"center\">Definition</th>\n",
    "<th align=\"center\">Type</th>\n",
    "<th align=\"center\">Genomic material <i>transcribed_to</i> RNA</th>\n",
    "<th align=\"center\">RNA <i>translated_to</i> Protein</th>\n",
    "<th align=\"center\">Genomic material <i>has_gene_product</i> Protein</th>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">biological-region</td> \n",
    "  <td rowspan=\"2\">Biological_region (SO:0001411); Special note: This is a parental feature spanning all other feature annotation on each RefSeq Functional Element record. It is a 'misc_feature' in GenBank flat files but a 'Region' feature in ASN.1 and GFF3 formats</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_C_gene</td> \n",
    "  <td rowspan=\"2\">Constant chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_C_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\t \t \n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_D_gene</td> \n",
    "  <td rowspan=\"2\">Diversity chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_J_gene</td> \n",
    "  <td rowspan=\"2\">IG J gene: Joining chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_J_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_V_gene</td> \n",
    "  <td rowspan=\"2\">Variable chain immunoglobulin gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">IG_V_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">lncRNA</td> \n",
    "  <td rowspan=\"2\">RNA, long non-coding - non-protein coding genes that encode long non-coding RNAs (lncRNAs) (SO:0001877); these are at least 200 nt in length. Subtypes include intergenic (SO:0001463), intronic (SO:0001903) and antisense (SO:0001904)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">miRNA</td> \n",
    "  <td rowspan=\"2\">RNA, micro - non-protein coding genes that encode microRNAs (miRNAs) (SO:0001265)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">misc_RNA</td> \n",
    "  <td rowspan=\"2\">Non-protein coding genes that encode miscellaneous types of small ncRNAs, such as vault (SO:0000404) and Y (SO:0000405) RNA genes</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">Mt_rRNA</td> \n",
    "  <td rowspan=\"2\">Mitochondrial rRNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">Mt_tRNA</td> \n",
    "  <td rowspan=\"2\">Mitochondrial tRNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">ncRNA</td> \n",
    "  <td rowspan=\"2\">Noncoding RNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">non_stop_decay</td> \n",
    "  <td rowspan=\"2\">Transcripts that have polyA features (including signal) without a prior stop codon in the CDS, i.e. a non-genomic polyA tail attached directly to the CDS without 3' UTR. These transcripts are subject to degradation</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">nonsense_mediated_decay</td> \n",
    "  <td rowspan=\"2\">If the coding sequence (following the appropriate reference) of a transcript finishes >50bp from a downstream splice site then it is tagged as NMD. If the variant does not cover the full reference coding sequence then it is annotated as NMD if NMD is unavoidable i.e. no matter what the exon structure of the missing portion is the transcript will be subject to NMD</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">other</td> \n",
    "  <td rowspan=\"2\">other</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">phenotype</td> \n",
    "  <td rowspan=\"2\"> Mapped phenotypes where the causative gene has not been identified (SO:0001500) </td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">polymorphic_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene owing to a SNP/DIP but in other individuals/haplotypes/strains the gene is translated</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene that lack introns and is thought to arise from reverse transcription of mRNA followed by reinsertion of DNA into the genome</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">processed_transcript</td> \n",
    "  <td rowspan=\"2\">Gene/transcript that doesn't contain an open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">protein_coding</td> \n",
    "  <td rowspan=\"2\">Contains an open reading frame (ORF)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>yes</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">pseudogene</td> \n",
    "  <td rowspan=\"2\">Have homology to proteins but generally suffer from a disrupted coding sequence and an active homologous gene can be found at another locus. Sometimes these entries have an intact coding sequence or an open but truncated open reading frame, in which case there is other evidence used (for example genomic polyA stretches at the 3' end) to classify them as a pseudogene</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">retained_intron</td> \n",
    "  <td rowspan=\"2\">Has an alternatively spliced transcript believed to contain intronic sequence relative to other, coding, variants</td>\n",
    "  <td>gene</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">ribozyme</td> \n",
    "  <td rowspan=\"2\">Ribozymes are RNA molecules that have the ability to catalyze specific biochemical reactions, including RNA splicing in gene expression, similar to the action of protein enzymes</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">rRNA</td> \n",
    "  <td rowspan=\"2\">RNA, ribosomal - non-protein coding genes that encode ribosomal RNAs (rRNAs) (SO:0001637)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">rRNA_pseudogene</td> \n",
    "  <td rowspan=\"2\">A gene that has homology to known protein-coding genes but contain a frameshift and/or stop codon(s) which disrupts the open reading frame. Thought to have arisen through duplication followed by loss of function</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">scaRNA</td> \n",
    "  <td rowspan=\"2\">Small Cajal body-specific RNAs are a class of small nucleolar RNAs that specifically localize to the Cajal body, a nuclear organelle involved in the biogenesis of small nuclear ribonucleoproteins/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">scRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small cytoplasmic - non-protein coding genes that encode small cytoplasmic RNAs (scRNAs) (SO:0001266)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">snoRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small nucleolar - non-protein coding genes that encode small nucleolar RNAs (snoRNAs) containing C/D or H/ACA box domains (SO:0001267)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">snRNA</td> \n",
    "  <td rowspan=\"2\">RNA, small nuclear - non-protein coding genes that encode small nuclear RNAs (snRNAs) (SO:0001268)</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">sRNA</td> \n",
    "  <td rowspan=\"2\">Bacterial small RNAs (sRNA) are small RNAs produced by bacteria; they are 50- to 500-nucleotide non-coding RNA molecules, highly structured and containing several stem-loops</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TEC</td> \n",
    "  <td rowspan=\"2\">TEC (To be Experimentally Confirmed). This is used for non-spliced EST clusters that have polyA features. This category has been specifically created for the ENCODE project to highlight regions that could indicate the presence of protein coding genes that require experimental validation, either by 5' RACE or RT-PCR to extend the transcripts, or by confirming expression of the putatively-encoded peptide with specific antibodies</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>yes</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_C_gene</td> \n",
    "  <td rowspan=\"2\">Constant chain T cell receptor gene that undergoes somatic recombination before transcription/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_D_gene</td> \n",
    "  <td rowspan=\"2\">Diversity chain T cell receptor gene that undergoes somatic recombination before transcription/td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_J_gene</td> \n",
    "  <td rowspan=\"2\">Joining chain T cell receptor gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_J_pseudogene</td> \n",
    "  <td rowspan=\"2\">T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_V_gene</td> \n",
    "  <td rowspan=\"2\">Variable chain T cell receptor gene that undergoes somatic recombination before transcription</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">TR_V_pseudogene</td> \n",
    "  <td rowspan=\"2\">T cell receptor pseudogene - T cell receptor gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_unitary_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">transcribed_unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene where protein homology or genomic structure indicates a pseudogene, but the presence of locus-specific transcripts indicates expression. These can be classified into 'Processed', 'Unprocessed' and 'Unitary'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">translated_processed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogenes that have mass spec data suggesting that they are also translated. These can be classified into 'Processed', 'Unprocessed'</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">translated_unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Inactivated immunoglobulin gene. Immunoglobulin gene segments that are inactivated due to frameshift mutations and/or stop codons in the open reading frame</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">tRNA</td> \n",
    "  <td rowspan=\"2\">Transfer RNA</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unitary_pseudogene</td> \n",
    "  <td rowspan=\"2\">A species specific unprocessed pseudogene without a parent gene, as it has an active orthologue in another species</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unknown</td> \n",
    "  <td rowspan=\"2\">Entries where the locus type is currently unknown</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "  <td> --- </td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\">unprocessed_pseudogene</td> \n",
    "  <td rowspan=\"2\">Pseudogene that can contain introns since produced by gene duplication</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "<tr>\n",
    "  <td rowspan=\"2\" align=\"center\">vaultRNA</td> \n",
    "  <td rowspan=\"2\" align=\"center\">Short non coding RNA genes that form part of the vault ribonucleoprotein complex</td>\n",
    "  <td>gene</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td>\n",
    "</tr>\n",
    "  <td>transcript</td> \n",
    "  <td>yes</td> \n",
    "  <td>no</td> \n",
    "  <td>no</td> \n",
    "</tr>\n",
    "</table> \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:** This script downloads and saves the following data:  \n",
    "- Human Ensembl Gene Set ➞ `Homo_sapiens.GRCh38.<<release>>.gtf`\n",
    "- Human Ensembl-UniProt Identifiers ➞ `Homo_sapiens.GRCh38.<<release>>.uniprot.tsv` \n",
    "- Human Ensembl-Entrez Identifiers ➞ `Homo_sapiens.GRCh38.<<release>>.entrez.tsv` \n",
    "- Human Gene Identifiers ➞ [`Homo_sapiens.gene_info`](ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz), [`hgnc_complete_set.txt`](ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt)  \n",
    "- Human Protein Identifiers ➞ [`promapping.txt`](https://proconsortium.org/download/current/promapping.txt)  \n",
    "- UniProt Identifiers ➞ [`uniprot_identifier_mapping.tab`](https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Cdatabase(GeneID)%2Cdatabase(Ensembl)%2Cdatabase(HGNC)%2Cgenes(PREFERRED)%2Cgenes(ALTERNATIVE))\n",
    "\n",
    "*All Merged Data Sets:*  \n",
    "- `Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt` \n",
    "- `Merged_gene_rna_protein_identifiers.pkl`  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genomic Typing Dictionary**  \n",
    "Read in the  `genomic_typing_dict.pkl` dictionary, which is needed in order to preprocess the genomic identifier datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/curated_data/genomic_typing_dict.pkl'\n",
    "if not os.path.exists(unprocessed_data_location + 'genomic_typing_dict.pkl'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "genomic_type_mapper = pickle.load(open(unprocessed_data_location + 'genomic_typing_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HGNC Data** \n",
    "\n",
    "_Human Gene Set Data_ - `hgnc_complete_set.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'hgnc_complete_set.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "hgnc = pandas.read_csv(unprocessed_data_location + 'hgnc_complete_set.txt', header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be protein-coding, other or ncRNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc = hgnc.loc[hgnc['status'].apply(lambda x: x == 'Approved')]\n",
    "hgnc = hgnc[['hgnc_id', 'entrez_id', 'ensembl_gene_id', 'uniprot_ids', 'symbol', 'locus_type', 'alias_symbol', 'name', 'location', 'alias_name']]\n",
    "hgnc.rename(columns={'uniprot_ids': 'uniprot_id', 'location': 'map_location', 'locus_type': 'hgnc_gene_type'}, inplace=True)\n",
    "hgnc['hgnc_id'] = hgnc['hgnc_id'].str.replace('.*\\:', '', regex=True)  # strip 'HGNC' off of the identifiers\n",
    "hgnc.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "hgnc['entrez_id'] = hgnc['entrez_id'].apply(lambda x: str(int(x)) if x != 'None' else 'None')  # make col str\n",
    "\n",
    "# combine certain columns into single column\n",
    "hgnc['name'] = hgnc['name'] + '|' + hgnc['alias_name']\n",
    "hgnc['synonyms'] = hgnc['alias_symbol'] + '|' + hgnc['alias_name'] + '|' + hgnc['name']\n",
    "hgnc['symbol'] = hgnc['symbol'] + '|' + hgnc['alias_symbol']\n",
    "\n",
    "# explode nested data and reformat values in preparation for combining it with other gene identifiers\n",
    "explode_df_hgnc = explodes_data(hgnc.copy(), ['ensembl_gene_id', 'uniprot_id', 'symbol', 'name', 'synonyms'], '|')\n",
    "\n",
    "# reformat hgnc gene type\n",
    "for val in genomic_type_mapper['hgnc_gene_type'].keys():\n",
    "    explode_df_hgnc['hgnc_gene_type'] = explode_df_hgnc['hgnc_gene_type'].str.replace(val, genomic_type_mapper['hgnc_gene_type'][val])\n",
    "\n",
    "# reformat master hgnc gene type\n",
    "explode_df_hgnc['master_gene_type'] = explode_df_hgnc['hgnc_gene_type']\n",
    "master_dict = genomic_type_mapper['hgnc_master_gene_type']\n",
    "for val in master_dict.keys():\n",
    "    explode_df_hgnc['master_gene_type'] = explode_df_hgnc['master_gene_type'].str.replace(val, master_dict[val])\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_hgnc.drop(['alias_symbol', 'alias_name'], axis=1, inplace=True)  # remove original gene type column\n",
    "explode_df_hgnc.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_hgnc.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Ensembl Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Human Gene Set Data_ - `Homo_sapiens.GRCh38.102.gtf.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ensembl.org/pub/release-102/gtf/homo_sapiens/Homo_sapiens.GRCh38.102.gtf.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.gtf'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ensembl_geneset = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.gtf',\n",
    "                                  header = None, delimiter='\\t', skiprows=5, usecols=[8], low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be reformatted in order for it to be able to be merged with the other gene, RNA, and protein identifier data. To do this, we iterate over each row of the data and extract the fields shown below in `column_names`, making each of these extracted fields their own column. The final step is to update the gene_type variable such that each of the variable values is re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_data = list(ensembl_geneset[8]); ensembl_df_data = []\n",
    "for i in tqdm(range(0, len(ensembl_data))):\n",
    "    if 'gene_id' in ensembl_data[i] and 'transcript_id' in ensembl_data[i]:\n",
    "        row_dict = {x.split(' \"')[0].lstrip(): x.split(' \"')[1].strip('\"') for x in ensembl_data[i].split(';')[0:-1]}\n",
    "        ensembl_df_data += [(row_dict['gene_id'], row_dict['transcript_id'], row_dict['gene_name'],\n",
    "                           row_dict['gene_biotype'], row_dict['transcript_name'], row_dict['transcript_biotype'])]\n",
    "# convert to data frame\n",
    "ensembl_geneset = pandas.DataFrame(ensembl_df_data,\n",
    "                                   columns=['ensembl_gene_id', 'transcript_stable_id', 'symbol',\n",
    "                                            'ensembl_gene_type', 'transcript_name', 'ensembl_transcript_type'])\n",
    "\n",
    "# reformat ensembl gene type\n",
    "gene_dict = genomic_type_mapper['ensembl_gene_type']\n",
    "for val in gene_dict.keys():\n",
    "    ensembl_geneset['ensembl_gene_type'] = ensembl_geneset['ensembl_gene_type'].str.replace(val, gene_dict[val])\n",
    "# reformat master gene type\n",
    "ensembl_geneset['master_gene_type'] = ensembl_geneset['ensembl_gene_type']\n",
    "gene_dict = genomic_type_mapper['ensembl_master_gene_type']\n",
    "for val in gene_dict.keys():\n",
    "    ensembl_geneset['master_gene_type'] = ensembl_geneset['master_gene_type'].str.replace(val, gene_dict[val])\n",
    "# reformat master transcript type\n",
    "ensembl_geneset['ensembl_transcript_type'] = ensembl_geneset['ensembl_transcript_type'].str.replace('vault_RNA', 'vaultRNA', regex=False)\n",
    "ensembl_geneset['master_transcript_type'] = ensembl_geneset['ensembl_transcript_type']\n",
    "trans_dict = genomic_type_mapper['ensembl_master_transcript_type']\n",
    "for val in trans_dict.keys():\n",
    "    ensembl_geneset['master_transcript_type'] = ensembl_geneset['master_transcript_type'].str.replace(val, trans_dict[val])\n",
    "\n",
    "# post-process reformatted data\n",
    "ensembl_geneset.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_geneset.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Ensembl Annotation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-UniProt_ - `Homo_sapiens.GRCh38.102.uniprot.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-uniprot` mapping file. These files are vital for successfully merging the ensembl identifiers with the uniprot data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url_uniprot = 'ftp://ftp.ensembl.org/pub/release-102/tsv/homo_sapiens/Homo_sapiens.GRCh38.102.uniprot.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.uniprot.tsv'):\n",
    "    data_downloader(url_uniprot, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ensembl_uniprot = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.uniprot.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# preprocess data\n",
    "ensembl_uniprot.rename(columns={'xref': 'uniprot_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_uniprot.replace('-', 'None', inplace=True)\n",
    "ensembl_uniprot.fillna('None', inplace=True)\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['xref_identity'].apply(lambda x: x != 'None')]\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['uniprot_id'].apply(lambda x: '-' not in x)]  # remove isoforms\n",
    "ensembl_uniprot = ensembl_uniprot.loc[ensembl_uniprot['info_type'].apply(lambda x: x == 'DIRECT')]\n",
    "# ensembl_uniprot['master_gene_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "# ensembl_uniprot['master_transcript_type'] = ['protein-coding'] * len(ensembl_uniprot)\n",
    "ensembl_uniprot.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ensembl-Entrez_ - `Homo_sapiens.GRCh38.102.entrez.tsv`  \n",
    "Once the main ensembl gene set has been read in, the next step is to read in the `ensembl-entrez` mapping file. These files are vital for successfully merging the ensembl identifiers with the entrez data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url_entrez = 'ftp://ftp.ensembl.org/pub/release-102/tsv/homo_sapiens/Homo_sapiens.GRCh38.102.entrez.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.entrez.tsv'):\n",
    "    data_downloader(url_entrez, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ensembl_entrez = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.GRCh38.102.entrez.tsv', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# preprocess data\n",
    "ensembl_entrez.rename(columns={'xref': 'entrez_id', 'gene_stable_id': 'ensembl_gene_id'}, inplace=True)\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['db_name'].apply(lambda x: x == 'EntrezGene')]\n",
    "ensembl_entrez = ensembl_entrez.loc[ensembl_entrez['info_type'].apply(lambda x: x == 'DEPENDENT')]\n",
    "ensembl_entrez.replace('-', 'None', inplace=True)\n",
    "ensembl_entrez.fillna('None', inplace=True)\n",
    "ensembl_entrez.drop(['db_name', 'info_type', 'source_identity', 'xref_identity', 'linkage_type'], axis=1, inplace=True)\n",
    "ensembl_entrez.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Annotation Data_ - `ensembl_uniprot` + `ensembl_entrez`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_entrez).intersection(set(ensembl_uniprot)))\n",
    "ensembl_annot = pandas.merge(ensembl_uniprot, ensembl_entrez, on=merge_cols, how='outer')\n",
    "ensembl_annot.fillna('None', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_annot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge Ensembl Annotation and Gene Set Data_ - `ensembl_geneset` + `ensembl_annot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_annot).intersection(set(ensembl_geneset)))\n",
    "ensembl = pandas.merge(ensembl_geneset, ensembl_annot, on=merge_cols, how='outer')\n",
    "ensembl.fillna('None', inplace=True)\n",
    "ensembl.replace('NA','None', inplace=True, regex=False)\n",
    "\n",
    "# preview data\n",
    "ensembl.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Save Cleaned Ensembl Data_  \n",
    "Save the cleaned Ensembl data so that it can be used when generating node metadata for transcript identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl.to_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt', header=True, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**UniProt Data**   \n",
    "_Human Gene Set Data_ - `uniprot_identifier_mapping.tab`\n",
    "\n",
    "This data was obtained by querying the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) using the *organism:\"Homo sapiens (Human) [9606]\"* keyword and including the following columns:\n",
    "- Entry (Standard)    \n",
    "- GeneID (*Genome Annotation*)  \n",
    "- Ensembl (*Genome Annotation*)  \n",
    "- HGNC (*Organism-specific*)  \n",
    "- Gene names (primary) (*Names & Taxonomy*)    \n",
    "- Gene synonym (primary) (*Names & Taxonomy*)    \n",
    "\n",
    "The URL to access the results of this query is obtained by clicking on the share symbol and copying the free-text from the box. To obtain the data in a tab-delimited format the following string is appended to the end of the URL: \"&format=tab\".\n",
    "\n",
    "**NOTE.** Be sure to obtain a new URL from the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) when rebuilding to ensure you are getting the most up-to-date data. This query was last generated on `01/30/2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Creviewed%2Cdatabase(GeneID)%2Cdatabase(Ensembl)%2Cdatabase(HGNC)%2Cgenes(ALTERNATIVE)%2Cgenes(PREFERRED)&format=tab'\n",
    "if not os.path.exists(unprocessed_data_location + 'uniprot_identifier_mapping.tab'):\n",
    "    data_downloader(url, unprocessed_data_location, 'uniprot_identifier_mapping.tab')\n",
    "\n",
    "# load data\n",
    "uniprot = pandas.read_csv(unprocessed_data_location + 'uniprot_identifier_mapping.tab', header=0, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, and unnesting `\"|\"` delimited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "uniprot.rename(columns={'Entry': 'uniprot_id',\n",
    "                        'Cross-reference (GeneID)': 'entrez_id',\n",
    "                        'Ensembl transcript': 'transcript_stable_id',\n",
    "                        'Cross-reference (HGNC)': 'hgnc_id',\n",
    "                        'Gene names  (synonym )': 'synonyms',\n",
    "                        'Gene names  (primary )' :'symbol'}, inplace=True)\n",
    "\n",
    "# update space-delimited synonyms to a pipe (i.e. '|')\n",
    "uniprot['synonyms'] = uniprot['synonyms'].apply(lambda x: '|'.join(x.split()) if x.isupper() else x)\n",
    "\n",
    "# only keep reviewed entries\n",
    "uniprot = uniprot.loc[uniprot['Status'].apply(lambda x: x != 'unreviewed')]\n",
    "\n",
    "# explode nested data\n",
    "explode_df_uniprot = explodes_data(uniprot.copy(), ['transcript_stable_id', 'entrez_id', 'hgnc_id'], ';')\n",
    "explode_df_uniprot = explodes_data(explode_df_uniprot.copy(), ['symbol', 'synonyms'], '|')\n",
    "\n",
    "# strip out uniprot names\n",
    "explode_df_uniprot['transcript_stable_id'] = explode_df_uniprot['transcript_stable_id'].str.replace('\\s.*','', regex=True)\n",
    "\n",
    "# remove duplicates\n",
    "explode_df_uniprot.drop(['Status'], axis=1, inplace=True)\n",
    "explode_df_uniprot.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_uniprot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**NCBI Data**   \n",
    "_Human Gene Set Data_ - `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'Homo_sapiens.gene_info'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "ncbi_gene = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info', header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Data file needs to be lightly cleaned before it can be merged with other data. This light cleaning includes renaming columns, replacing `NaN` with `None`, updating data types (i.e. making all columns type `str`), and unnesting `|` delimited data. Then, the `gene_type` variable is cleaned such that each of the variable's values are re-grouped to be `protein-coding`, `other` or `ncRNA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "ncbi_gene = ncbi_gene.loc[ncbi_gene['#tax_id'].apply(lambda x: x == 9606)]  # remove non-human rows\n",
    "ncbi_gene.replace('-', 'None', inplace=True)\n",
    "ncbi_gene.rename(columns={'GeneID': 'entrez_id', 'Symbol': 'symbol', 'Synonyms': 'synonyms'}, inplace=True)\n",
    "ncbi_gene['synonyms'] = ncbi_gene['synonyms'] + '|' + ncbi_gene['description'] + '|' + ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['Other_designations']\n",
    "ncbi_gene['symbol'] = ncbi_gene['Symbol_from_nomenclature_authority'] + '|' + ncbi_gene['symbol']\n",
    "ncbi_gene['name'] = ncbi_gene['Full_name_from_nomenclature_authority'] + '|' + ncbi_gene['description']\n",
    "\n",
    "# explode nested data\n",
    "explode_df_ncbi_gene = explodes_data(ncbi_gene.copy(), ['symbol', 'synonyms', 'name', 'dbXrefs'], '|')\n",
    "\n",
    "# clean up results\n",
    "explode_df_ncbi_gene['entrez_id'] = explode_df_ncbi_gene['entrez_id'].astype(str)\n",
    "explode_df_ncbi_gene = explode_df_ncbi_gene.loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.split(':')[0] in ['Ensembl', 'HGNC', 'IMGT/GENE-DB'])]\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('HGNC'))]\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['dbXrefs'].loc[explode_df_ncbi_gene['dbXrefs'].apply(lambda x: x.startswith('Ensembl'))]\n",
    "explode_df_ncbi_gene.fillna('None', inplace=True)\n",
    "\n",
    "# reformat entrez gene type\n",
    "explode_df_ncbi_gene['entrez_gene_type'] = explode_df_ncbi_gene['type_of_gene']\n",
    "gene_dict = genomic_type_mapper['entrez_gene_type']\n",
    "for val in gene_dict.keys():\n",
    "    explode_df_ncbi_gene['entrez_gene_type'] = explode_df_ncbi_gene['entrez_gene_type'].str.replace(val, gene_dict[val])\n",
    "# reformat master gene type\n",
    "explode_df_ncbi_gene['master_gene_type'] = explode_df_ncbi_gene['entrez_gene_type']\n",
    "gene_dict = genomic_type_mapper['master_gene_type']\n",
    "for val in gene_dict.keys():\n",
    "    explode_df_ncbi_gene['master_gene_type'] = explode_df_ncbi_gene['master_gene_type'].str.replace(val, gene_dict[val])\n",
    "\n",
    "# post-process reformatted data\n",
    "explode_df_ncbi_gene.drop(['type_of_gene', 'dbXrefs', 'description', 'Nomenclature_status', 'Modification_date',\n",
    "                           'LocusTag', '#tax_id', 'Full_name_from_nomenclature_authority', 'Feature_type',\n",
    "                           'Symbol_from_nomenclature_authority'], axis=1, inplace=True)\n",
    "explode_df_ncbi_gene['hgnc_id'] = explode_df_ncbi_gene['hgnc_id'].str.replace('HGNC:', '', regex=True)\n",
    "explode_df_ncbi_gene['ensembl_gene_id'] = explode_df_ncbi_gene['ensembl_gene_id'].str.replace('Ensembl:', '', regex=True)\n",
    "explode_df_ncbi_gene.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "explode_df_ncbi_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Protein Ontology Identifier Mapping Data**   \n",
    "_Protein Ontology Identifier Data_ - `promapping.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://proconsortium.org/download/current/promapping.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'promapping.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "pro_map = pandas.read_csv(unprocessed_data_location + 'promapping.txt', header=None, names=['pro_id', 'entry', 'pro_mapping'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preprocess Data_  \n",
    "Basic filtering to to include `Protein Ontology` mappings to `Uniprot` identifiers and cleaning to update formatting of accession values (i.e. removing `UniProtKB:`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_map = pro_map.loc[pro_map['entry'].apply(lambda x: x.startswith('Uni') and '_VAR' not in x and ', ' not in x)]  # keep 'UniProtKB' rows\n",
    "pro_map = pro_map.loc[pro_map['pro_mapping'].apply(lambda x: x.startswith('exact'))] # keep exact mappings\n",
    "pro_map['pro_id'] = pro_map['pro_id'].str.replace('PR:','PR_', regex=True)  # replace PR: with PR_\n",
    "pro_map['entry'] = pro_map['entry'].str.replace('(^\\w*\\:)','', regex=True)  # remove id prefixes\n",
    "pro_map = pro_map.loc[pro_map['pro_id'].apply(lambda x: '-' not in x)] # remove isoforms\n",
    "pro_map.rename(columns={'entry': 'uniprot_id'}, inplace=True)  # rename columns before merging\n",
    "pro_map.drop(['pro_mapping'], axis=1, inplace=True)  # remove uneeded columns\n",
    "pro_map.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "pro_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Merging Processed Genomic Identifier Data Sources  \n",
    "Merging all of the genomic identifier data sources is needed in order to create a map that can be used to integrate the different genomic data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `hgnc` + `ensembl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "merge_cols = list(set(explode_df_hgnc.columns).intersection(set(ensembl.columns)))\n",
    "ensembl_hgnc_merged_data = pandas.merge(ensembl, explode_df_hgnc, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_merged_data` + `explode_df_uniprot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(ensembl_hgnc_merged_data.columns).intersection(set(explode_df_uniprot.columns)))\n",
    "ensembl_hgnc_uniprot_merged_data = pandas.merge(ensembl_hgnc_merged_data, explode_df_uniprot, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_uniprot_merged_data` + `Homo_sapiens.gene_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "merge_cols = merge_cols = list(set(ensembl_hgnc_uniprot_merged_data).intersection(set(explode_df_ncbi_gene.columns)))\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data = pandas.merge(ensembl_hgnc_uniprot_merged_data, explode_df_ncbi_gene, on=merge_cols, how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.fillna('None', inplace=True)\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "ensembl_hgnc_uniprot_ncbi_merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Data Sources:* `ensembl_hgnc_uniprot_ncbi_merged_data` + `promapping.txt`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "merged_data = pandas.merge(ensembl_hgnc_uniprot_ncbi_merged_data, pro_map, on='uniprot_id', how='outer')\n",
    "\n",
    "# clean up merged data\n",
    "merged_data.fillna('None', inplace=True)\n",
    "merged_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "# preview data\n",
    "merged_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fix Symbol Formatting_  \n",
    "some genes are formatted similarly to dates (e.g. `DEC1`), which can be erroneously re-formatted during input as a date value (i.e. `1-DEC`). In order for the data to be successfully merged with other data sources, all date-formatted genes need to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dates = []\n",
    "for x in tqdm(list(merged_data['symbol'])):\n",
    "    if '-' in x and len(x.split('-')[0]) < 3 and len(x.split('-')[1]) == 3:\n",
    "        clean_dates.append(x.split('-')[1].upper() + x.split('-')[0])\n",
    "    else: clean_dates.append(x)\n",
    "\n",
    "# add cleaned date var back to data set\n",
    "merged_data['symbol'] = clean_dates\n",
    "merged_data.fillna('None', inplace=True)\n",
    "\n",
    "# make sure that all gene and transcript type colunmns have none recoded to unknown or not protein-coding\n",
    "merged_data['hgnc_gene_type'] = merged_data['hgnc_gene_type'].str.replace('None', 'unknown', regex=False)\n",
    "merged_data['ensembl_gene_type'] = merged_data['ensembl_gene_type'].str.replace('None', 'unknown', regex=False)\n",
    "merged_data['entrez_gene_type'] = merged_data['entrez_gene_type'].str.replace('None', 'unknown', regex=False)\n",
    "merged_data['master_gene_type'] = merged_data['master_gene_type'].str.replace('None', 'unknown', regex=False)\n",
    "merged_data['master_transcript_type'] = merged_data['master_transcript_type'].str.replace('None', 'not protein-coding', regex=False)\n",
    "merged_data['ensembl_transcript_type'] = merged_data['ensembl_transcript_type'].str.replace('None', 'unknown', regex=False)\n",
    "\n",
    "# remove duplicates\n",
    "merged_data_clean = merged_data.drop_duplicates(subset=None, keep='first')\n",
    "\n",
    "# write data\n",
    "merged_data_clean.to_csv(processed_data_location + 'Merged_Human_Ensembl_Entrez_HGNC_Uniprot_Identifiers.txt', header=True, sep='\\t', index=False)\n",
    "    \n",
    "# preview data\n",
    "merged_data_clean.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Create a Master Mapping Dictionary**  \n",
    "Although the above steps result in a `pandas.Dataframe` of the merged identifiers, there is still work needed in order to be able to obtain a complete mapping between the identifiers. For example, if you were to search for Entrez gene identifier `entrez_259234` you would find the following mappings: `entrez_259234-ENSG00000233316`, `entrez_259234-DSCR10`. If you only had `ENSG00000233316`, with the current data you would be unable to obtain the gene symbol without first mapping to the Entrez gene identifier. \n",
    "\n",
    "To solve this problem, we build a master dictionary where the keys are `ensembl_gene_id`, `transcript_stable_id`, `protein_stable_id`, `uniprot_id`, `entrez_id`, `hgnc_id`, `pro_id`, and `symbol` identifiers and values are the list of genomic identifiers that match to each identifier. It's important to note that there are several labeling identifiers (i.e. `name`, `chromosome`, `map_location`, `Other_designations`, `synonyms`, `transcript_name`, `*_gene_types`, and `trasnscript_type_update`), which will only be mapped when clustered against one of the primary identifier types (i.e. the keys described above).\n",
    "\n",
    "_Note_. The next chunk does a lot of heavy lifting and takes approximately ~40 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data to convert all nones, empty values, and unknowns to NaN\n",
    "for col in merged_data_clean.columns:\n",
    "    merged_data_clean[col] = merged_data_clean[col].apply(lambda x: '|'.join([i for i in x.split('|') if i != 'None']))\n",
    "merged_data_clean.replace(to_replace=['None', '', 'unknown'], value=numpy.nan, inplace=True)\n",
    "identifiers = [x for x in merged_data_clean.columns if x.endswith('_id')] + ['symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to dictionary\n",
    "master_dict = {}\n",
    "for idx in tqdm(identifiers):\n",
    "    grouped_data = merged_data_clean.groupby(idx)\n",
    "    grp_ids = set([x for x in list(grouped_data.groups.keys()) if x != numpy.nan])\n",
    "    for grp in grp_ids:\n",
    "        df = grouped_data.get_group(grp).dropna(axis=1, how='all')\n",
    "        df_cols, key = df.columns, idx + '_' + grp\n",
    "        val_df = [[col + '_' + x for x in set(df[col]) if isinstance(x, str)] for col in df_cols if col != idx]\n",
    "        if len(val_df) > 0:\n",
    "            if key in master_dict.keys(): master_dict[key] += [i for j in val_df for i in j if len(i) > 0]\n",
    "            else: master_dict[key] = [i for j in val_df for i in j if len(i) > 0]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finalizing Master Mapping Dictionary_  \n",
    "Then, we need to identify a master gene and transcript type for each entity because the last ran code chunk can result in several genes and transcripts with differing types (i.e. `protein-coding` or `not protein-coding`). The next step collects all information for each gene and transcript and performs a voting procedure to select a single primary gene and transcript type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformatted_mapped_identifiers = dict()\n",
    "for key, values in tqdm(master_dict.items()):\n",
    "    identifier_info = set(values); gene_prefix = 'master_gene_type_'; trans_prefix = 'master_transcript_type_'\n",
    "    if key.split('_')[0] in ['protein', 'uniprot', 'pro']: pass\n",
    "    elif 'transcript' in key:\n",
    "        trans_match = [x.replace(trans_prefix, '') for x in values if trans_prefix in x]\n",
    "        if len(trans_match) > 0:\n",
    "            t_type_list = ['protein-coding' if ('protein-coding' in trans_match or 'protein_coding' in trans_match) else 'not protein-coding']\n",
    "            identifier_info |= {'transcript_type_update_' + max(set(t_type_list), key=t_type_list.count)}\n",
    "    else:\n",
    "        gene_match = [x.replace(gene_prefix, '') for x in values if x.startswith(gene_prefix) and 'type' in x]\n",
    "        if len(gene_match) > 0:\n",
    "            g_type_list = ['protein-coding' if ('protein-coding' in gene_match or 'protein_coding' in gene_match) else 'not protein-coding']\n",
    "            identifier_info |= {'gene_type_update_' + max(set(g_type_list), key=g_type_list.count)}\n",
    "    reformatted_mapped_identifiers[key] = identifier_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the dictionary\n",
    "# output > 4GB requires special approach: https://stackoverflow.com/questions/42653386/does-pickle-randomly-fail-with-oserror-on-large-files\n",
    "filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "max_bytes, bytes_out = 2**31 - 1, pickle.dumps(reformatted_mapped_identifiers)\n",
    "n_bytes = sys.getsizeof(bytes_out)\n",
    "\n",
    "with open(filepath, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # load data\n",
    "# filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "\n",
    "# # defensive way to write pickle.load, allowing for very large files on all platforms\n",
    "# max_bytes = 2**31 - 1\n",
    "# input_size = os.path.getsize(filepath)\n",
    "# bytes_in = bytearray(0)\n",
    "\n",
    "# with open(filepath, 'rb') as f_in:\n",
    "#     for _ in range(0, input_size, max_bytes):\n",
    "#         bytes_in += f_in.read(max_bytes)\n",
    "\n",
    "# # load pickled data\n",
    "# reformatted_mapped_identifiers = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ensembl Gene-Entrez Gene <a class=\"anchor\" id=\"ensemblgene-entrezgene\"></a>\n",
    "\n",
    "\n",
    "**Purpose:** To map Ensembl gene identifiers to Entrez gene identifiers when creating `gene`-`gene` edges\n",
    "\n",
    "**Output:** `ENSEMBL_GENE_ENTREZ_GENE_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                  'ensembl_gene_id', 'entrez_id', 'ensembl_gene_type', 'entrez_gene_type',\n",
    "                  'gene_type_update', 'gene_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "egeg_data = pandas.read_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False,\n",
    "                            names=['Ensembl_Gene_IDs', 'Entrez_Gene_IDs',\n",
    "                                   'Ensembl_Gene_Type', 'Entrez_Gene_Type',\n",
    "                                   'Master_Gene_Type1', 'Master_Gene_Type2'])\n",
    "\n",
    "# add prefix to output edge\n",
    "egeg_data['Entrez_Gene_IDs'] = 'NCBIGene_' + egeg_data['Entrez_Gene_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "egeg_data.to_csv(processed_data_location + 'ENSEMBL_GENE_ENTREZ_GENE_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} ensembl gene-entrez gene edges'.format(edge_count=len(egeg_data)))\n",
    "egeg_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ensembl Transcript-Protein Ontology <a class=\"anchor\" id=\"ensembltranscript-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Ensembl transcript identifiers to Protein Ontology identifiers when creating `rna`-`protein` edges\n",
    "\n",
    "**Output:** `ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                  'transcript_stable_id', 'pro_id', 'ensembl_transcript_type', None,\n",
    "                  'transcript_type_update', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "etpr_data = pandas.read_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1, 2, 4],\n",
    "                            names=['Ensembl_Transcript_IDs', 'Protein_Ontology_IDs',\n",
    "                                   'Ensembl_Transcript_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "# add prefix to output edge\n",
    "etpr_data['Ensembl_Transcript_ID_Edge'] = 'ensembl_' + etpr_data['Ensembl_Transcript_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "etpr_data.to_csv(processed_data_location + 'ENSEMBL_TRANSCRIPT_PROTEIN_ONTOLOGY_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} ensembl transcript-protein ontology edges'.format(edge_count=len(etpr_data)))\n",
    "etpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Entrez Gene-Ensembl Transcript <a class=\"anchor\" id=\"entrezgene-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map entrez gene identifiers to Ensembl transcript identifiers when creating `gene`-`rna` edges\n",
    "\n",
    "**Output:** `ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'entrez_id', 'transcript_stable_id', 'entrez_gene_type', 'ensembl_transcript_type',\n",
    "                  'gene_type_update', 'transcript_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "eet_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None, delimiter='\\t', low_memory=False,\n",
    "                           names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs',\n",
    "                                  'Entrez_Gene_Type', 'Ensembl_Transcript_Type',\n",
    "                                  'Master_Gene_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "# add prefix to output edge\n",
    "eet_data['Ensembl_Transcript_IDs'] = 'ensembl_' + eet_data['Ensembl_Transcript_IDs'].astype(str)\n",
    "eet_data['Entrez_Gene_Edge'] = 'NCBIGene_' + eet_data['Entrez_Gene_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "eet_data.to_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} entrez gene identifiers-ensembl transcript edges'.format(edge_count=len(eet_data)))\n",
    "eet_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Entrez Gene-Protein Ontology <a class=\"anchor\" id=\"entrezgene-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Protein Ontology identifiers to Ensembl transcript identifiers when creating the following edges:   \n",
    "- chemical-protein  \n",
    "- gene-protein\n",
    "\n",
    "**Output:** `ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'entrez_id', 'pro_id', 'entrez_gene_type', None,\n",
    "                  'gene_type_update', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "egpr_data = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols = [0, 1, 2, 4],\n",
    "                            names=['Gene_IDs', 'Protein_Ontology_IDs',\n",
    "                                   'Entrez_Gene_Type', 'Master_Gene_Type'])\n",
    "\n",
    "# add prefix to output edge\n",
    "egpr_data['Entrez_Gene_Edge'] = 'NCBIGene_' + egpr_data['Gene_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "egpr_data.to_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} entrez gene-protein ontology edges'.format(edge_count=len(egpr_data)))\n",
    "egpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Gene Symbol-Ensembl Transcript <a class=\"anchor\" id=\"genesymbol-ensembltranscript\"></a>\n",
    "\n",
    "**Purpose:** To map gene symbols to Ensembl transcript identifiers when creating the following edges: \n",
    "- chemical-rna  \n",
    "- rna-anatomy  \n",
    "- rna-cell  \n",
    "\n",
    "**Output:** `GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                  'symbol', 'transcript_stable_id', 'master_gene_type', 'ensembl_transcript_type',\n",
    "                  'gene_type_update', 'transcript_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "set_data = pandas.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None, delimiter='\\t', low_memory=False,\n",
    "                           names=['Gene_Symbols', 'Ensembl_Transcript_IDs',\n",
    "                                  'Gene_Type', 'Ensembl_Transcript_Type',\n",
    "                                  'Master_Gene_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "# add prefix to output edge\n",
    "set_data['Ensembl_Transcript_IDs'] = 'ensembl_' + set_data['Ensembl_Transcript_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "set_data.to_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} gene symbol-ensembl transcript edges'.format(edge_count=len(set_data.drop_duplicates())))\n",
    "set_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### STRING-Protein Ontology <a class=\"anchor\" id=\"string-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map STRING identifiers to Protein Ontology identifiers when creating `protein`-`protein` edges \n",
    "\n",
    "**Output:** `STRING_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'protein_stable_id', 'pro_id', None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "stpr_data = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                            names=['STRING_IDs', 'Protein_Ontology_IDs'])\n",
    "\n",
    "# add prefix to output edge\n",
    "stpr_data['STRING_IDs'] = '9606.' + stpr_data['STRING_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "stpr_data.to_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} string-protein ontology edges'.format(edge_count=len(stpr_data.drop_duplicates())))\n",
    "stpr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Uniprot Accession-Protein Ontology <a class=\"anchor\" id=\"uniprotaccession-proteinontology\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Protein Ontology identifiers when creating the following edges:  \n",
    "- protein-gobp  \n",
    "- protein-gomf  \n",
    "- protein-gocc  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst \n",
    "- protein-pathway\n",
    "\n",
    "**Output:** `UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                  'uniprot_id', 'pro_id', None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "uapr_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                            names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs'])\n",
    "\n",
    "print('There are {edge_count} uniprot accession-protein ontology edges'.format(edge_count=len(uapr_data.drop_duplicates())))\n",
    "uapr_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Uniprot Accession-Entrez Gene <a class=\"anchor\" id=\"uniprotaccession-entrezgene\"></a>\n",
    "\n",
    "**Purpose:** To map Uniprot accession identifiers to Entrez Gene identifiers when creating the following edges:  \n",
    "- gene-gene  \n",
    "\n",
    "**Output:** `UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomic_id_mapper(reformatted_mapped_identifiers,\n",
    "                  processed_data_location + 'UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt',\n",
    "                  'uniprot_id', 'entrez_id', None, 'master_gene_type', None, 'gene_type_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print the number of rows, and preview it\n",
    "uaeg_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt',\n",
    "                            header=None, delimiter='\\t', low_memory=False, usecols=[0, 1, 2, 3],\n",
    "                            names=['Uniprot_Accession_IDs', 'Entrez_Gene_IDs',\n",
    "                                  'master_gene_type', 'gene_type_update'])\n",
    "\n",
    "# add prefix to output edge\n",
    "uaeg_data['Entrez_Gene_IDs'] = 'NCBIGene_' + uaeg_data['Entrez_Gene_IDs'].astype(str)\n",
    "\n",
    "# write data back to file\n",
    "uaeg_data.to_csv(processed_data_location + 'UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt', header=None, sep='\\t', index=False)\n",
    "\n",
    "print('There are {edge_count} uniprot accession-entrez gene edges'.format(edge_count=len(uaeg_data.drop_duplicates())))\n",
    "uaeg_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "### Other Identifier Mapping <a class=\"anchor\" id=\"other-identifier-mapping\"></a>\n",
    "***\n",
    "* [ChEBI Identifiers](#mesh-chebi)  \n",
    "* [Human Protein Atlas Tissue and Cell Types](#hpa-uberon) \n",
    "* [Human Disease and Phenotype Identifiers](#disease-identifiers) \n",
    "* [Reactome Pathways and the Pathway Ontology](#reactome-pw)  \n",
    "* [Genomic Identifiers and the Sequence Ontology](#genomic-so)  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChEBI-MeSH Identifiers <a class=\"anchor\" id=\"mesh-chebi\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [mapping-mesh-to-chebi](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#mapping-mesh-identifiers-to-chebi-identifiers)  \n",
    "\n",
    "**Purpose:** Map MeSH identifiers to ChEBI identifiers when creating the following edges:  \n",
    "- chemical-gene  \n",
    "- chemical-disease\n",
    "\n",
    "**Dependencies:** Recapitulates the [`LOOM`](https://www.bioontology.org/wiki/BioPortal_Mappings) algorithm implemented by BioPortal when creating mappings between resources. The procedure is relatively straightforward and consists of the following:\n",
    "- For all MeSH `SCR Chemicals`, obtain the following information:  \n",
    "  - <u>Identifiers</u>: MeSH identifiers     \n",
    "  - <u>Labels</u>: string labels using the `RDFS:label` object property  \n",
    "  - <u>Synonyms</u>: track down all synonyms using the `vocab:concept` and `vocab:preferredConcept` object properties   \n",
    "- For all ChEBI classes, obtain the following information:  \n",
    "  - <u>Labels</u>: string labels using the `RDFS:label` object property  \n",
    "  - <u>Synonyms</u>: track down all synonyms using all `synonym` object properties \n",
    "  \n",
    "*Alternatively:* You can use the [`ncbo_rest_api.py`](https://gist.github.com/callahantiff/a28fb3160782f42f104e9ec41553af0d) script to pull mappings from the BioPortal API, but note that it takes >2 days for it to finish.\n",
    "\n",
    "**Output:** `CHEBI_MESH_MAP.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**MeSH**  \n",
    "Downloads the `nt`-formatted version of the current MeSH vocabulary. Preprocesing is then performed in order to reformat the data so that it can be converted into a Pandas DataFrame in preparation of merging it with `ChEBI` in order to identify overlapping concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://nlmpubs.nlm.nih.gov/online/mesh/rdf/2021/mesh2021.nt'\n",
    "if not os.path.exists(unprocessed_data_location + 'mesh2021.nt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "mesh = [x.split('> ') for x in tqdm(open(unprocessed_data_location + 'mesh2021.nt').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "mesh_dict, results = {}, []\n",
    "for row in tqdm(mesh):\n",
    "    dbx, lab, msh_type = None, None, None\n",
    "    s, p, o = row[0].split('/')[-1], row[1].split('#')[-1], row[2]  \n",
    "    if s[0] in ['C', 'D'] and ('.' not in s and 'Q' not in s) and len(s) >= 5:\n",
    "        s = 'MESH_' + s\n",
    "        if p == 'preferredConcept' or p == 'concept': dbx = 'MESH_' + o.split('/')[-1]\n",
    "        if 'label' in p.lower(): lab = o.split('\"')[1]\n",
    "        if 'type' in p.lower(): msh_type = o.split('#')[1]\n",
    "        if s in mesh_dict.keys():\n",
    "            if dbx is not None: mesh_dict[s]['dbxref'].add(dbx)\n",
    "            if lab is not None: mesh_dict[s]['label'].add(lab)\n",
    "            if msh_type is not None: mesh_dict[s]['type'].add(msh_type)\n",
    "        else:\n",
    "            mesh_dict[s] = {'dbxref': set() if dbx is None else {dbx},\n",
    "                            'label': set() if lab is None else {lab},\n",
    "                            'type': set() if msh_type is None else {msh_type},\n",
    "                            'synonym': set()}\n",
    "\n",
    "# fine tune dictionary - obtain labels for each entry's synonym identifiers\n",
    "for key in tqdm(mesh_dict.keys()):\n",
    "    for i in mesh_dict[key]['dbxref']:\n",
    "        if len(mesh_dict[key]['dbxref']) > 0 and i in mesh_dict.keys():\n",
    "            mesh_dict[key]['synonym'] |= mesh_dict[i]['label']\n",
    "\n",
    "# expand data and convert to pandas DataFrame\n",
    "for key, value in tqdm(mesh_dict.items()):\n",
    "    results += [[key, list(value['label'])[0], 'NAME']]\n",
    "    if len(value['synonym']) > 0:\n",
    "        for i in value['synonym']:\n",
    "            results += [[key, i, 'SYNONYM']]\n",
    "mesh_filtered = pandas.DataFrame({'CODE': [x[0] for x in results],\n",
    "                                  'TYPE': [x[2] for x in results],\n",
    "                                  'STRING': [x[1] for x in results]})\n",
    "\n",
    "# lowercase all strings and remove white space and punctuation\n",
    "mesh_filtered['STRING'] = mesh_filtered['STRING'].str.lower()\n",
    "mesh_filtered['STRING'] = mesh_filtered['STRING'].str.replace('[^\\w]','')\n",
    "\n",
    "# preview data\n",
    "mesh_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**ChEBI**  \n",
    "Downloads the flat-file containing labels and synonyms for all classes in the `ChEBI` ontology. Preprocessing is then performed in order to reformat the data so that it can be converted into a Pandas DataFrame in preparation of merging it with `MeSH` in order to identify overlapping concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ebi.ac.uk/pub/databases/chebi/Flat_file_tab_delimited/names.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'names.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "chebi = pandas.read_csv(unprocessed_data_location + 'names.tsv', header=0, delimiter='\\t')\n",
    "\n",
    "# preprocess data\n",
    "chebi_filtered = chebi[['COMPOUND_ID', 'TYPE', 'NAME']]\n",
    "chebi_filtered.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "chebi_filtered.columns = ['CODE', 'TYPE', 'STRING']\n",
    "\n",
    "# append CHEBI to the number in each code\n",
    "chebi_filtered['CODE'] = chebi_filtered['CODE'].apply(lambda x: \"{}{}\".format('CHEBI_', x))\n",
    "\n",
    "# lowercase all strings and remove white space and punctuation\n",
    "chebi_filtered['STRING'] = chebi_filtered['STRING'].str.lower()\n",
    "chebi_filtered['STRING'] = chebi_filtered['STRING'].str.replace('[^\\w]','')\n",
    "\n",
    "# preview data\n",
    "chebi_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***  \n",
    "**Merge Identifier Data**  \n",
    "Performs an inner merge of the `MeSH` and `ChEBI` Pandas DataFrames in order to find concepts that exist in both DataFrames. Results are then written out to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "chem_merge = pandas.merge(chebi_filtered[['STRING', 'CODE']], mesh_filtered[['STRING', 'CODE']], on='STRING', how='inner')\n",
    "\n",
    "# filter results\n",
    "mesh_edges = set()\n",
    "for idx, row in chem_merge.drop_duplicates().iterrows():\n",
    "    mesh, chebi = row['CODE_y'], row['CODE_x']\n",
    "    syns = [x for x in mesh_dict[mesh]['dbxref'] if 'C' in x or 'D' in x]\n",
    "    mesh_edges.add(tuple([mesh, chebi]))\n",
    "    if len(syns) > 0:\n",
    "        for x in syns:\n",
    "            mesh_edges.add(tuple([x, chebi]))\n",
    "\n",
    "# write resulting mappings\n",
    "with open(processed_data_location + 'MESH_CHEBI_MAP.txt', 'w') as out:\n",
    "    for pair in mesh_edges:\n",
    "        out.write(pair[0].replace('_', ':') + '\\t' + pair[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt', header=None, names=['MESH_ID', 'CHEBI_ID'], delimiter='\\t')\n",
    "\n",
    "# preview mapping results\n",
    "print('There are {} MeSH-ChEBI Edges'.format(len(data)))\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Disease and Phenotype Identifiers <a class=\"anchor\" id=\"disease-identifiers\"></a>\n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [DisGeNET](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#disgenet)  \n",
    "- [MedGen](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#national-center-for-biotechnology-information-medgen) \n",
    "\n",
    "**Purpose:** This script downloads the Human Phenotype Ontology (HPO), the MonDO Disease Ontology (MONDO), [disease_mappings.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz), and [MGCONSO.RRF](https://ftp.ncbi.nlm.nih.gov/pub/medgen/MGCONSO.RRF.gz) in order to map UMLS identifiers to HPO and MONDO identifiers when creating the following edges:  \n",
    "- chemical-disease  \n",
    "- disease-phenotype  \n",
    "- chemical-phenotype  \n",
    "- gene-phenotype  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Output:**   \n",
    "- Human Disease Ontology Mappings ➞ `DISEASE_MONDO_MAP.txt`\n",
    "- Human Phenotype Ontology Mappings ➞ `PHENOTYPE_HPO_MAP.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**MONDO Identifiers**  \n",
    "`MONDO` contains DbXRef mappings to other disease terminology identifiers. To make this useful, we will store the DbXRefs as a dictionary with `MONDO` identifiers as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'mondo_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/mondo.owl',\n",
    "                             unprocessed_data_location + 'mondo_with_imports.owl'))\n",
    "    \n",
    "# read data into RDFLib graph object\n",
    "mondo_graph = Graph().parse(unprocessed_data_location + 'mondo_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(mondo_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "# get dbxrefs for all MONDO classes\n",
    "dbxref_res = gets_ontology_class_dbxrefs(mondo_graph)[0]\n",
    "mondo_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'MONDO' in str(v)}\n",
    "\n",
    "# pickle dictionary\n",
    "pickle.dump(mondo_dict, open(processed_data_location + 'Mondo_Identifier_Map.pkl', 'wb'), protocol=4)\n",
    "\n",
    "# convert to pandas DataFrame\n",
    "temp_list = []\n",
    "for k, v in mondo_dict.items():\n",
    "    if k.startswith('umls:'): new_k = k.split(':')[-1].upper()\n",
    "    elif k.startswith('hp:'): new_k = k.upper()\n",
    "    elif k.startswith('mesh:'): new_k = 'MESH:' + k.split(':')[-1].upper()\n",
    "    elif k.startswith('orphanet:'): new_k = 'ORPHA:' + k.split(':')[-1].upper()\n",
    "    elif k.startswith('omimps:'): new_k = 'OMIM:' + k.split(':')[-1].upper()\n",
    "    else: new_k = k\n",
    "    for i in v:\n",
    "        temp_list += [[new_k, i.replace(':', '_')]]\n",
    "        temp_list += [[i, i.replace(':', '_')]]\n",
    "\n",
    "        # convert to \n",
    "mondo_df = pandas.DataFrame({'other_id': [x[0] for x in temp_list],\n",
    "                             'ontology_id': [x[1] for x in temp_list]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**HPO Identifiers**  \n",
    "`HPO` contains DbXRef mappings to other disease terminology identifiers. To make this useful, we will store the DbXRefs as a dictionary with `HPO` identifiers as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'hp_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/hp.owl',\n",
    "                             unprocessed_data_location + 'hp_with_imports.owl'))\n",
    "\n",
    "# read data into RDFLib graph object\n",
    "hp_graph = Graph().parse(unprocessed_data_location + 'hp_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(hp_graph), datetime.datetime.now().strftime('%m/%d/%Y')))\n",
    "\n",
    "# get dbxrefs for all HPO classes\n",
    "dbxref_res = gets_ontology_class_dbxrefs(hp_graph)[0]\n",
    "hp_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'HP' in str(v)}\n",
    "\n",
    "# pickle dictionary\n",
    "pickle.dump(hp_dict, open(processed_data_location + 'HPO_Identifier_Map.pkl', 'wb'), protocol=4)\n",
    "\n",
    "# convert to pandas DataFrame\n",
    "temp_list = []\n",
    "for k, v in hp_dict.items():\n",
    "    if k.startswith('umls:'): new_k = k.split(':')[-1].upper()\n",
    "    elif k.startswith('mondo:'): new_k = k.upper()\n",
    "    elif k.startswith('msh:'): new_k = 'MESH:' + k.split(':')[-1].upper()\n",
    "    elif k.startswith('orpha:'): new_k = 'ORPHA:' + k.split(':')[-1].upper()\n",
    "    else: new_k = k\n",
    "    for i in v:\n",
    "        temp_list += [[new_k, i.replace(':', '_')]]\n",
    "        temp_list += [[i, i.replace(':', '_')]]\n",
    "\n",
    "# convert to \n",
    "hp_df = pandas.DataFrame({'other_id': [x[0] for x in temp_list],\n",
    "                          'ontology_id': [x[1] for x in temp_list]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Combine MONDO and HP Disease Mapping DataFrames into a Single DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data frames\n",
    "disease_map_df = pandas.concat([mondo_df, hp_df])\n",
    "\n",
    "# preview data\n",
    "disease_map_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**DisGeNET Disease Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/disease_mappings.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'disease_mappings.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data\n",
    "disease_data = pandas.read_csv(unprocessed_data_location + 'disease_mappings.tsv', header=0, delimiter='\\t')\n",
    "\n",
    "# reformat data\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.lower()\n",
    "disease_data['diseaseId'] = disease_data['diseaseId'].str.lower()\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('hpo', 'HP')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('mondo', 'MONDO')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('msh', 'MESH')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('omim', 'OMIM')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('do', 'doid')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('ordo', 'ORPHA')\n",
    "disease_data['vocabulary'] = disease_data['vocabulary'].str.replace('ORPHAid', 'ORPHA')\n",
    "\n",
    "# capitalize UMLS id\n",
    "disease_data['diseaseId'] = disease_data['diseaseId'].str.upper()\n",
    "\n",
    "# create a disease code column\n",
    "disease_data['code'] = disease_data['vocabulary'] + ':' + disease_data['code']\n",
    "disease_data['code'] = disease_data['code'].str.replace('HP:HP:', 'HP:')\n",
    "\n",
    "# rename columns\n",
    "disease_data.rename(columns={'diseaseId': 'cui', 'vocabularyName': 'code_name'}, inplace=True)\n",
    "\n",
    "# remove unneeded columns\n",
    "disease_data = disease_data[['cui', 'code', 'code_name', 'vocabulary']].drop_duplicates()\n",
    "\n",
    "# preview data\n",
    "disease_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**MedGen Disease Mappings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://ftp.ncbi.nlm.nih.gov/pub/medgen/MGCONSO.RRF.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'MGCONSO.RRF'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data and clean data\n",
    "medgen_data = pandas.read_csv(unprocessed_data_location + 'MGCONSO.RRF', header=0, delimiter='|')\n",
    "medgen_data = medgen_data[medgen_data['SUPPRESS'] == 'N'].drop_duplicates()\n",
    "medgen_data = medgen_data[medgen_data['SAB'].isin(['HPO', 'MONDO', 'MSH', 'ORDO', 'OMIM'])].drop_duplicates()\n",
    "\n",
    "# reformat codes\n",
    "medgen_data['temp_code'] = medgen_data.apply(lambda x: 'MESH:' + x['CODE'] if x['SAB'] == 'MSH'\n",
    "                                             else 'OMIM:' + x['CODE'] if x['SAB'] == 'OMIM'\n",
    "                                             else 'ORPHA:' + x['SDUI'].split('_')[-1] if x['SAB'] == 'ORDO'\n",
    "                                             else x['SDUI'] if x['SAB'] == 'HPO'\n",
    "                                             else x['SDUI'] if x['SAB'] == 'MONDO'\n",
    "                                             else 'None', axis=1)\n",
    "\n",
    "# add rows for MedGen identifiers\n",
    "temp = medgen_data[['#CUI']]\n",
    "temp['temp_code'] = 'MedGen:' + medgen_data['#CUI']\n",
    "medgen_data = pandas.concat([medgen_data, temp])\n",
    "\n",
    "# remove unneeded columns\n",
    "medgen_data = medgen_data[['#CUI', 'temp_code', 'STR', 'SAB']].drop_duplicates()\n",
    "\n",
    "# rename columns\n",
    "medgen_data.rename(columns={'#CUI': 'cui',\n",
    "                            'STR': 'code_name',\n",
    "                            'temp_code': 'code',\n",
    "                           'SAB': 'vocabulary'}, inplace=True)\n",
    "\n",
    "# reformat vocabulary ids\n",
    "medgen_data['vocabulary'] = medgen_data['vocabulary'].str.replace('HPO', 'HP')\n",
    "medgen_data['vocabulary'] = medgen_data['vocabulary'].str.replace('MSH', 'MESH')\n",
    "\n",
    "# preview data\n",
    "medgen_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Combine DisGeNET and MedGen Mappings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "disease_mapping_data = pandas.concat([disease_data, medgen_data]).drop_duplicates()\n",
    "\n",
    "# preview data\n",
    "disease_mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Build Disease Identifier Dictionary_  \n",
    "In order to improve efficiency when mapping different disease terminology identifiers to the [MonDO Disease Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#mondo-disease-ontology) and [Human Phenotype Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-phenotype-ontology), we create a dictionary of disease identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cuis that map to HP or MONDO\n",
    "disease_data_keep = disease_mapping_data.copy()\n",
    "disease_data_keep = disease_data_keep.query('vocabulary == \"HP\" | vocabulary == \"MONDO\"')\n",
    "disease_data_keep = disease_data_keep[['cui', 'code']]\n",
    "cui_list = set(disease_data_keep['cui'])\n",
    "\n",
    "# obtain a list of other ids that map to the cuis\n",
    "temp_df = disease_mapping_data[disease_mapping_data['cui'].isin(cui_list)]\n",
    "\n",
    "# merge back with original data\n",
    "merged_temp = temp_df.merge(disease_data_keep, on='cui')\n",
    "merged_temp = merged_temp[['code_x', 'code_y', 'code_name', 'vocabulary']].drop_duplicates()\n",
    "\n",
    "# rename the columns\n",
    "merged_temp.rename(columns={'code_x': 'cui', 'code_y': 'code'}, inplace=True)\n",
    "\n",
    "# combine the columns back to main data\n",
    "disease_mapping_data = pandas.concat([disease_mapping_data, merged_temp]).drop_duplicates()\n",
    "disease_mapping_data = disease_mapping_data[['cui', 'code']].drop_duplicates()\n",
    "\n",
    "# merge ontology and other mappings together\n",
    "cleaned_disease_map = disease_mapping_data.merge(disease_map_df, left_on='cui', right_on='other_id')\n",
    "\n",
    "# clean up file\n",
    "cleaned_disease_map = cleaned_disease_map[['cui', 'ontology_id']]\n",
    "cleaned_disease_map.rename(columns={'cui': 'disease_id'}, inplace=True)\n",
    "\n",
    "# format ontology identifiers\n",
    "cleaned_disease_map['ontology_id'] = cleaned_disease_map['ontology_id'].str.replace(':', '_')\n",
    "cleaned_disease_map['vocabulary'] = cleaned_disease_map['ontology_id'].str.replace('\\_.*', '', regex=True)\n",
    "cleaned_disease_map.drop_duplicates(inplace=True)\n",
    "\n",
    "# preview data\n",
    "cleaned_disease_map.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write Mapping Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# split data by ontology and write to file\n",
    "mondo_map = cleaned_disease_map[cleaned_disease_map['vocabulary'] == 'MONDO'].drop_duplicates()\n",
    "hp_map = cleaned_disease_map[cleaned_disease_map['vocabulary'] == 'HP'].drop_duplicates()\n",
    "mondo_map = mondo_map[['disease_id', 'ontology_id']]\n",
    "hp_map = hp_map[['disease_id', 'ontology_id']]\n",
    "\n",
    "\n",
    "# write data\n",
    "mondo_map.to_csv(processed_data_location + 'DISEASE_MONDO_MAP.txt', header=None, index=False, sep='\\t')\n",
    "hp_map.to_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed MONDO Disease Ontology Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "dis_data = pandas.read_csv(processed_data_location + 'DISEASE_MONDO_MAP.txt', header=None, names=['Disease_IDs', 'MONDO_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {} disease-MONDO edges'.format(len(dis_data)))\n",
    "dis_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Human Phenotype Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "hp_data = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', header=None, names=['Disease_IDs', 'HP_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {} phenotype-HPO edges'.format(len(hp_data)))\n",
    "hp_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Human Protein Atlas/GTEx Tissue/Cells - UBERON + Cell Ontology + Cell Line Ontology <a class=\"anchor\" id=\"hpa-uberon\"></a>\n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-protein-atlas) \n",
    "- [genotype-tissue-expression-project](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#genotype-tissue-expression-project)  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Purpose:** Downloads a query for cell, tissue, and blood types with overexpressed protein-coding genes in the human proteome ([`proteinatlas_search.tsv`](https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv)) via [API](https://www.proteinatlas.org/about/help/dataaccess) and median gene-level TPM by tissue for all genes that are not protein-coding ([`GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct`](https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz)) in order to create mappings between cell and tissue type strings to the Uber-Anatomy, Cell Ontology, and Cell Line Ontology concepts (see [human-protein-atlas](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-protein-atlas) for details on the mapping process). The mappings are then used to create the following edge types:  \n",
    "- rna-cell line  \n",
    "- rna-tissue type   \n",
    "- protein-cell line  \n",
    "- protein-tissue type  \n",
    "\n",
    "\n",
    "**Output:**  \n",
    "- All HPA tissue and cell type strings ➞ `HPA_tissues.txt`  \n",
    "- Mapping HPA strings to ontology concepts (documentation) ➞ `zooma_tissue_cell_mapping_04JAN2020.xlsx` \n",
    "- Final HPA-ontology mappings ➞ `HPA_GTEx_TISSUE_CELL_MAP.txt`\n",
    "- HPA Edges ➞ `HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Human Protein Atlas**  \n",
    "To expedite the mapping process, all HPA tissues, cells, cell lines, and fluid types are extracted from the HPA data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.proteinatlas.org/api/search_download.php?search=&columns=g,eg,up,pe,rnatsm,rnaclsm,rnacasm,rnabrsm,rnabcsm,rnablsm,scl,t_RNA_adipose_tissue,t_RNA_adrenal_gland,t_RNA_amygdala,t_RNA_appendix,t_RNA_basal_ganglia,t_RNA_bone_marrow,t_RNA_breast,t_RNA_cerebellum,t_RNA_cerebral_cortex,t_RNA_cervix,_uterine,t_RNA_colon,t_RNA_corpus_callosum,t_RNA_ductus_deferens,t_RNA_duodenum,t_RNA_endometrium_1,t_RNA_epididymis,t_RNA_esophagus,t_RNA_fallopian_tube,t_RNA_gallbladder,t_RNA_heart_muscle,t_RNA_hippocampal_formation,t_RNA_hypothalamus,t_RNA_kidney,t_RNA_liver,t_RNA_lung,t_RNA_lymph_node,t_RNA_midbrain,t_RNA_olfactory_region,t_RNA_ovary,t_RNA_pancreas,t_RNA_parathyroid_gland,t_RNA_pituitary_gland,t_RNA_placenta,t_RNA_pons_and_medulla,t_RNA_prostate,t_RNA_rectum,t_RNA_retina,t_RNA_salivary_gland,t_RNA_seminal_vesicle,t_RNA_skeletal_muscle,t_RNA_skin_1,t_RNA_small_intestine,t_RNA_smooth_muscle,t_RNA_spinal_cord,t_RNA_spleen,t_RNA_stomach_1,t_RNA_testis,t_RNA_thalamus,t_RNA_thymus,t_RNA_thyroid_gland,t_RNA_tongue,t_RNA_tonsil,t_RNA_urinary_bladder,t_RNA_vagina,t_RNA_B-cells,t_RNA_dendritic_cells,t_RNA_granulocytes,t_RNA_monocytes,t_RNA_NK-cells,t_RNA_T-cells,t_RNA_total_PBMC,cell_RNA_A-431,cell_RNA_A549,cell_RNA_AF22,cell_RNA_AN3-CA,cell_RNA_ASC_diff,cell_RNA_ASC_TERT1,cell_RNA_BEWO,cell_RNA_BJ,cell_RNA_BJ_hTERT+,cell_RNA_BJ_hTERT+_SV40_Large_T+,cell_RNA_BJ_hTERT+_SV40_Large_T+_RasG12V,cell_RNA_CACO-2,cell_RNA_CAPAN-2,cell_RNA_Daudi,cell_RNA_EFO-21,cell_RNA_fHDF/TERT166,cell_RNA_HaCaT,cell_RNA_HAP1,cell_RNA_HBEC3-KT,cell_RNA_HBF_TERT88,cell_RNA_HDLM-2,cell_RNA_HEK_293,cell_RNA_HEL,cell_RNA_HeLa,cell_RNA_Hep_G2,cell_RNA_HHSteC,cell_RNA_HL-60,cell_RNA_HMC-1,cell_RNA_HSkMC,cell_RNA_hTCEpi,cell_RNA_hTEC/SVTERT24-B,cell_RNA_hTERT-HME1,cell_RNA_HUVEC_TERT2,cell_RNA_K-562,cell_RNA_Karpas-707,cell_RNA_LHCN-M2,cell_RNA_MCF7,cell_RNA_MOLT-4,cell_RNA_NB-4,cell_RNA_NTERA-2,cell_RNA_PC-3,cell_RNA_REH,cell_RNA_RH-30,cell_RNA_RPMI-8226,cell_RNA_RPTEC_TERT1,cell_RNA_RT4,cell_RNA_SCLC-21H,cell_RNA_SH-SY5Y,cell_RNA_SiHa,cell_RNA_SK-BR-3,cell_RNA_SK-MEL-30,cell_RNA_T-47d,cell_RNA_THP-1,cell_RNA_TIME,cell_RNA_U-138_MG,cell_RNA_U-2_OS,cell_RNA_U-2197,cell_RNA_U-251_MG,cell_RNA_U-266/70,cell_RNA_U-266/84,cell_RNA_U-698,cell_RNA_U-87_MG,cell_RNA_U-937,cell_RNA_WM-115,blood_RNA_basophil,blood_RNA_classical_monocyte,blood_RNA_eosinophil,blood_RNA_gdT-cell,blood_RNA_intermediate_monocyte,blood_RNA_MAIT_T-cell,blood_RNA_memory_B-cell,blood_RNA_memory_CD4_T-cell,blood_RNA_memory_CD8_T-cell,blood_RNA_myeloid_DC,blood_RNA_naive_B-cell,blood_RNA_naive_CD4_T-cell,blood_RNA_naive_CD8_T-cell,blood_RNA_neutrophil,blood_RNA_NK-cell,blood_RNA_non-classical_monocyte,blood_RNA_plasmacytoid_DC,blood_RNA_T-reg,blood_RNA_total_PBMC,brain_RNA_amygdala,brain_RNA_basal_ganglia,brain_RNA_cerebellum,brain_RNA_cerebral_cortex,brain_RNA_hippocampal_formation,brain_RNA_hypothalamus,brain_RNA_midbrain,brain_RNA_olfactory_region,brain_RNA_pons_and_medulla,brain_RNA_thalamus&format=tsv'\n",
    "if not os.path.exists(unprocessed_data_location + 'proteinatlas_search.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location, 'proteinatlas_search.tsv.gz')\n",
    "\n",
    "# load data\n",
    "hpa = pandas.read_csv(unprocessed_data_location + 'proteinatlas_search.tsv', header=0, delimiter='\\t')\n",
    "hpa.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve terms to map and write results\n",
    "with open(unprocessed_data_location + 'HPA_tissues.txt', 'w') as outfile:\n",
    "    for x in tqdm(list(hpa.columns)):\n",
    "        if x.endswith('[nTPM]'):\n",
    "            outfile.write(x.split('RNA - ')[-1].split(' [nTPM]')[:-1][0] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Genotype-Tissue Expression Project**  \n",
    "Import the tissues, cells, cell lines, and fluids that we externally mapped from HPA and GTEx data to [UBERON](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#uber-anatomy-ontology), the [Cell Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#cell-ontology), and the [Cell Line Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#cell-line-ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url='https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "gtex = pandas.read_csv(unprocessed_data_location + 'GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_median_tpm.gct', header=0, skiprows=2, delimiter='\\t')\n",
    "gtex.fillna('None', inplace=True)  # replace NaN with 'None'\n",
    "gtex['Name'] = gtex['Name'].str.replace('(\\..*)','', regex=True)  # remove identifier type, which appears after '.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url='https://storage.googleapis.com/pheknowlator/curated_data/zooma_tissue_cell_mapping_04JAN2020.xlsx'\n",
    "if not os.path.exists(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load ontology mapping data\n",
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'zooma_tissue_cell_mapping_04JAN2020.xlsx', 'rb'),\n",
    "                                 sheet_name='Concept_Mapping - 04JAN2020', header=0, engine='openpyxl')\n",
    "mapping_data.fillna('None', inplace=True)  # convert NaN to None\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write HPA and GTEx Mapping Data_  \n",
    "The HPA and GTEx mapping data is written locally so that it can be used by the `PheKnowLator` algorithm when creating the knowledge graph edge lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', 'w') as out:\n",
    "    for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "        if row['UBERON'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['UBERON']).strip() + '\\n')\n",
    "        if row['CL'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['CL']).strip() + '\\n')\n",
    "        if row['CLO'] != 'None': out.write(str(row['TERM']).strip() + '\\t' + str(row['CLO']).strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mapping data\n",
    "mapping_data = pandas.read_csv(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', header=None, names=['TISSUE_CELL_TERM', 'ONTOLOGY_IDs'], delimiter='\\t')\n",
    "\n",
    "# preview data\n",
    "mapping_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Create Edge Data Set**\n",
    "\n",
    "_Human Protein Atlas_  \n",
    "hpaThe `HPA` data is looped over and reformatted such that all tissue, cell, cell lines, and fluid types are stored as a nested list. The anatomy type is specified as an item in the list according to its type in order to make mapping more efficient while building the knowledge graph edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_results = []\n",
    "for idx, row in tqdm(hpa.iterrows(), total=hpa.shape[0]):\n",
    "    ens = str(row['Ensembl']); gene = str(row['Gene']); uni = str(row['Uniprot'])\n",
    "    evid = str(row['Evidence']); sub = str(row['Subcellular location']); source = 'The Human Protein Atlas'\n",
    "    if row['RNA tissue specific nTPM'] != 'None':\n",
    "        row_val = row['RNA tissue specific nTPM']\n",
    "        if ';' in row_val:\n",
    "            for x in row_val.split(';'):\n",
    "                x1 = str(x.split(':')[0]); x2 = float(x.split(': ')[1])\n",
    "                hpa_results += [ [ens, gene, uni, evid, 'anatomy', 'None', x1, x2, source]]\n",
    "        else:\n",
    "            x1 = str(row_val.split(':')[0]); x2 = float(row_val.split(': ')[1])\n",
    "            hpa_results += [[ens, gene, uni, evid, 'anatomy', 'None', x1, x2, source]]\n",
    "    if row['RNA cell line specific nTPM'] != 'None':\n",
    "        row_val = row['RNA cell line specific nTPM']\n",
    "        if ';' in row_val:\n",
    "            for x in row_val.split(';'):\n",
    "                x1 = str(x.split(':')[0]); x2 = float(x.split(': ')[1])\n",
    "                hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]\n",
    "        else:\n",
    "            x1 = str(row_val.split(':')[0]); x2 = float(row_val.split(': ')[1])\n",
    "            hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]\n",
    "    if row['RNA brain regional specific nTPM'] != 'None':\n",
    "        row_val = row['RNA brain regional specific nTPM']\n",
    "        if ';' in row_val:\n",
    "            for x in row_val.split(';'):\n",
    "                x1 = str(x.split(':')[0]); x2 = float(x.split(': ')[1])\n",
    "                hpa_results += [[ens, gene, uni, evid, 'anatomy', 'None', x1, x2, source]]\n",
    "        else:\n",
    "            x1 = str(row_val.split(':')[0]); x2 = float(row_val.split(': ')[1])\n",
    "            hpa_results += [[ens, gene, uni, evid, 'anatomy', 'None', x1, x2, source]]\n",
    "    if row['RNA blood cell specific nTPM'] != 'None':\n",
    "        row_val = row['RNA blood cell specific nTPM']\n",
    "        if ';' in row_val:\n",
    "            for x in row_val.split(';'):\n",
    "                x1 = str(x.split(':')[0]); x2 = float(x.split(': ')[1])\n",
    "                hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]\n",
    "        else:\n",
    "            x1 = str(row_val.split(':')[0]); x2 = float(row_val.split(': ')[1])\n",
    "            hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]\n",
    "    if row['RNA blood lineage specific nTPM'] != 'None':\n",
    "        row_val = row['RNA blood lineage specific nTPM']\n",
    "        if ';' in row_val:\n",
    "            for x in row_val.split(';'):\n",
    "                x1 = str(x.split(':')[0]); x2 = float(x.split(': ')[1])\n",
    "                hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]\n",
    "        else:\n",
    "            x1 = str(row_val.split(':')[0]); x2 = float(row_val.split(': ')[1])\n",
    "            hpa_results += [[ens, gene, uni, evid, 'cell line', sub, x1, x2, source]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Genotype-Tissue Expression Project_  \n",
    "The `GTEx` edge data is created by first filtering out all _protein-coding_ genes that appear in the `HPA` cell transcriptome data set. Once filter so that we are only left noncoding genes, we perform an additional filtering step to only add genes and their corresponding tissue, cell, or fluid, if the median expression is `>= 1.0`. The `GTEx` is formatted such that all anatomical entities occur as their own column and all unique genes occur as a row, thus the expression filtering step is performed while also reformatting the file. The genes and tissues/cells/fluids that meet criteria are stored as a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that contain protein coding genes already in the hpa data\n",
    "hpa_genes = list(hpa['Ensembl'].drop_duplicates(keep='first', inplace=False))\n",
    "gtex = gtex.loc[gtex['Name'].apply(lambda x: x not in hpa_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over data and re-organize - only keep results with tpm >= 1 and if gene symbol is not a protein-coding gene\n",
    "gtex_results = []\n",
    "source = 'Genotype-Tissue Expression (GTEx) Project'\n",
    "for idx, row in tqdm(gtex.iterrows(), total=gtex.shape[0]):\n",
    "    for col in list(gtex.columns)[2:]:\n",
    "        typ = 'cell line' if 'Cells' in col else 'anatomy'\n",
    "        evidence = 'Evidence at transcript level'\n",
    "        gtex_results += [[str(row['Name']), str(row['Description']), 'None', evidence, typ, 'None', col, float(row[col]), source]]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Writes Edge Data*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt', 'w') as out:\n",
    "    for x in tqdm(hpa_results + gtex_results):\n",
    "        out.write(x[0] + '\\t' + x[1] + '\\t' + x[2] + '\\t' + x[3] + '\\t' + x[4] + '\\t' + x[5] + '\\t' + x[6] + '\\t' + str(x[7]) + '\\t' + x[8] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, return edge count, and preview it\n",
    "hpa_edges = pandas.read_csv(processed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt',\n",
    "                           header=None, low_memory=False, sep='\\t',\n",
    "                           names=['Ensembl_IDs', 'Gene_Symbols', 'Uniprot_IDs', 'Evidence',\n",
    "                                  'Anatomy_Type', 'Subcellular_Location', 'Anatomy', 'Expresison_Value',\n",
    "                                 'Source'])\n",
    "\n",
    "print('There are {edge_count} edges'.format(edge_count=len(hpa_edges)))\n",
    "hpa_edges.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Mapping Reactome Pathways to the Pathway Ontology <a class=\"anchor\" id=\"reactome-pw\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Pathway Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#pathway-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [canonical pathways](http://compath.scai.fraunhofer.de/export_mappings) and [kegg-reactome pathway mappings](https://github.com/ComPath/resources/blob/master/mappings/kegg_reactome.csv) files from the [ComPath Ecosystem](https://github.com/ComPath) in order to create the following identifier mappings:  \n",
    "- `Reactome Pathway Identifiers`  ➞ `KEGG Pathway Identifiers` ➞ `Pathway Ontology Identifiers` \n",
    "\n",
    "**Output:**  \n",
    "- `REACTOME_PW_GO_MAPPINGS.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Pathway Ontology**   \n",
    "Use [OWL Tools](https://github.com/owlcollab/owltools/wiki) to download the [Pathway Ontology](http://www.obofoundry.org/ontology/pw.html). Once downloaded, we read the ontology in as a `RDFLib` graph object so that we can query it to obtain all `DbXRefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'pw_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/pw.owl',\n",
    "                             unprocessed_data_location + 'pw_with_imports.owl'))\n",
    "\n",
    "# load the knowledge graph\n",
    "pw_graph = Graph().parse(unprocessed_data_location + 'pw_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(pw_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reformat Mapping Results_  \n",
    "Create a dictionary of mapping results where pathway ontology identifiers are values and the keys are `DbXRef` identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dbxref results\n",
    "dbxref_res = gets_ontology_class_dbxrefs(pw_graph)[0]\n",
    "dbxref_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in dbxref_res.items() if 'PW_' in str(v)}\n",
    "\n",
    "# get synonym results\n",
    "syn_res = gets_ontology_class_synonyms(pw_graph)[0]\n",
    "synonym_dict = {str(k).lower().split('/')[-1]: {str(i).split('/')[-1].replace('_', ':') for i in v} for k, v in syn_res.items() if 'PW_' in str(v)}\n",
    "\n",
    "# combine results into single dictionary\n",
    "id_mappings = {**dbxref_dict, **synonym_dict}\n",
    "\n",
    "print('There are {} results (date: {})'.format(len(id_mappings), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Reactome Pathways**  \n",
    "Download a file of all [Reactome Pathways](https://reactome.org/download/current/ReactomePathways.txt), [Reactome's GO Annotations]('https://reactome.org/download/current/gene_association.reactome.gz'), and [Reactome's mappings to CHEBI](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt). This file will be filtered to only include human pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome Pathway Stable Identifiers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/ReactomePathways.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'ReactomePathways.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "reactome_pathways = pandas.read_csv(unprocessed_data_location + 'ReactomePathways.txt', header=None, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways = reactome_pathways.loc[reactome_pathways[2].apply(lambda x: x == 'Homo sapiens')] \n",
    "reactome_map = {x:set(['PW_0000001']) for x in set(list(reactome_pathways[0]))}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome's Mappings to GO Annotations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/gene_association.reactome.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'gene_association.reactome'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "reactome_pathways2 = pandas.read_csv(unprocessed_data_location + 'gene_association.reactome', header=None, delimiter='\\t', skiprows=4, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways2 = reactome_pathways2.loc[reactome_pathways2[12].apply(lambda x: x == 'taxon:9606')] \n",
    "reactome_map.update({x.split(':')[-1]:set(['PW_0000001']) for x in set(list(reactome_pathways2[5]))})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Reactome's Mappings to ChEBI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "reactome_pathways3 = pandas.read_csv(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt', header=None, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways3 = reactome_pathways3.loc[reactome_pathways3[5].apply(lambda x: x == 'Homo sapiens')] \n",
    "reactome_map.update({x:set(['PW_0000001']) for x in set(list(reactome_pathways3[1]))})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**ComPath Reactome Pathway Mappings**  \n",
    "Use [ComPath Mappings](https://github.com/ComPath/resources/tree/master/mappings) to obtain the following mappings:  `Reactome Pathways`  ➞ `KEGG Pathways` ➞ `Pathway Ontology` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Canonical Pathways_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url1 = 'http://compath.scai.fraunhofer.de/export_mappings'\n",
    "if not os.path.exists(unprocessed_data_location + 'compath_canonical_pathway_mappings.txt'):\n",
    "    data_downloader(url1, unprocessed_data_location, 'compath_canonical_pathway_mappings.txt')\n",
    "\n",
    "# load data\n",
    "compath_cannonical = pandas.read_csv(unprocessed_data_location + 'compath_canonical_pathway_mappings.txt', header=None, delimiter='\\t', low_memory=False)\n",
    "compath_cannonical.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(compath_cannonical.iterrows(), total=compath_cannonical.shape[0]):\n",
    "    if row[6] == 'kegg' and 'kegg:' + row[5].strip('path:hsa') in id_mappings.keys() and row[2] == 'reactome':\n",
    "        for x in id_mappings['kegg:' + row[5].strip('path:hsa')]:\n",
    "            if row[1] in reactome_map.keys(): reactome_map[row[1]] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row[1]] = set([x.split('/')[-1]])\n",
    "    if (row[2] == 'kegg' and 'kegg:' + row[1].strip('path:hsa') in id_mappings.keys()) and row[6] == 'reactome':\n",
    "        for x in id_mappings['kegg:' + row[1].strip('path:hsa')]:\n",
    "            if row[5] in reactome_map.keys(): reactome_map[row[5]] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row[5]] = set([x.split('/')[-1]])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_KEGG - Reactome Mappings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url2 = 'https://raw.githubusercontent.com/ComPath/resources/master/mappings/kegg_reactome.csv'\n",
    "if not os.path.exists(unprocessed_data_location + 'kegg_reactome.csv'):\n",
    "    data_downloader(url2, unprocessed_data_location, 'kegg_reactome.csv')\n",
    "\n",
    "# load data\n",
    "kegg_reactome_map = pandas.read_csv(unprocessed_data_location + 'kegg_reactome.csv', header=0, delimiter=',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(kegg_reactome_map.iterrows(), total=kegg_reactome_map.shape[0]):\n",
    "    if row['Source Resource'] == 'reactome' and 'kegg:' + row['Target ID'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['kegg:' + row['Target ID'].strip('path:hsa')]:\n",
    "            if row['Source ID'] in reactome_map.keys(): reactome_map[row['Source ID']] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row['Source ID']] = set([x.split('/')[-1]])\n",
    "    if row['Target Resource'] == 'reactome' and 'kegg:' + row['Source Resource'].strip('path:hsa') in id_mappings.keys():\n",
    "        for x in id_mappings['kegg:' + row['Source ID'].strip('path:hsa')]:\n",
    "            if row['Target ID'] in reactome_map.keys(): reactome_map[row['Target ID']] |= set([x.split('/')[-1]])\n",
    "            else: reactome_map[row['Target ID']] = set([x.split('/')[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Reactome Pathway GO Annotation Mappings**  \n",
    "Use Reactome's [API](https://reactome.org/dev/content-service) to obtain the following mappings: `Reactome Pathway Identifiers`  ➞ `Gene Ontology Identifiers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for request_ids in tqdm(list(chunks(list(reactome_map.keys()), 20))):\n",
    "    result, key = content.query_ids(ids=','.join(request_ids)), 'goBiologicalProcess'\n",
    "    if result is not None and (isinstance(result, List) or result['code'] != 404):\n",
    "        for res in result:\n",
    "            if key in res.keys():\n",
    "                if res['stId'] in reactome_map.keys(): reactome_map[res['stId']] |= {'GO_' + res[key]['accession']}\n",
    "                else: reactome_map[res['stId']] = {'GO_' + res[key]['accession']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat identifiers -- replacing ontology concepts with ':' to '_'\n",
    "temp_dict = dict()\n",
    "for key, value in tqdm(reactome_map.items()):\n",
    "    temp_dict[key] = set(x.replace(':', '_') for x in value)\n",
    "\n",
    "# overwrite original reactome dict with cleaned mappings\n",
    "reactome_map = temp_dict\n",
    "\n",
    "# output data\n",
    "with open(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt', 'w') as out:\n",
    "    for key in tqdm(reactome_map.keys()):\n",
    "        for x in reactome_map[key]:\n",
    "            if x.startswith('PW') or x.startswith('GO'): out.write(key + '\\t' + x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "pw_data = pandas.read_csv(processed_data_location + 'REACTOME_PW_GO_MAPPINGS.txt', header=None, names=['Pathway_IDs', 'Mapping_IDs'], delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} pathway ontology mappings'.format(edge_count=len(pw_data)))\n",
    "pw_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "### Mapping Genomic Identifiers to the Sequence Ontology <a class=\"anchor\" id=\"genomic-soo\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Sequence Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources/_edit#sequence-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the `genomic_sequence_ontology_mappings.xlsx` file in order to create the following identifier mappings:  \n",
    "- `Gene BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `RNA BioTypes`  ➞ `Sequence Ontology Identifiers`  \n",
    "- `variant Types`  ➞ `Sequence Ontology Identifiers`\n",
    "\n",
    "**Output:**  \n",
    "- `SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url='https://storage.googleapis.com/pheknowlator/curated_data/genomic_sequence_ontology_mappings.xlsx'\n",
    "if not os.path.exists(unprocessed_data_location + 'genomic_sequence_ontology_mappings.xlsx'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "mapping_data = pandas.read_excel(open(unprocessed_data_location + 'genomic_sequence_ontology_mappings.xlsx', 'rb'),\n",
    "                                 sheet_name='GenomicType_SO_Map_09Mar2020', header=0, engine='openpyxl')\n",
    "\n",
    "# convert data to dictionary\n",
    "genomic_type_so_map = {}\n",
    "for idx, row in tqdm(mapping_data.iterrows(), total=mapping_data.shape[0]):\n",
    "    genomic_type_so_map[row['source_*_type'] + '_' + row['Genomic']] = row['SO ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Genes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in genomic mapping data\n",
    "genomic_mapped_ids = pickle.load(open(processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl', 'rb'))\n",
    "\n",
    "sequence_map = {}\n",
    "for identifier in tqdm(genomic_mapped_ids.keys()):    \n",
    "    if identifier.startswith('entrez_id_') and identifier.replace('entrez_id_', '') != 'None':\n",
    "        id_clean = identifier.replace('entrez_id_', '')\n",
    "        \n",
    "        # get identifier types\n",
    "        ensembl = [x.replace('ensembl_gene_type_', '') for x in genomic_mapped_ids[identifier] if x.startswith('ensembl_gene_type') and x != 'ensembl_gene_type_unknown']\n",
    "        hgnc = [x.replace('hgnc_gene_type_', '')  for x in genomic_mapped_ids[identifier] if x.startswith('hgnc_gene_type') and x != 'hgnc_gene_type_unknown']\n",
    "        entrez = [x.replace('entrez_gene_type_', '')  for x in genomic_mapped_ids[identifier] if x.startswith('entrez_gene_type') and x != 'entrez_gene_type_unknown']\n",
    "        \n",
    "        # determine gene type\n",
    "        if len(ensembl) > 0: gene_type = genomic_type_so_map[ensembl[0].replace('ensembl_gene_type_', '') + '_Gene']\n",
    "        elif len(hgnc) > 0: gene_type = genomic_type_so_map[hgnc[0].replace('hgnc_gene_type_', '') + '_Gene']\n",
    "        elif len(entrez) > 0: gene_type = genomic_type_so_map[entrez[0].replace('entrez_gene_type_', '') + '_Gene']\n",
    "        else: gene_type = 'SO_0000704'  \n",
    "        \n",
    "        # update sequence map\n",
    "        if id_clean in sequence_map.keys(): sequence_map[id_clean] += [gene_type]\n",
    "        else: sequence_map[id_clean] = [gene_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Transcripts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in processed Ensembl Transcript data \n",
    "transcript_data = pandas.read_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# convert to dictionary\n",
    "transcripts = {}\n",
    "for idx, row in tqdm(transcript_data.iterrows(), total=transcript_data.shape[0]):\n",
    "    if row['transcript_stable_id'] != 'None':\n",
    "        if row['transcript_stable_id'].replace('transcript_stable_id_', '') in transcripts.keys():\n",
    "            transcripts[row['transcript_stable_id'].replace('transcript_stable_id_', '')] += [row['ensembl_transcript_type']]\n",
    "        else: transcripts[row['transcript_stable_id'].replace('transcript_stable_id_', '')] = [row['ensembl_transcript_type']]\n",
    "            \n",
    "# update so map dictionary\n",
    "for identifier in tqdm(transcripts.keys()):\n",
    "    if transcripts[identifier][0] == 'protein_coding': trans_type = genomic_type_so_map['protein-coding_Transcript']\n",
    "    elif transcripts[identifier][0] == 'misc_RNA': trans_type = genomic_type_so_map['miscRNA_Transcript']\n",
    "    else: trans_type = genomic_type_so_map[list(set(transcripts[identifier]))[0] + '_Transcript']\n",
    "    sequence_map[identifier] = [trans_type, 'SO_0000673']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Variants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# read in variant summary data \n",
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'variant_summary.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "    \n",
    "# load data    \n",
    "variant_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# convert to dictionary\n",
    "variants = {}\n",
    "for idx, row in tqdm(variant_data.iterrows(), total=variant_data.shape[0]):\n",
    "    if row['Assembly'] == 'GRCh38' and row['RS# (dbSNP)'] != -1:\n",
    "        if 'rs' + str(row['RS# (dbSNP)']) in variants.keys(): variants['rs' + str(row['RS# (dbSNP)'])] |= set([row['Type']])\n",
    "        else: variants['rs' + str(row['RS# (dbSNP)'])] = set([row['Type']])\n",
    "\n",
    "# update so map dictionary\n",
    "for identifier in tqdm(variants.keys()):\n",
    "    for typ in variants[identifier]:\n",
    "        var_type = genomic_type_so_map[typ.lower() + '_Variant']\n",
    "        if identifier in sequence_map.keys(): sequence_map[identifier] += [var_type]\n",
    "        else: sequence_map[identifier] = [var_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "**Write Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt', 'w') as outfile:\n",
    "    for key in tqdm(sequence_map.keys()):\n",
    "        for map_type in sequence_map[key]:\n",
    "            outfile.write(key + '\\t' + map_type + '\\n')\n",
    "\n",
    "# load data, print row count, and preview it\n",
    "so_data = pandas.read_csv(processed_data_location + 'SO_GENE_TRANSCRIPT_VARIANT_TYPE_MAPPING.txt', header=None, delimiter='\\t', names=['Identifier', 'Sequence_Ontology_ID'])\n",
    "\n",
    "print('There are {edge_count} sequence ontology mappings'.format(edge_count=len(so_data)))\n",
    "so_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Combine Pathway and Sequence Ontology Mapping Data in Dictionary**  \n",
    "Combine the pathway and sequence mapping data into a dictionary and output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine genomic and pathway maps\n",
    "subclass_mapping = {}  \n",
    "sequence_map.update(reactome_map)\n",
    "\n",
    "# iterate over pathway lists and combine them\n",
    "for key in tqdm(sequence_map.keys()):\n",
    "    subclass_mapping[key] = sequence_map[key]\n",
    "\n",
    "# save a copy of the dictionary\n",
    "pickle.dump(subclass_mapping, open(construction_approach_location + 'subclass_construction_map.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "### CREATE EDGE DATASETS  <a class=\"anchor\" id=\"create-edge-datasets\"></a>\n",
    "***\n",
    "***\n",
    "\n",
    "### Ontologies  <a class=\"anchor\" id=\"ontologies\"></a>\n",
    "***\n",
    "- [Protein Ontology](#protein-ontology)  \n",
    "- [Relations Ontology](#relations-ontology)  \n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Protein Ontology <a class=\"anchor\" id=\"protein-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [protein-ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-phenotype-ontology)  \n",
    "\n",
    "**Purpose:** This script uses [OWLTools](https://github.com/owlcollab/owltools) to download the [pr.owl](http://purl.obolibrary.org/obo/pr.owl) (with imports) file from [ProConsortium.org](https://proconsortium.org/) in order to create a version of the ontology that contains only human proteins. This is achieved by performing forward and reverse breadth first search over all proteins which are `owl:subClassOf` [Homo sapiens protein](https://proconsortium.org/app/entry/PR%3A000029067/).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- Human Protein Ontology ➞ `human_pro.owl`\n",
    "- Classified Human Protein Ontology (Hermit) ➞ `human_pro_closed.owl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'pr_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/pr.owl',\n",
    "                             unprocessed_data_location + 'pr_with_imports.owl'))\n",
    "    \n",
    "# read in ontology as graph (the ontology is large so this takes ~60 minutes)\n",
    "print('Loading Protein Ontology')\n",
    "pr_graph = Graph().parse(unprocessed_data_location + 'pr_with_imports.owl')\n",
    "print('There are {} axioms in the ontology (date: {})'.format(len(pr_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Convert Ontology to Directed MulitGraph_  \n",
    "In order to create a version of the ontology which includes all relevant human edges, we need to first convert the KG to a [directed multigraph](https://networkx.github.io/documentation/stable/reference/classes/multidigraph.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkx_mdg: networkx.MultiDiGraph = networkx.MultiDiGraph()\n",
    "    \n",
    "for s, p, o in tqdm(pr_graph):\n",
    "    networkx_mdg.add_edge(s, o, **{'key': p})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Identify Human Proteins_   \n",
    "A list of human proteins is obtained by querying the ontology to return all ontology classes `only_in_taxon some Homo sapiens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Approach 1 - Query Loaded Graph to Obtain Human Protein Classes*  \n",
    "Does not require using external resources or SPARQL Endpoints. This is the preferred approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "human_classes_restriction = list(pr_graph.triples((None, OWL.someValuesFrom, obo.NCBITaxon_9606)))\n",
    "human_classes = [list(pr_graph.subjects(RDFS.subClassOf, x[0])) for x in human_classes_restriction]\n",
    "human_pro_classes = list(str(i) for j in human_classes for i in j if 'PR_' in str(i))\n",
    "\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(human_pro_classes), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Approach 2 - Query PRO Consortium SPARQL Endpoint to Obtain Human Protein Classes*  \n",
    "This approach should only be used when the PRO endpoint is not limiting the number of results that are returned. As of `October 2021`, this was happening so please use *Approach 1* which is guaranteed to return the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # download data\n",
    "# url = 'https://sparql.proconsortium.org/virtuoso/sparql?query=PREFIX+obo%3A+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F%3E%0D%0A%0D%0ASELECT+%3FPRO_term%0D%0AFROM+%3Chttp%3A%2F%2Fpurl.obolibrary.org%2Fobo%2Fpr%3E%0D%0AWHERE+%7B%0D%0A+++++++%3FPRO_term+rdf%3Atype+owl%3AClass+.%0D%0A+++++++%3FPRO_term+rdfs%3AsubClassOf+%3Frestriction+.%0D%0A+++++++%3Frestriction+owl%3AonProperty+obo%3ARO_0002160+.%0D%0A+++++++%3Frestriction+owl%3AsomeValuesFrom+obo%3ANCBITaxon_9606+.%0D%0A%0D%0A+++++++%23+use+this+to+filter-out+things+like+hgnc+ids%0D%0A+++++++FILTER+%28regex%28%3FPRO_term%2C%22http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2F*%22%29%29+.%0D%0A%7D&format=text%2Fhtml&debug='\n",
    "# if not os.path.exists(unprocessed_data_location + 'human_pro_classes.html'):\n",
    "#     data_downloader(url, unprocessed_data_location, 'human_pro_classes.html')\n",
    "\n",
    "# # load data\n",
    "# df_list = pandas.read_html(unprocessed_data_location + 'human_pro_classes.html')\n",
    "\n",
    "# # extract data from html table - pro classes only_in_taxon some Homo sapiens\n",
    "# human_pro_classes = list(df_list[-1]['PRO_term'])\n",
    "# print('There are {} edges in the ontology (date:{})'.format(len(human_pro_classes), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Construct Human PRO_   \n",
    "Now that we have all of the paths from the original graph that are relevant to humans, we can construct a human-only version of the PRotein Ontology. After building the human subset, we verify the number of connected components and get 1. However, after reformatting the graph using [OWLTools](https://github.com/owlcollab/owltools) you will see that there are 3 connected components: component 1 (n=`1051673`); component 2 (n=`12`); and component 3 (n=`2`). The contents of components 2 and 3 are shown below:\n",
    "\n",
    "```python\n",
    "[{'http://purl.obolibrary.org/obo/IAO_0000115',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasAlternativeId',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasDbXref',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasExactSynonym',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasOBONamespace',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#id',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#is_transitive',\n",
    "  'http://www.geneontology.org/formats/oboInOwl#shorthand',\n",
    "  'http://www.w3.org/2002/07/owl#AnnotationProperty'},\n",
    " \n",
    " {'N41f0be4cf00c48929605b1e69a09f326',\n",
    "  'http://www.w3.org/2002/07/owl#Ontology'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new graph using bfs paths\n",
    "human_pro_graph = Graph()\n",
    "human_networkx_mdg = networkx.MultiDiGraph()\n",
    "\n",
    "for node in tqdm(human_pro_classes):\n",
    "    forward = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='original'))\n",
    "    reverse = list(networkx.edge_bfs(networkx_mdg, URIRef(node), orientation='reverse'))\n",
    "    \n",
    "    # add edges from forward and reverse bfs paths\n",
    "    for path in set(forward + reverse):\n",
    "        human_pro_graph.add((path[0], path[2], path[1]))\n",
    "        human_networkx_mdg.add_edge(path[0], path[1], **{'key': path[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connected component information\n",
    "print('Finding Connected Components')\n",
    "components = list(networkx.connected_components(human_networkx_mdg.to_undirected()))\n",
    "component_dict = sorted(components, key=len, reverse=True)\n",
    "\n",
    "# if more than 1 connected component, only keep the biggest\n",
    "if len(component_dict) > 1:\n",
    "    print('Cleaning Graph: Removing Small Disconnected Components')\n",
    "    for node in tqdm([x for y in component_dict[1:] for x in list(y)]):\n",
    "        human_pro_graph.remove((node, None, None))\n",
    "\n",
    "# save data\n",
    "print('Saving Human Subset of the Protein Ontology')\n",
    "human_pro_graph.serialize(destination=unprocessed_data_location + 'human_pro.owl', format='xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Classify Ontology_  \n",
    "To ensure that we have correctly built the new ontology, we run the hermit reasoner over it to ensure that there are no incomplete triples or inconsistent classes. In order to do this, we will call the reasoner using [OWLTools](https://github.com/owlcollab/owltools), which this script assumes has already been downloaded to the `./resources/lib` directory. The following arguments are then called to run the reasoner (from the command line):  \n",
    "\n",
    "___\n",
    "\n",
    "```bash\n",
    "../pkt_kg/libs/owltools ./resources/processed_data/unprocessed_data/human_pro.owl --reasoner elk --run-reasoner --assert-implied -o ./resources/processed_data/human_pro_closed.owl\n",
    "```\n",
    "___\n",
    "\n",
    "\n",
    "_**Note.** This step takes around 5 minutes to run. When run from the command line the reasoner determined that the ontology was consistent and 200 new axioms were inferred (12/01/2020)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run reasoner\n",
    "command = '{} {} --reasoner {} --run-reasoner --assert-implied -o {}'\n",
    "os.system(command.format(owltools_location, unprocessed_data_location + 'human_pro.owl', 'elk',\n",
    "                         ontology_data_location + 'pr_with_imports.owl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Examine Cleaned Human PRO_  \n",
    "Once we have cleaned the ontology we can get counts of components, nodes, and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gets_ontology_statistics(ontology_data_location + 'pr_with_imports.owl', '../pkt_kg/libs/owltools')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "### Relations Ontology <a class=\"anchor\" id=\"relations-ontology\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Relations Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#relations-ontology)  \n",
    "\n",
    "**Purpose:** This script downloads the [ro.owl](http://purl.obolibrary.org/obo/ro.owl) file from [obofoundry.org](http://www.obofoundry.org/) in order to obtain all `ObjectProperties` and their inverse relations.  \n",
    "\n",
    "**Output:** \n",
    "- Relations and Inverse Relations ➞ `INVERSE_RELATIONS.txt`\n",
    "- Relations and Labels ➞ `RELATIONS_LABELS.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'ro_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/ro.owl',\n",
    "                             unprocessed_data_location + 'ro_with_imports.owl'))\n",
    "# load graph\n",
    "ro_graph = Graph().parse(unprocessed_data_location + 'ro_with_imports.owl')\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(ro_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Identify Relations and Inverse Relations**  \n",
    "Identify all relations and their inverse relations using the `owl:inverseOf` property. To make it easier to look up the inverse relations, each pair is listed twice, for example:  \n",
    "- [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015) `owl:inverseOf` [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025)  \n",
    "- [located in](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001025) `owl:inverseOf` [location of](http://www.ontobee.org/ontology/RO?iri=http://purl.obolibrary.org/obo/RO_0001015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relations_data_location + 'INVERSE_RELATIONS.txt', 'w') as outfile:\n",
    "    outfile.write('Relation' + '\\t' + 'Inverse_Relation' + '\\n')\n",
    "    for s, p, o in tqdm(ro_graph):\n",
    "        if 'owl#inverseOf' in str(p):\n",
    "            if 'RO' in str(s) and 'RO' in str(o):\n",
    "                outfile.write(str(s.split('/')[-1]) + '\\t' + str(o.split('/')[-1]) + '\\n')\n",
    "                outfile.write(str(o.split('/')[-1]) + '\\t' + str(s.split('/')[-1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "ro_data = pandas.read_csv(relations_data_location + 'INVERSE_RELATIONS.txt', header=0, delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Inverse Relations'.format(edge_count=len(ro_data)))\n",
    "ro_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Get Relations Labels**  \n",
    "Identify all relations and their labels for use when building the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {str(x[2]).lower(): str(x[0]) for x in ro_graph if '/RO_' in str(x[0]) and 'label' in str(x[1]).lower()}\n",
    "\n",
    "# write data to file\n",
    "with open(relations_data_location + 'RELATIONS_LABELS.txt', 'w') as outfile:\n",
    "    outfile.write('Label' + '\\t' + 'Relation' + '\\n')\n",
    "    for k, v in results.items():\n",
    "        outfile.write(str(v).split('/')[-1] + '\\t' + str(k) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preview Processed Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "ro_data_label = pandas.read_csv(relations_data_location + 'RELATIONS_LABELS.txt', header=0, delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} RO Relations and Labels'.format(edge_count=len(ro_data_label)))\n",
    "ro_data_label.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "### Linked Data <a class=\"anchor\" id=\"linked-data\"></a>\n",
    "***\n",
    "* [Clinvar Variant-Diseases and Phenotypes](#clinvar-variant) \n",
    "* [Uniprot Protein-Cofactor and Protein-Catalyst](#uniprot-protein-cofactorcatalyst)  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "### Clinvar Variant-Diseases and Phenotypes <a class=\"anchor\" id=\"clinvar-variant\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [Clinvar](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#clinvar)  \n",
    "\n",
    "**Purpose:** This script downloads the data files list below in order to create the following edges:  \n",
    "- gene-variant  \n",
    "- variant-disease  \n",
    "- variant-phenotype  \n",
    "\n",
    "**Data Files:**  \n",
    "Details on each file have been taken from this [README](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/README.txt) and are provided in relevant code chunks below.  \n",
    "##### *Core Data Files* <a class=\"anchor\" id=\"core-data-files\"></a>  \n",
    "- [`variant_summary.txt.gz`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)    \n",
    "\n",
    "##### *Metadata Files*<a class=\"anchor\" id=\"metadata-files\"></a>    \n",
    "- [`var_citations.txt`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/var_citations.txt)  \n",
    "- [`allele_gene.txt.gz`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/allele_gene.txt.gz)  \n",
    "\n",
    "**Output:**  \n",
    "- `CLINVAR_VARIANT_GENE_EDGES.txt`  \n",
    "- `CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Download and Process Core Data Files <a class=\"anchor\" id=\"core-data-files\"></a>\n",
    "***\n",
    "\n",
    "*Data Files:*  \n",
    "- [`variant_summary.txt.gz`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)  \n",
    "\n",
    "*Processing Details*  \n",
    "The first step is down the `variant_summary.txt.gz` file. After downloading, the file is cleaned to handle missing data, unneeded variables are removed, identifiers and date fields are cleaned and reformatted, and rows without valid disease/phenotype identifiers are removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[**`variant_summary.txt.gz`**](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)\n",
    "\n",
    "> A tab-delimited report based on each variant at a location on the genome for which data have been submitted to ClinVar.  \n",
    "The data for the variant are reported for each assembly, so most variants have a line for GRCh37 (hg19) and another line for GRCh38 (hg38).\n",
    ">\n",
    ">  - <u>AlleleID</u>: integer value as stored in the AlleleID field in ClinVar  \n",
    ">  - <u>Type</u>: character, the type of variant represented by the AlleleID  \n",
    ">  - <u>Name</u>: character, ClinVar's preferred name for the record with this AlleleID  \n",
    ">  - <u>GeneID</u>: integer, GeneID in NCBI's Gene database, reported if there is a single gene, otherwise reported as -1. \n",
    ">  - <u>GeneSymbol</u>: character, comma-separated list of GeneIDs overlapping the variant  \n",
    ">  - <u>HGNC_ID</u>: string, of format HGNC:integer, reported if there is a single GeneID.  \n",
    ">  - <u>ClinicalSignificance</u>: character, comma-separated list of aggregate values of clinical significance calculated for this variant. \n",
    ">  - <u>ClinSigSimple</u>: integer,  \n",
    "               0 = no current value of Likely pathogenic or Pathogenic;\n",
    "               1 = at least one current record submitted with an interpretation of Likely pathogenic or          \n",
    "                   Pathogenic (independent of whether that record includes assertion criteria and \n",
    "                   evidence)  \n",
    "              -1 = no values for clinical significance at all for this variant or set of variants; \n",
    "                   used for the \"included\" variants that are only in ClinVar because they are included\n",
    "                   in a haplotype or genotype with an interpretation  \n",
    ">  - <u>LastEvaluated</u>: date, the latest date any submitter reported clinical significance  \n",
    ">  - <u>RS# (dbSNP)</u>: integer, rs# in dbSNP, reported as -1 if missing  \n",
    ">  - <u>nsv/esv (dbVar)</u>: character, the NSV identifier for the region in dbVar  \n",
    ">  - <u>RCVaccession</u>: character, list of RCV accessions that report this variant  \n",
    ">  - <u>PhenotypeIDs</u>: character, list of identifiers for phenotype(s) interpreted for this variant. If more than 5 conditions are reported, the number of conditions is reported instead.  \n",
    ">  - <u>PhenotypeList</u>: character, list of names corresponding to PhenotypeIDs. If more than 5 conditions are reported, the number of conditions is reported instead.  \n",
    ">  - <u>Origin</u>: character, list of all allelic origins for this variant  \n",
    ">  - <u>OriginSimple</u>: character, processed from Origin to make it easier to distinguish between germline and somatic  \n",
    ">  - <u>Assembly</u>: character, name of the assembly on which locations are based   \n",
    ">  - <u>ChromosomeAccession</u>: Accession and version of the RefSeq sequence defining the position reported in the start and stop columns.  \n",
    ">  - <u>Chromosome</u>: character, chromosomal location  \n",
    ">  - <u>Start</u>: integer, starting location, right-shifted, in pter->qter orientation  \n",
    ">  - <u>Stop</u>: integer, end location, right-shifted, in pter->qter orientation  \n",
    ">  - <u>ReferenceAllele</u>: The reference allele using the right-shifted location in Start and Stop.  \n",
    ">  - <u>AlternateAllele</u>: The alternate allele using the right-shifted location in Start and Stop.  \n",
    ">  - <u>Cytogenetic</u>: character, ISCN band\n",
    ">  - <u>ReviewStatus</u>: character, highest review status for reporting this measure.  \n",
    ">  - <u>NumberSubmitters</u>: integer, number of submitters describing this variant  \n",
    ">  - <u>Guidelines</u>: character, ACMG only right now  \n",
    ">  - <u>TestedInGTR</u>: character, Y/N for Yes/No if there is a test registered as specific to this variant in the NIH Genetic Testing Registry (GTR)  \n",
    ">  - <u>OtherIDs</u>: character, list of other identifiers or sources of information about this variant  \n",
    ">  - <u>SubmitterCategories</u>: coded value to indicate whether data were submitted by another resource (1), any other type of source (2), both (3), or none (4)  \n",
    ">  - <u>VariationID</u>: The identifier ClinVar uses specific to the AlleleID.  Not all VariationIDS that may be related to the AlleleID are reported in this file.  \n",
    ">  - <u>PositionVCF</u>: integer, starting location, left-shifted, in pter->qter orientation  \n",
    ">  - <u>ReferenceAlleleVCF</u>: The reference allele using the left-shifted location in vcf_pos.  \n",
    ">  - <u>AlternateAlleleVCF</u>: The alternate allele using the left-shifted location in vcf_pos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'variant_summary.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "var_summary = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt',\n",
    "                              header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"na\" and \"-\" with NaN\n",
    "var_summary = var_summary.replace('na', numpy.nan)\n",
    "var_summary = var_summary.replace('-', numpy.nan)\n",
    "\n",
    "# handle ids that are coded as missing (i.e., -1)\n",
    "var_summary['GeneID'] = var_summary['GeneID'].replace(-1, numpy.nan)\n",
    "var_summary['RS# (dbSNP)'] = var_summary['RS# (dbSNP)'].replace(-1, numpy.nan)\n",
    "\n",
    "# convert date format\n",
    "var_summary['LastEvaluated'] = var_summary['LastEvaluated'].str.replace('None', '')\n",
    "var_summary['LastEvaluated'] = pandas.to_datetime(var_summary['LastEvaluated'])\n",
    "var_summary['LastEvaluated'] = var_summary['LastEvaluated'].dt.strftime('%B %d, %Y')\n",
    "var_summary['LastEvaluated'] = var_summary['LastEvaluated'].replace('', numpy.nan)\n",
    "\n",
    "# rename variables\n",
    "var_summary.rename(columns={'#AlleleID': 'AlleleID',\n",
    "                           'nsv/esv (dbVar)': 'nsv',\n",
    "                           'Name': 'VariantName'}, inplace=True)\n",
    "\n",
    "# update variable types\n",
    "var_summary['GeneID'] = var_summary['GeneID'].astype('Int64')\n",
    "var_summary['RS# (dbSNP)'] = var_summary['RS# (dbSNP)'].astype('Int64')\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} variant edges'.format(edge_count=len(var_summary)))\n",
    "var_summary.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Address Duplicate Rows for GRCh37 and GRCh38 Assemblies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset df\n",
    "var_summary_update_assemb = var_summary.copy()\n",
    "var_summary_update_assemb = var_summary_update_assemb[['VariationID', 'Assembly', 'ChromosomeAccession',\n",
    "                                                       'Chromosome', 'Start', 'Stop', 'ReferenceAllele',\n",
    "                                                       'AlternateAllele', 'Cytogenetic', 'PositionVCF']].drop_duplicates()\n",
    "\n",
    "# identify columns to process\n",
    "assemb_cols = ['ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele',\n",
    "               'AlternateAllele','Cytogenetic', 'PositionVCF', 'ReferenceAlleleVCF', 'AlternateAlleleVCF']\n",
    "\n",
    "# group data by variant\n",
    "df = var_summary_update_assemb.fillna('None')\n",
    "df = df.groupby('VariationID').apply(lambda g: str(g.drop(['VariationID'], axis=1).to_dict('records'))).to_dict()\n",
    "\n",
    "# convert to Pandas DataFrame\n",
    "df_items = df.items()\n",
    "temp_df = pandas.DataFrame({'VariationID': [x[0] for x in df_items], 'Assembly': [x[1] for x in df_items]})\n",
    "\n",
    "# join temp df with original data\n",
    "var_summary_assemb = var_summary.copy().drop(assemb_cols + ['Assembly'], axis = 1)\n",
    "var_summary_update = var_summary_assemb.merge(temp_df, on='VariationID', how='left')\n",
    "\n",
    "# drop duplicates\n",
    "var_summary_update.drop_duplicates(inplace=True)\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_update)))\n",
    "var_summary_update.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Process `PhenotypeIDS` and `PhenotypeList` Columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# clean-up identifiers\n",
    "var_summary_update['Phenotype'] = var_summary_update['PhenotypeIDS'].str.replace('|', ';').str.replace(',', ';')\n",
    "var_summary_update['OtherIDs'] = var_summary_update['OtherIDs'].str.replace(';', '|').str.replace(',', '|')\n",
    "\n",
    "# remove unneeded variables\n",
    "drop_list = ['PhenotypeList', 'PhenotypeIDS']\n",
    "var_summary_update = var_summary_update.drop(drop_list, axis = 1).drop_duplicates()\n",
    "\n",
    "# replace NaN with 'None'\n",
    "var_summary_update['Phenotype'] = var_summary_update['Phenotype'].fillna('None')\n",
    "\n",
    "# reformat phenotypeIDS and trim leading whitespace from unnested columns\n",
    "var_summary_update['Phenotype'] = var_summary_update['Phenotype'].apply(\n",
    "    lambda x: ';'.join(set(x for x in ['MONDO:' + i.split(':')[-1] if i.startswith('MONDO')\n",
    "                        else 'HP:' + i.split(':')[-1] if i.startswith('Human Phenotype')\n",
    "                        else 'ORPHA:' + i.split(':')[-1] if i.startswith('Orphanet')\n",
    "                        else 'None' if i.endswith(' conditions')\n",
    "                        else i for i in x.split(';')] if x != 'None')))\n",
    "\n",
    "# drop duplicates\n",
    "var_summary_update.drop_duplicates(inplace=True)\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_update)))\n",
    "var_summary_update.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Metadata Files <a class=\"anchor\" id=\"metadata-files\"></a>\n",
    "***\n",
    "\n",
    "*Data Files:*  \n",
    "- [`var_citations.txt`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/var_citations.txt)  \n",
    "- [`allele_gene.txt.gz`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/allele_gene.txt.gz)  \n",
    "- [`gene_specific_summary.txt`](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/gene_specific_summary.txt)  \n",
    "\n",
    "*Processing Details*  \n",
    "<u>Step 1</u>: The first step is down the files. After downloading, the files are cleaned to handle missing data, unneeded variables are removed, and identifiers and date fields are cleaned and reformatted.  \n",
    "\n",
    "<u>Step 2</u>: Merge each cleaned file with the processed variant summary data from the prior steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[**`var_citations.txt`**](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/var_citations.txt)\n",
    "\n",
    "> A tab-delimited report of citations associated with data in ClinVar, connected to the AlleleID, the VariationID, and either rs# from dbSNP or nsv in dbVar.\n",
    ">\n",
    "> - <u>AlleleID</u>: integer value as stored in the AlleleID field in ClinVar  \n",
    "> - <u>VariationID</u>: The identifier ClinVar uses to anchor its default display  \n",
    "> - <u>rs</u>: rs identifier from dbSNP, null if missing  \n",
    "> - <u>nsv</u>: nsv identifier from dbVar, null if missing  \n",
    "> - <u>citation_source</u>: The source of the citation, either PubMed, PubMedCentral, or the NCBI Bookshelf  \n",
    "> - <u>citation_id</u>: The identifier used by that source  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/var_citations.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'var_citations.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "var_citations = pandas.read_csv(unprocessed_data_location + 'var_citations.txt',\n",
    "                                header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"na\" and \"-\" with NaN\n",
    "var_citations = var_citations.replace('na', numpy.nan)\n",
    "var_citations = var_citations.replace('-', numpy.nan)\n",
    "\n",
    "# combine citation information\n",
    "var_citations['Citation'] = var_citations['citation_source'] + ':' + var_citations['citation_id']\n",
    "# remove unneeded variables\n",
    "drop_list = ['citation_source', 'citation_id']\n",
    "var_citations = var_citations.drop(drop_list, axis = 1).drop_duplicates()\n",
    "\n",
    "# group data by citations\n",
    "var_citations = var_citations.groupby('VariationID').Citation.agg([('Citation', '|'.join)]).reset_index()\n",
    "var_citations = var_citations.drop_duplicates().sort_values(by=['VariationID'])\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_citations)))\n",
    "var_citations.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[**`allele_gene.txt.gz`**](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/allele_gene.txt.gz)\n",
    "\n",
    "> Reports per ClinVar's AlleleID, the genes that are related to that gene and how they are related.\n",
    ">\n",
    "> - <u>AlleleID</u>: the integer identifier assigned by ClinVar to each simple allele\n",
    "> - <u>GeneID</u>: integer, GeneID in NCBI's Gene database  \n",
    "> - <u>Symbol</u>: character, Symbol preferred in NCBI's Gene database. Is the symbol from HGNC when available  \n",
    "> - <u>Name</u>: character, full name of the gene  \n",
    "> - <u>GenesPerAlleleID</u>: integer, number of genes related to the allele  \n",
    "> - <u>Category</u>: character, type of allele-gene relationship. The values for category are:\n",
    ">   - <u>asserted, but not computed</u>: Submitted as related to a gene, but not within the location of that gene on the genome  \n",
    ">   - <u>genes overlapped by variant</u>: The gene and variant overlap  \n",
    ">   - <u>near gene, downstream</u>: Outside the location of the gene on the genome, within 5 kb  \n",
    ">   - <u>near gene, upstream</u>: Outside the location of the gene on the genome, within 5 kb  \n",
    ">   - <u>within multiple genes by overlap</u>: The variant is within genes that overlap on the genome. Includes introns  \n",
    ">   - <u>within single gene</u>: The variant is in only one gene. Includes introns    \n",
    "> - <u>Source</u>: character, was the relationship submitted or calculated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/allele_gene.txt.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'allele_gene.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "allele_gene = pandas.read_csv(unprocessed_data_location + 'allele_gene.txt',\n",
    "                              header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"na\" and \"-\" with NaN\n",
    "allele_gene = allele_gene.replace('na', numpy.nan)\n",
    "allele_gene = allele_gene.replace('-', numpy.nan)\n",
    "\n",
    "# handle gene ids that may be coded as -1\n",
    "allele_gene['GeneID'] = allele_gene['GeneID'].replace(-1, numpy.nan)\n",
    "\n",
    "# rename variables\n",
    "allele_gene.rename(columns={'#AlleleID': 'AlleleID',\n",
    "                           'Symbol': 'GeneSymbol',\n",
    "                           'Name': 'GeneName'}, inplace=True)\n",
    "\n",
    "# update variable types\n",
    "allele_gene['GeneID'] = allele_gene['GeneID'].astype('Int64')\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(allele_gene)))\n",
    "allele_gene.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Merge and Process Data Sources_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge `var_summary_update` with `var_citations` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "merge_cols = list(set(var_summary_update.columns).intersection(set(var_citations.columns)))\n",
    "var_summary_merged = var_summary_update.merge(var_citations, on=merge_cols, how='left')\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_merged)))\n",
    "var_summary_merged.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge merged `var_summary_update` with `allele_gene` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "merge_cols = list(set(var_summary_merged.columns).intersection(set(allele_gene.columns)))\n",
    "var_summary_merged = var_summary_merged.merge(allele_gene, on=merge_cols, how='left')\n",
    "\n",
    "# update variable types\n",
    "var_summary_merged['GenesPerAlleleID'] = var_summary_merged['GenesPerAlleleID'].astype('Int64')\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_merged)))\n",
    "var_summary_merged.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Edge Lists**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`variant`-`gene` Edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data set\n",
    "var_summary_merged_gene = var_summary_merged.copy()\n",
    "var_summary_merged_gene = var_summary_merged_gene[[\n",
    "    'VariationID', 'AlleleID', 'RS# (dbSNP)', 'Type', 'VariantName',\n",
    "    'OtherIDs', 'GeneID', 'GeneSymbol', 'GeneName', 'GenesPerAlleleID',\n",
    "    'Assembly', 'Category', 'Guidelines', 'TestedInGTR', 'RCVaccession', 'LastEvaluated',\n",
    "    'ReviewStatus', 'ClinicalSignificance', 'ClinSigSimple', 'Origin', 'OriginSimple', 'Source',\n",
    "    'SubmitterCategories', 'NumberSubmitters', 'Citation']]\n",
    "var_summary_merged_gene.drop_duplicates(inplace=True)\n",
    "\n",
    "# remove any rows missing a gene id\n",
    "var_summary_merged_gene = var_summary_merged_gene.dropna(subset=['GeneID'])\n",
    "\n",
    "# head prefix to output\n",
    "var_summary_merged_gene['GeneID'] = 'NCBIGene_' + var_summary_merged_gene['GeneID'].astype(str)\n",
    "var_summary_merged_gene['VariationID'] = 'clinvar_' + var_summary_merged_gene['VariationID'].astype(str)\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_merged_gene)))\n",
    "var_summary_merged_gene.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out data\n",
    "var_summary_merged_gene.to_csv(open(processed_data_location + 'CLINVAR_VARIANT_GENE_EDGES.txt', 'w'),\n",
    "                          index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`variant`-`disease` / `variant`-`phenotype` Edges*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data set\n",
    "var_summary_merged_disease = var_summary_merged.copy()\n",
    "var_summary_merged_disease = var_summary_merged_disease[[\n",
    "    'VariationID', 'AlleleID', 'RS# (dbSNP)', 'Type', 'VariantName', 'RCVaccession',\n",
    "    'LastEvaluated', 'ReviewStatus', 'ClinicalSignificance', 'ClinSigSimple', 'GeneID',\n",
    "    'NumberSubmitters', 'SubmitterCategories', 'Guidelines', 'TestedInGTR',\n",
    "    'Origin', 'OriginSimple', 'Assembly', 'Phenotype', 'Citation', 'OtherIDs']]\n",
    "var_summary_merged_disease.drop_duplicates(inplace=True)\n",
    "\n",
    "# expand results by disease identifier\n",
    "cols = ['Phenotype']\n",
    "for col in tqdm(cols): var_summary_merged_disease = var_summary_merged_disease.assign(**{col: var_summary_merged_disease[col].str.split(';')}).explode(col)\n",
    "    \n",
    "# remove phenotype rows with None and drop duplicates\n",
    "var_summary_merged_disease = var_summary_merged_disease[var_summary_merged_disease['Phenotype'] != 'None']\n",
    "var_summary_merged_disease.drop_duplicates(inplace=True)\n",
    "\n",
    "# head prefix to output\n",
    "var_summary_merged_disease['VariationID'] = 'clinvar_' + var_summary_merged_disease['VariationID'].astype(str)\n",
    "\n",
    "# print row count and preview data\n",
    "print('There are {edge_count} edges'.format(edge_count=len(var_summary_merged_disease)))\n",
    "var_summary_merged_disease.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to file\n",
    "var_summary_merged_disease.to_csv(open(processed_data_location + 'CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt', 'w'),\n",
    "                          index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "### Uniprot  Protein-Cofactor and Protein-Catalyst <a class=\"anchor\" id=\"uniprot-protein-cofactorcatalyst\"></a>\n",
    "\n",
    "**Data Source Wiki Page:** [UniProt](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#universal-protein-resource-knowledgebase)  \n",
    "\n",
    "**Purpose:** This script downloads the [uniprot-cofactor-catalyst.tab](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#universal-protein-resource-knowledgebase) file from the [Uniprot Knowledge Base](https://www.uniprot.org) in order to create the following edges:  \n",
    "- protein-cofactor  \n",
    "- protein-catalyst  \n",
    "\n",
    "**Data:** This data was obtained by querying the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) using the *reviewed:yes AND organism:\"Homo sapiens (Human) [9606]\"\"* keyword and including the following columns:\n",
    "- Entry (Standard) \n",
    "- Status (Standard) \n",
    "- PRO (*Miscellaneous*)  \n",
    "- ChEBI (Cofactor) (*Chemical entities*)   \n",
    "- ChEBI (Catalytic activity) (*Chemical entities*)  \n",
    "\n",
    "The URL to access the results of this query is obtained by clicking on the share symbol and copying the free-text from the box. To obtain the data in a tab-delimited format the following string is appended to the end of the URL: \"&format=tab\".\n",
    "\n",
    "**NOTE.** Be sure to obtain a new URL from the [UniProt Knowledgebase](https://www.uniprot.org/uniprot/) when rebuilding to ensure you are getting the most up-to-date data. This query was last generated on `12/02/2020`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Output:**  \n",
    "- protein-cofactor ➞ `UNIPROT_PROTEIN_COFACTOR.txt`\n",
    "- protein-catalyst ➞ `UNIPROT_PROTEIN_CATALYST.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.uniprot.org/uniprot/?query=&fil=organism%3A%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&columns=id%2Creviewed%2Centry%20name%2Cdatabase(PRO)%2Cchebi(Cofactor)%2Cchebi(Catalytic%20activity)&format=tab'\n",
    "if not os.path.exists(unprocessed_data_location + 'uniprot-cofactor-catalyst.tab'):\n",
    "    data_downloader(url, unprocessed_data_location, 'uniprot-cofactor-catalyst.tab')\n",
    "\n",
    "# upload data\n",
    "data = open(unprocessed_data_location + 'uniprot-cofactor-catalyst.tab').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data and write it out\n",
    "with open(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', 'w') as outfile1, open(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', 'w') as outfile2:\n",
    "    for line in tqdm(data):\n",
    "        status = line.split('\\t')[1]; upt_id = line.split('\\t')[0]; upt_entry = line.split('\\t')[2]\n",
    "        pr_id = 'PR_' + line.split('\\t')[3].strip(';')\n",
    "        # get cofactors\n",
    "        if 'CHEBI' in line.split('\\t')[4]: \n",
    "            for i in line.split('\\t')[4].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile1.write(pr_id + '\\t' + chebi + '\\t' + status + '\\t' + upt_id + '\\t' + upt_entry + '\\n')\n",
    "        # get catalysts\n",
    "        if 'CHEBI' in line.split('\\t')[5]:       \n",
    "            for i in line.strip('\\n').split('\\t')[5].split(';'):\n",
    "                chebi = i.split('[')[-1].replace(']', '').replace(':', '_')\n",
    "                outfile2.write(pr_id + '\\t' + chebi + '\\t' + status + '\\t' + upt_id + '\\t' + upt_entry + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Cofactor Data**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "pcp1_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', header=None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs', 'Status', 'Uniprot_ID', 'Uniprot_Entry_name'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} protein-cofactor edges'.format(edge_count=len(pcp1_data)))\n",
    "pcp1_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "\n",
    "**Catalyst Data**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, print row count, and preview it\n",
    "pcp2_data = pandas.read_csv(processed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', header=None,\n",
    "                            names=['Protein_Ontology_IDs', 'CHEBI_IDs', 'Status', 'Uniprot_ID', 'Uniprot_Entry_name'],\n",
    "                            delimiter='\\t')\n",
    "\n",
    "print('There are {edge_count} protein-catalyst edges'.format(edge_count=len(pcp2_data)))\n",
    "pcp2_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "### NODE AND RELATION METADATA<a class=\"anchor\" id=\"node-relation-metadata\"></a>\n",
    "***\n",
    "\n",
    "**Data Source Wiki Page:** [Dependencies](https://github.com/callahantiff/PheKnowLator/wiki/Dependencies/#metadata) \n",
    "\n",
    "**Purpose:** The goal of this section is to obtain metadata for each entity that is not from an ontology and all relations used in the knowledge graph. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Metadata:**  \n",
    "A variety of <u>metadata</u> are pulled from the data sources that are used to support external edges added to enhance the core set of ontologies. For the monthly PheknowLator builds, please see [`pheknowlator_source_metadata.xlsx`](https://github.com/callahantiff/PheKnowLator/blob/master/resources/pheknowlator_source_metadata.xlsx) spreadsheet. This spreadsheet has two tabs, one for nodes and one for edges. Each each entity (i.e., node or relation) there are several columns, including descriptions of the metadata, the variable type, and even examples of values for each type of metadata. \n",
    "\n",
    "*Example Metadata Dictionary Output*. The code snippet below is meant to provide a snapshot of how data are organized in the metadata dictionary. As demonstrated by this example, there are three high-level keys:  \n",
    "  - `nodes`: Nodes are keyed by CURIE. Every node has a `Label`, `Description`, `Synonym`, and `Dbxref` (whenever possible). Metadata that are obtained from specific sources that are not ontologies are added as a nested dictionary keyed by the filename.   \n",
    "  - `edges`: Edges are keyed by a label which represents the edge type (the same label that is used in `resource_info.txt` and `edge_source_list.txt` files). Metadata that are obtained from specific sources that are not ontologies are added as a nested dictionary keyed by the filename.    \n",
    "  - `relations`: Relations or `owl:ObjectProperty` objects are keyed by CURIE. Similar to nodes, every relation has a `Label`, `Description`, and `Synonym` (whenever possible). Metadata that are obtained from specific sources that are not ontologies are added as a nested dictionary keyed by the filename.     \n",
    "\n",
    "```python\n",
    "{\n",
    "    'nodes': {\n",
    "        'NCBIGene_2052': {\n",
    "            'Label': 'EPHX1',\n",
    "            'Description': \"EPHX1 has locus group 'protein-coding' and is located on chromosome 1 (1q42.12).\",\n",
    "            'Synonym': 'epoxide hydrolase 1, microsomal (xenobiotic)|epoxide hydratase|EPHX|HYL1|MEHepoxide hydrolase 1|epoxide hydrolase 1 microsomal|EPOX',\n",
    "            'Dbxref': 'MIM:132810|HGNC:HGNC:3401|Ensembl:ENSG00000143819', ... },\n",
    "        'CHEBI_4592': {\n",
    "            'Label': 'Dihydroxycarbazepine',\n",
    "            'Description': \"None\",\n",
    "            'Synonym': '10,11-Dihydro-10,11-dihydroxy-5H-dibenzazepine-5-carboxamide|10,11-Dihydroxycarbamazepine',\n",
    "            'Dbxref': 'CAS:35079-97-1|KEGG:C07495',\n",
    "            'CTD_chem_gene_ixns.tsv.gz': {  \n",
    "                'CTD_ChemicalID': {'MESH:C004822'},\n",
    "                'CTD_CasRN': {'35079-97-1'},\n",
    "                'CTD_ChemicalName': {'10,11-dihydro-10,11-dihydroxy-5H-dibenzazepine-5-carboxamide'}}, ... }, ... },\n",
    "    'edges': {\n",
    "        'chemical-gene': {\n",
    "            'CHEBI_4592-NCBIGene_2052': {\n",
    "                {'CTD_chem_gene_ixns.tsv': {\n",
    "                    'CTD_Evidence': [{'CTD_Interaction': '[EPHX1 gene SNP affects the metabolism of carbamazepine epoxide] which affects the chemical synthesis of 10,11-dihydro-10,11-dihydroxy-5H-dibenzazepine-5-carboxamide',\n",
    "                     'CTD_InteractionActions': 'affects^chemical synthesis|affects^metabolic processing',\n",
    "                     'CTD_PubMedIDs': '15692831'}]}}, ...}, ...}, ...}, \n",
    "    'relations': {\n",
    "        'RO_0002434': {\n",
    "            'Label': 'interacts with',\n",
    "            'Description': 'A relationship that holds between two entities in which the processes executed by the two entities are causally connected.',\n",
    "            'Synonym': 'in pairwise interaction with'}, ... }\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<i><b>NOTE.</b> All entity metadata are written to the `metadata` directory as a `pickled` dictionary called `entity_metadata_dict.pkl`. The algorithm will look for this dictionary in the `metadata` directory and if it is not there, then no entity metadata will be created.</i>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Prepare Metadata Dictionaries\n",
    "***\n",
    "\n",
    "**Purpose:** To create the resources needed in order to create metadata dictionaries. This process has the following steps:\n",
    "\n",
    "**1. [Generate Metadata Dictionaries](#generate-metadata-dictionaries):** In order to obtain metadata, we first read in the data source for each type and convert it into a dictionary. Then, each metadata dictionary is merged together and saved to a `master_metadata_dictionary`, keyed by identifier.\n",
    "  - <u>Input Datasets</u>:  \n",
    "    - [CTD_chem_gene_ixns.tsv](http://ctdbase.org/reports/CTD_chem_gene_ixns.tsv.gz)   \n",
    "      - Edges: `chemical-gene`, `chemical-protein`, `chemical-rna`  \n",
    "      - Identifier Maps:  \n",
    "        - Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "        - Proteins: [ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt)   \n",
    "        - RNA: [ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt) \n",
    "    - [CTD_chem_go_enriched.tsv](http://ctdbase.org/reports/CTD_chem_go_enriched.tsv.gz)   \n",
    "      - Edges: `chemical-gobp`, `chemical-gocc`, `chemical-gomf`  \n",
    "      - Identifier Maps:  \n",
    "        - Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "    - [CTD_chemicals_diseases.tsv](http://ctdbase.org/reports/CTD_chemicals_diseases.tsv.gz)   \n",
    "      - Edges: `chemical-disease`, `chemical-phenotype`  \n",
    "      - Identifier Maps:  \n",
    "        - Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "        - Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt) \n",
    "        - Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)  \n",
    "    - [ChEBI2Reactome_All_Levels.txt](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt)   \n",
    "      - Edge: `chemical-pathway`   \n",
    "    - [goa_human.gaf](http://current.geneontology.org/annotations/goa_human.gaf.gz)   \n",
    "      - Edges: `protein-gobp`, `protein-gocc`, `protein-gomf`    \n",
    "      - Identifier Maps:  \n",
    "        - Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt) \n",
    "    - [COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt](http://genemania.org/data/current/Homo_sapiens.COMBINED/COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt)   \n",
    "      - Edge: `gene-gene`    \n",
    "      - Identifier Maps:  \n",
    "        - Genes: [UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt) \n",
    "    - [phenotype.hpoa](http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa)   \n",
    "      - Edge: `disease-phenotype`    \n",
    "      - Identifier Maps:  \n",
    "        - Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt)   \n",
    "    - [ChEBI2Reactome_All_Levels.txt](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt)   \n",
    "      - Edge: `chemical-pathway`   \n",
    "    - [gene_association.reactome](https://reactome.org/download/current/gene_association.reactome.gz)   \n",
    "      - Edge: `gobp-pathway`, `pathway-gocc`, `pathway-gomf`      \n",
    "    - [UniProt2Reactome_All_Levels.txt](https://reactome.org/download/current/UniProt2Reactome_All_Levels.txt)   \n",
    "      - Edge: `protein-pathway`      \n",
    "      - Identifier Maps:  \n",
    "        - Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt) \n",
    "    - [CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt)   \n",
    "      - Edge: `variant-disease`, `variant-disease`      \n",
    "      - Identifier Maps:  \n",
    "        - Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt)\n",
    "        - Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)        \n",
    "    - [CLINVAR_VARIANT_GENE_EDGES.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_GENE_EDGES.txt)   \n",
    "      - Edge: `variant-gene`      \n",
    "    - [HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt)   \n",
    "      - Edge: `protein-anatomy`, `protein-cell`, `rna-anatomy`, `rna-cell`         \n",
    "      - Identifier Maps:  \n",
    "        - Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt)         \n",
    "\t\t- Anatomy: [HPA_GTEx_TISSUE_CELL_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEx_TISSUE_CELL_MAP.txt)\n",
    "        - Cells: [HPA_GTEx_TISSUE_CELL_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEx_TISSUE_CELL_MAP.txt)  \n",
    "        - RNA: [GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt) \n",
    "    - [UNIPROT_PROTEIN_CATALYST.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_CATALYST.txt)   \n",
    "      - Edge: `protein-catalyst`\n",
    "    - [UNIPROT_PROTEIN_COFACTOR.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_COFACTOR.txt)   \n",
    "      - Edge: `protein-cofactor`\n",
    "    - [9606.protein.links.v11.0.txt.gz](https://stringdb-static.org/download/protein.links.v11.0/9606.protein.links.v11.0.txt.gz)   \n",
    "      - Edge: `protein-protein`      \n",
    "      - Identifier Maps:  \n",
    "        - Proteins: [STRING_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/STRING_PRO_ONTOLOGY_MAP.txt)\n",
    "    - [curated_gene_disease_associations.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/curated_gene_disease_associations.tsv.gz)   \n",
    "      - Edge: `gene-disease`, `gene-phenotype`      \n",
    "      - Identifier Maps:  \n",
    "        - Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt)\n",
    "        - Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)  \n",
    "    \n",
    "<br>\n",
    "\n",
    "**2. [Write Metadata Files](#write-metadata-files):** The `master_metadata_dictionary` dictionary from _Step 1_ is `pickled` and saved to the `resources/metadata/entity_metadata_dict.pkl` directory.\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the shell for the node and relation dictionary\n",
    "master_metadata_dictionary = {'nodes': {}, 'relations': {}, 'edges': {}}\n",
    "\n",
    "# # create temp metadata directory\n",
    "# temp_location = metadata_location + 'temp'\n",
    "# if os.path.exists(temp_location): shutil.rmtree(temp_location)\n",
    "# os.mkdir(temp_location)\n",
    "# os.mkdir(temp_location + '/nodes')\n",
    "# os.mkdir(temp_location + '/relations')\n",
    "# os.mkdir(temp_location + '/edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Generate Metadata Dictionaries  <a class=\"anchor\" id=\"generate-metadata-dictionaries\"></a>\n",
    "\n",
    "There are two types of data that are processed when building the metadata dictionary. The first type of data is *Primary*, meaning it consists of a small set of variables that are collected for all entities that are included in the knowledge graph (i.e., `Label`, `Description`, `DbXref`, `Synonym`). These data are collected for entities of type: genes, RNA, variants, and pathways. *Secondary* data are then collected for all edges in the knowledge graph that include entities that are not obtained from an ontology. For these sources, metadata may differ by source.  \n",
    "- [Primary Metadata Elements](#primary)  \n",
    "- [Secondary Metadata Elements](#secondary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Metadata Elements<a class=\"anchor\" id=\"primary\"></a>  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Genes Metadata Dictionary <a class=\"anchor\" id=\"gene-metadata\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [National Center for Biotechnology Information Gene](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#national-center-for-biotechnology-information-gene)\n",
    "\n",
    "The nested dictionary of gene metadata is created by looping over the cleaned human [National Center for Biotechnology Information Gene](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#national-center-for-biotechnology-information-gene) identifier data set ([`ensembl_identifier_data_cleaned.txt`](ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz)). The `keys` of the dictionary are `Entrez gene identifiers` and the `values` are dictionaries for each metadata type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrez gene data\n",
    "entrez_gene_data = pandas.read_csv(unprocessed_data_location + 'Homo_sapiens.gene_info', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# remove all rows that are not human\n",
    "entrez_gene_data = entrez_gene_data.loc[entrez_gene_data['#tax_id'].apply(lambda x: x == 9606)]\n",
    "\n",
    "# replace NaN and '-' with 'None'\n",
    "entrez_gene_data.fillna('None', inplace=True)\n",
    "entrez_gene_data.replace('-','None', inplace=True, regex=False)\n",
    "\n",
    "# update prefixes\n",
    "entrez_gene_data['GeneID'] = 'NCBIGene_' + entrez_gene_data['GeneID'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "for idx, row in tqdm(entrez_gene_data.iterrows(), total=entrez_gene_data.shape[0]):\n",
    "    if row['GeneID'] != 'None':\n",
    "        genes, lab, desc, syn = [], [], [], []\n",
    "        gene_id, sym, defn = row['GeneID'], row['Symbol'], row['description']\n",
    "        gene_type, dbxref = row['type_of_gene'], row['dbXrefs']\n",
    "        chrom, map_loc, s1, s2 = row['chromosome'], row['map_location'], row['Synonyms'], row['Other_designations']\n",
    "        genes.append('http://www.ncbi.nlm.nih.gov/gene/' + str(gene_id))\n",
    "        if sym != 'None' or sym != '': lab.append(sym)\n",
    "        else: lab.append('Entrez_ID:' + gene_id)\n",
    "        if 'None' not in [defn, gene_type, chrom, map_loc]:\n",
    "            desc_str = \"{} has locus group '{}' and is located on chromosome {} ({}).\"\n",
    "            desc.append(desc_str.format(sym, gene_type, chrom, map_loc))\n",
    "        else: desc.append(\"{} locus group '{}'.\".format(sym, gene_type))\n",
    "        if s1 != 'None' and s2 != 'None': syn.append('|'.join(set([x for x in (s1 + s2).split('|') if x != 'None' or x != ''])))\n",
    "        elif s1 != 'None': syn.append('|'.join(set([x for x in s1.split('|') if x != 'None' or x != ''])))\n",
    "        elif s2 != 'None': syn.append('|'.join(set([x for x in s2.split('|') if x != 'None' or x != ''])))\n",
    "        else: syn.append('None')\n",
    "        # update master dictionary\n",
    "        master_metadata_dictionary['nodes'][gene_id] = {\n",
    "            'Label': ''.join(lab),\n",
    "            'Description': ''.join(desc),\n",
    "            'Synonym': '|'.join(syn),\n",
    "            'Dbxref': dbxref}\n",
    "\n",
    "# delete unneeded data\n",
    "del entrez_gene_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNA Metadata Dictionary  <a class=\"anchor\" id=\"rna-metadata\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#ensembl)\n",
    "\n",
    "The nested dictionary of rna metadata is created by looping over the cleaned human [Ensembl](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#ensembl) gene, RNA, and protein identifier data set (`ensembl_identifier_data_cleaned.txt`). The `keys` of the dictionary are `Ensembl transcript identifiers` and the `values` are dictionaries for each metadata type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "rna_gene_data = pandas.read_csv(processed_data_location + 'ensembl_identifier_data_cleaned.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# remove rows without identifiers\n",
    "rna_gene_data = rna_gene_data.loc[rna_gene_data['transcript_stable_id'].apply(lambda x: x != 'None')]\n",
    "\n",
    "# remove unneeded columns\n",
    "rna_gene_data.drop(['ensembl_gene_id', 'symbol', 'protein_stable_id', 'uniprot_id', 'master_transcript_type',\n",
    "                    'entrez_id', 'ensembl_gene_type', 'master_gene_type', 'symbol'], axis=1, inplace=True)\n",
    "\n",
    "# remove duplicates\n",
    "rna_gene_data.drop_duplicates(subset=['transcript_stable_id', 'transcript_name', 'ensembl_transcript_type'], keep='first', inplace=True)\n",
    "\n",
    "# update prefixes\n",
    "rna_gene_data['transcript_stable_id'] = 'ensembl_' + rna_gene_data['transcript_stable_id'].astype('str')\n",
    "\n",
    "# replace NaN with 'None'\n",
    "rna_gene_data.fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "for idx, row in tqdm(rna_gene_data.iterrows(), total=rna_gene_data.shape[0]):\n",
    "    rna, lab, desc, syn = [], [], [], []\n",
    "    rna_id = row['transcript_stable_id']\n",
    "    ent_type, nme = row['ensembl_transcript_type'], row['transcript_name']\n",
    "    rna.append('https://uswest.ensembl.org/Homo_sapiens/Transcript/Summary?t=' + rna_id)\n",
    "    if nme != 'None': lab.append(nme)\n",
    "    else:\n",
    "        lab.append('Ensembl_Transcript_ID:' + rna_id)\n",
    "        nme = 'Ensembl_Transcript_ID:' + rna_id\n",
    "    if ent_type != 'None': desc.append(\"Transcript {} is classified as type '{}'.\".format(nme, ent_type))\n",
    "    else: desc.append('None')\n",
    "    syn.append('None')\n",
    "    \n",
    "    # update master dictionary\n",
    "    master_metadata_dictionary['nodes'][rna_id] = {\n",
    "        'Label': ''.join(lab),\n",
    "        'Description': ''.join(desc),\n",
    "        'Synonym': '|'.join(syn)}\n",
    "\n",
    "# delete unneeded data\n",
    "del rna_gene_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variant Metadata Dictionary <a class=\"anchor\" id=\"variant-metadata\"></a>   \n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar Variant](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#clinvar)\n",
    "\n",
    "The nested dictionary of rna metadata is created by looping over the human [ClinVar Variant](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#clinvar) identifier data set ([`variant_summary.txt`](ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz)). The `keys` of the dictionary are `dbSNP identifiers` and the `values` are dictionaries for each metadata type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'variant_summary.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "\n",
    "# load data\n",
    "var_data = pandas.read_csv(unprocessed_data_location + 'variant_summary.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# remove rows without identifiers\n",
    "var_data = var_data.loc[var_data['Assembly'].apply(lambda x: x == 'GRCh38')]\n",
    "var_data = var_data.loc[var_data['RS# (dbSNP)'].apply(lambda x: x != -1)]\n",
    "\n",
    "# de-dup data\n",
    "var_metadata = var_data[['VariationID', '#AlleleID', 'Type', 'Name', 'ClinicalSignificance', 'RS# (dbSNP)', 'Origin',\n",
    "                         'ChromosomeAccession', 'Chromosome', 'Start', 'Stop', 'ReferenceAllele', 'OtherIDs',\n",
    "                         'Assembly', 'AlternateAllele','Cytogenetic', 'ReviewStatus', 'LastEvaluated']] \n",
    "# update prefixes\n",
    "var_metadata['VariationID'] = 'clinvar_' + var_metadata['VariationID'].astype('str')\n",
    "\n",
    "\n",
    "# replace NaN with 'None'\n",
    "var_metadata.replace('na', 'None', inplace=True)\n",
    "var_metadata.fillna('None', inplace=True)\n",
    "\n",
    "# remove duplicate dbSNP ids by choosing the most recent reviewed variant\n",
    "var_metadata.sort_values('LastEvaluated', ascending=False, inplace=True)\n",
    "var_metadata.drop_duplicates(subset='RS# (dbSNP)', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "for idx, row in tqdm(var_metadata.iterrows(), total=var_metadata.shape[0]):\n",
    "    if row['VariationID'] != 'None':\n",
    "        variant, label, desc, syn = [], [], [], []\n",
    "        var_id, lab, dbxref = row['VariationID'], row['Name'], row['OtherIDs']\n",
    "        variant.append('https://www.ncbi.nlm.nih.gov/snp/rs' + str(var_id))\n",
    "        if lab != 'None': label.append(lab)\n",
    "        else: label.append('dbSNP_ID:rs' + str(var_id))\n",
    "        sent = \"This variant is a {} {} located on chromosome {} ({}, start:{}/stop:{} positions, \" +\\\n",
    "               \"cytogenetic location:{}) and has clinical significance '{}'. \" +\\\n",
    "               \"This entry is for the {} and was last reviewed on {} with review status '{}'.\"\n",
    "        desc.append(sent.format(row['Origin'].replace(';', '/'), row['Type'].replace(';', '/'), row['Chromosome'], row['ChromosomeAccession'],\n",
    "                                row['Start'], row['Stop'], row['Cytogenetic'], row['ClinicalSignificance'],\n",
    "                                row['Assembly'], row['LastEvaluated'], row['ReviewStatus']).replace('None', 'UNKNOWN'))\n",
    "        syn.append('None')\n",
    "        \n",
    "        # update master dictionary\n",
    "        master_metadata_dictionary['nodes'][var_id] = {\n",
    "            'Label': ''.join(lab),\n",
    "            'Description': ''.join(desc),\n",
    "            'Synonym': '|'.join(syn),\n",
    "            'Dbxref': dbxref}\n",
    "\n",
    "# delete unneeded data\n",
    "del var_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pathway Metadata Dictionary <a class=\"anchor\" id=\"pathway-metadata\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Reactome Pathway Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#reactome-pathway-database)\n",
    "\n",
    "The nested dictionary of pathway metadata is created by looping over the human [Reactome Pathway Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#reactome-pathway-database) identifier data set ([`ReactomePathways.txt`](https://reactome.org/download/current/ReactomePathways.txt)); Reactome-Gene Association data ([`gene_association.reactome.gz`](https://reactome.org/download/current/gene_association.reactome.gz)), and Reactome-ChEBI data ([`ChEBI2Reactome_All_Levels.txt`](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt)). The `keys` of the dictionary are `Reactome identifiers` and the `values` are dictionaries for each metadata type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download reactome pathways data\n",
    "url = 'https://reactome.org/download/current/ReactomePathways.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'ReactomePathways.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "# load data\n",
    "reactome_pathways = pandas.read_csv(unprocessed_data_location + 'ReactomePathways.txt', header=None, delimiter='\\t', low_memory=False)\n",
    "reactome_pathways = reactome_pathways.loc[reactome_pathways[2].apply(lambda x: x == 'Homo sapiens')] \n",
    "\n",
    "# download reactome gene association data\n",
    "url = 'https://reactome.org/download/current/gene_association.reactome.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'gene_association.reactome'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "# load data\n",
    "reactome_pathways2 = pandas.read_csv(unprocessed_data_location + 'gene_association.reactome', header=None, delimiter='\\t', skiprows=4, low_memory=False)\n",
    "reactome_pathways2 = reactome_pathways2.loc[reactome_pathways2[12].apply(lambda x: x == 'taxon:9606')]\n",
    "reactome_pathways2[5] = reactome_pathways2[5].str.replace('REACTOME:','', regex=True) \n",
    "\n",
    "# download reactome CHEBI data\n",
    "url = 'https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt'):\n",
    "    data_downloader(url, unprocessed_data_location)\n",
    "# load data\n",
    "reactome_pathways3 = pandas.read_csv(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt', header=None, delimiter='\\t', low_memory=False)\n",
    "# remove all non-human pathways and save as list\n",
    "reactome_pathways3 = reactome_pathways3.loc[reactome_pathways3[5].apply(lambda x: x == 'Homo sapiens')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "nodes = list(set(reactome_pathways[0]) | set(reactome_pathways2[5]) | set(reactome_pathways3[1]))\n",
    "pathway_metadata_final = metadata_api_mapper(nodes)\n",
    "\n",
    "# update dictionary\n",
    "pathway_metadata_final['ID'] = pathway_metadata_final['ID'].map('reactome_{}'.format)\n",
    "pathway_metadata_final.set_index('ID', inplace=True)\n",
    "\n",
    "# add entries to existing dictionary\n",
    "master_metadata_dictionary['nodes'].update(pathway_metadata_final.to_dict('index'))\n",
    "\n",
    "# delete unneeded data\n",
    "del reactome_pathways, reactome_pathways2, reactome_pathways3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relations Metadata Dictionary <a class=\"anchor\" id=\"relations-metadata\"></a>   \n",
    "\n",
    "**Data Source Wiki Page:** [Relations Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#relations-ontology)\n",
    "\n",
    "The nested dictionary of relation metadata is created by looping over the human [Relations Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#relations-ontology) identifier data set (`ro_with_imports.owl`). The `keys` of the dictionary are `Relations Ontology identifiers` and the `values` are dictionaries for each metadata type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download ontology\n",
    "if not os.path.exists(unprocessed_data_location + 'ro_with_imports.owl'):\n",
    "    command = '{} {} --merge-import-closure -o {}'\n",
    "    os.system(command.format(owltools_location, 'http://purl.obolibrary.org/obo/ro.owl',\n",
    "                             unprocessed_data_location + 'ro_with_imports.owl'))\n",
    "# load graph\n",
    "ro_graph = Graph().parse(unprocessed_data_location + 'ro_with_imports.owl')\n",
    "print('There are {} edges in the ontology (date:{})'.format(len(ro_graph), datetime.datetime.now().strftime('%m/%d/%Y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "relation_metadata_dict, obo = {}, Namespace('http://purl.obolibrary.org/obo/')\n",
    "\n",
    "# get ontology information\n",
    "cls = [x for x in gets_ontology_classes(ro_graph) if '/RO_' in str(x)] +\\\n",
    "      [x for x in gets_object_properties(ro_graph) if '/RO_' in str(x)]\n",
    "master_synonyms = [x for x in ro_graph if 'synonym' in str(x[1]).lower() and isinstance(x[0], URIRef)]\n",
    "\n",
    "for x in tqdm(cls):\n",
    "    # labels\n",
    "    cls_label = [x for x in ro_graph.objects(x, RDFS.label) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    labels = str(cls_label[0]) if len(cls_label) > 0 else 'None'\n",
    "    # synonyms\n",
    "    cls_syn = [str(i[2]) for i in master_synonyms if x == i[0]]\n",
    "    synonym = str(cls_syn[0]) if len(cls_syn) > 0 else 'None'\n",
    "    # description\n",
    "    cls_desc = [x for x in ro_graph.objects(x, obo.IAO_0000115) if '@' not in n3(x) or '@en' in n3(x)]\n",
    "    desc = '|'.join([str(cls_desc[0])]) if len(cls_desc) > 0 else 'None'\n",
    "    \n",
    "    relation_metadata_dict[str(x).split('/')[-1]] = {\n",
    "        'Label': labels, 'Description': desc, 'Synonym': synonym\n",
    "    }\n",
    "\n",
    "# add entries to existing dictionary\n",
    "master_metadata_dictionary['relations'].update(relation_metadata_dict)\n",
    "\n",
    "# delete unneeded data\n",
    "del ro_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Secondary Metadata Elements<a class=\"anchor\" id=\"secondary\"></a>  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Identifier Maps\n",
    "\n",
    "This code chunk downloads identifier mapping files that were creating in the prior steps.\n",
    "\n",
    "- Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "- Genes: [UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt) \n",
    "- Proteins:  \n",
    "  - [ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt)   \n",
    "  - [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt) \n",
    "  - [STRING_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/STRING_PRO_ONTOLOGY_MAP.txt)\n",
    "- RNA:  \n",
    "  - [ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt)  \n",
    "  - [GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt)\n",
    "- Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt) \n",
    "- Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrez-ensembl map\n",
    "rna_map = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                          header=None, delimiter='\\t', low_memory=False,\n",
    "                          names=['Entrez_Gene_IDs', 'Ensembl_Transcript_IDs', 'Entrez_Gene_Type',\n",
    "                                 'Ensembl_Transcript_Type', 'Master_Gene_Type', 'Master_Transcript_Type',\n",
    "                                 'Entrez_Gene_prefix'])\n",
    "# entrez-pro map\n",
    "entrez_pro_map = pandas.read_csv(processed_data_location + 'ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt',\n",
    "                                 header=None, delimiter='\\t', low_memory=False, usecols = [0, 1, 2, 4],\n",
    "                                 names=['Gene_IDs', 'Protein_Ontology_IDs', 'Entrez_Gene_Type',\n",
    "                                        'Master_Gene_Type', 'Entrez_Gene_Prefix'])\n",
    "# symbol-ensembl map\n",
    "symbol_transcript_map = pandas.read_csv(processed_data_location + 'GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt',\n",
    "                           header=None, delimiter='\\t', low_memory=False,\n",
    "                           names=['Gene_Symbols', 'Ensembl_Transcript_IDs',\n",
    "                                  'Gene_Type', 'Ensembl_Transcript_Type',\n",
    "                                  'Master_Gene_Type', 'Master_Transcript_Type'])\n",
    "\n",
    "# string-pro map\n",
    "string_pro_map = pandas.read_csv(processed_data_location + 'STRING_PRO_ONTOLOGY_MAP.txt',\n",
    "                                 header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                                 names=['STRING_IDs', 'Protein_Ontology_IDs'])\n",
    "# uniprot-pro map\n",
    "uniprot_pro_map = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt',\n",
    "                                  header=None, delimiter='\\t', low_memory=False, usecols=[0, 1],\n",
    "                                  names=['Uniprot_Accession_IDs', 'Protein_Ontology_IDs'])\n",
    "# uniprot-entrez gene map\n",
    "uniprot_entrez_data = pandas.read_csv(processed_data_location + 'UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt',\n",
    "                                      header=None, delimiter='\\t', low_memory=False, usecols=[0, 1, 2, 3],\n",
    "                                      names=['Uniprot_Accession_IDs', 'Entrez_Gene_IDs',\n",
    "                                             'master_gene_type', 'gene_type_update'])\n",
    "# mesh-chebi map\n",
    "mesh_chebi_map = pandas.read_csv(processed_data_location + 'MESH_CHEBI_MAP.txt', header=None, \n",
    "                                 names=['MESH_ID', 'CHEBI_ID'], delimiter='\\t')\n",
    "# disease maps\n",
    "disease_maps = pandas.read_csv(processed_data_location + 'DISEASE_MONDO_MAP.txt', header=None,\n",
    "                               names=['Disease_IDs', 'MONDO_IDs'], delimiter='\\t')\n",
    "# phenotype maps\n",
    "phenotype_maps = pandas.read_csv(processed_data_location + 'PHENOTYPE_HPO_MAP.txt', header=None,\n",
    "                                 names=['Disease_IDs', 'HP_IDs'], delimiter='\\t')\n",
    "\n",
    "# cells and anatomical entities\n",
    "anatomy_maps = pandas.read_csv(processed_data_location + 'HPA_GTEx_TISSUE_CELL_MAP.txt', header=None,\n",
    "                               names=['anatomy_ids', 'ontolgoy_ids'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Genomic Entity Metadata<a class=\"anchor\" id=\"genomicinfo\"></a>  \n",
    "\n",
    "Process the dictionary created in the prior steps in order to assist with creating a master metadata file for all nodes that are a genomic entity (i.e., genes, transcripts, or proteins). Some example output is shown below:\n",
    "\n",
    "``` python\n",
    "{'NCBIGene_51471': {\n",
    "    'Synonyms': ['acetyltransferase 1',\n",
    "                 'Hcml2',\n",
    "                 'probable N-acetyltransferase 8B',\n",
    "                 'CML2',\n",
    "                 'putative N-acetyltransferase 8B',\n",
    "                 'ATase1',\n",
    "                 'N-acetyltransferase 8B (putative, gene/pseudogene)',\n",
    "                 'N-acetyltransferase Camello 2',\n",
    "                 'NAT8BP',\n",
    "                 'camello-like protein 2',\n",
    "                 'N-acetyltransferase 8B (GCN5-related, putative, gene/pseudogene)',\n",
    "                 'putative N-acetyltransferase 8B',\n",
    "                 'ATase1',\n",
    "                 'N-acetyltransferase 8B (GCN5-related, putative, gene/pseudogene)',\n",
    "                 'N-acetyltransferase Camello 2',\n",
    "                 'acetyltransferase 1',\n",
    "                 'camello-like protein 2',\n",
    "                 'probable N-acetyltransferase 8B'],\n",
    "   'PR': ['PR_Q9UHF3'],\n",
    "   'GeneSymbol': ['GeneSymbol_NAT8BP',\n",
    "                  'GeneSymbol_Hcml2',\n",
    "                  'GeneSymbol_CML2',\n",
    "                  'GeneSymbol_NAT8B'],\n",
    "   'ensembl gene': ['ensembl_ENSG00000204872'],\n",
    "   'ensembl protein': ['ensembl_ENSP00000485054'],\n",
    "   'map_location': ['2p13.1'],\n",
    "   'Label': ['N-acetyltransferase 8B (putative, gene/pseudogene)'],\n",
    "   'ensembl transcript': ['ensembl_ENST00000377712'],\n",
    "   'transcript_name': ['NAT8B-201'],\n",
    "   'HGNC_ID': ['HGNC_ID_30235'],\n",
    "   'chromosome': ['2'],\n",
    "   'uniprot': ['uniprot_Q9UHF3']}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data -- only reload if the dictionary has not been populated\n",
    "if not 'reformatted_mapped_identifiers' in locals():\n",
    "    filepath = processed_data_location + 'Merged_gene_rna_protein_identifiers.pkl'\n",
    "    max_bytes = 2**31 - 1; input_size = os.path.getsize(filepath); bytes_in = bytearray(0)\n",
    "    with open(filepath, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    reformatted_mapped_identifiers = pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data for use with master metadata\n",
    "genomic_metadata = dict()\n",
    "for key, value in tqdm(reformatted_mapped_identifiers.items()):\n",
    "    old_prefix = '_'.join(key.split('_')[0:-1]); idx = key.split('_')[-1]; pass_var = True; new_prefix = None\n",
    "    if old_prefix == 'entrez_id': new_prefix = 'NCBIGene'\n",
    "    elif old_prefix in ['ensembl_gene_id', 'protein_stable_id', 'transcript_stable_id']: new_prefix = 'ensembl'\n",
    "    elif old_prefix == 'pro_id_PR': new_prefix = 'PR'\n",
    "    else: pass_var = False\n",
    "    if pass_var and new_prefix is not None:\n",
    "        updated_key = new_prefix + '_' + idx; master_metadata_dict = {updated_key: {}}\n",
    "        for x in value:\n",
    "            i, j = '_'.join(x.split('_')[0:-1]), x.split('_')[-1]\n",
    "            if 'type' in i: continue\n",
    "            elif i == 'entrez_id': new_i = 'NCBIGene'; j = new_i + '_' + j\n",
    "            elif i == 'ensembl_gene_id': new_i = 'ensembl gene'; j = 'ensembl_' + j\n",
    "            elif i == 'protein_stable_id': new_i = 'ensembl protein'; j = 'ensembl_' + j\n",
    "            elif i == 'transcript_stable_id': new_i = 'ensembl transcript'; j = 'ensembl_' + j\n",
    "            elif i == 'pro_id_PR': new_i = 'PR'; j = new_i + '_' + j\n",
    "            elif i == 'hgnc_id': new_i = 'HGNC_ID'; j = new_i + '_' + j\n",
    "            elif i == 'uniprot_id': new_i = 'uniprot'; j = new_i + '_' + j\n",
    "            elif i == 'symbol': new_i = 'GeneSymbol'; j = new_i + '_' + j\n",
    "            else:\n",
    "                if i == 'synonyms': new_i = 'Synonyms'\n",
    "                elif i == 'name': new_i = 'Label'\n",
    "                elif i == 'Other_designations': new_i = 'Synonyms'; j = j.split('|')\n",
    "                else: new_i = i\n",
    "            if new_i in master_metadata_dict[updated_key].keys():\n",
    "                if isinstance(j , list): master_metadata_dict[updated_key][new_i] += j\n",
    "                else: master_metadata_dict[updated_key][new_i] += [j]\n",
    "            else: master_metadata_dict[updated_key][new_i] = [j]\n",
    "        genomic_metadata[updated_key] = master_metadata_dict\n",
    "        \n",
    "# delete unneeded data\n",
    "del reformatted_mapped_identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `CTD_chem_gene_ixns.tsv` <a class=\"anchor\" id=\"chemical-gene\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Comparative Toxicogenomics Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#comparative-toxicogenomics-database)\n",
    "\n",
    "\n",
    "**Edges:**  \n",
    "- `chemical-gene`  \n",
    "- `chemical-protein`  \n",
    "- `chemical-rna` \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "- Proteins: [ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_PRO_ONTOLOGY_MAP.txt)   \n",
    "- RNA: [ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/ENTREZ_GENE_ENSEMBL_TRANSCRIPT_MAP.txt)\n",
    "\n",
    "This chunk process the [`CTD_chem_gene_ixns.tsv`](http://ctdbase.org/reports/CTD_chem_gene_ixns.tsv.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "_chemical_  \n",
    "  - `ChemicalID`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, MeSH Identifier. Variable is provided as a string without a prefix.    \n",
    "  - `CasRN`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, a string containing a CAS Registry Number, if available.    \n",
    "  - `ChemicalName`: A string containing the concept's synonym. If derived from an ontology, the string will be prefixed by the synonym type. If not, a string containing the name of the chemical.   \n",
    "  \n",
    "  _Gene, RNA, and Protein_    \n",
    "  - `GenomicInformation`: A dictionary of gene, RNA, and protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details.  \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Interaction`: A string describing a chemical-gene/protein/rna interaction.   \n",
    "  - `InteractionActions`: A \"|\"-delimited list of the actions that underlie an interaction.   \n",
    "  - `PubMedIDs`: |'-delimited list of PubMed identifiers that do not include a prefix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://ctdbase.org/reports/CTD_chem_gene_ixns.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'CTD_chem_gene_ixns.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location, 'CTD_chem_gene_ixns.tsv')\n",
    "\n",
    "# load data\n",
    "ctd_gene_inx = pandas.read_csv(unprocessed_data_location + 'CTD_chem_gene_ixns.tsv', header=0, delimiter='\\t', skiprows=27)\n",
    "ctd_gene_inx = ctd_gene_inx[ctd_gene_inx['# ChemicalName'] != '#']\n",
    "ctd_gene_inx = ctd_gene_inx[ctd_gene_inx['OrganismID'] == 9606]\n",
    "ctd_gene_inx = ctd_gene_inx[ctd_gene_inx['PubMedIDs'] != numpy.nan]\n",
    "ctd_gene_inx.fillna('None', inplace=True)\n",
    "# fix variable typing\n",
    "ctd_gene_inx['GeneID'] = ctd_gene_inx['GeneID'].astype('Int64')\n",
    "ctd_gene_inx['OrganismID'] = ctd_gene_inx['OrganismID'].astype('Int64')\n",
    "# update prefix\n",
    "ctd_gene_inx['ChemicalID'] = 'MESH:' + ctd_gene_inx['ChemicalID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge identifier maps\n",
    "ctd_gene_inx = ctd_gene_inx.merge(mesh_chebi_map, left_on='ChemicalID', right_on='MESH_ID')\n",
    "ctd_gene_inx = ctd_gene_inx.merge(rna_map, left_on='GeneID', right_on='Entrez_Gene_IDs')\n",
    "ctd_gene_inx = ctd_gene_inx.merge(entrez_pro_map, left_on='GeneID', right_on='Gene_IDs')\n",
    "\n",
    "# visualize data\n",
    "ctd_gene_inx.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'] = {'chemical-gene': {}, 'chemical-rna': {}, 'chemical-protein': {}}\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(ctd_gene_inx.iterrows(), total=ctd_gene_inx.shape[0]):\n",
    "    chebi = row['CHEBI_ID'].rstrip(); gene_form = None\n",
    "    chemical_name = row['# ChemicalName']; chemical_id = row['ChemicalID'].rstrip(); casrn = row['CasRN']\n",
    "    evidence = [{'CTD_Interaction': row['Interaction'],\n",
    "                 'CTD_InteractionActions': row['InteractionActions'],\n",
    "                 'CTD_PubMedIDs': row['PubMedIDs']}]\n",
    "    if row['GeneForms'] == 'gene':\n",
    "        node_key = row['Entrez_Gene_prefix'].rstrip(); gene_form = row['GeneForms']\n",
    "        if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "        else: genomic_info_dict = None\n",
    "        edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-gene'\n",
    "    if row['GeneForms'] == 'protein':\n",
    "        node_key = row['Protein_Ontology_IDs'].rstrip(); gene_form = row['GeneForms']\n",
    "        if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "        else: genomic_info_dict = None\n",
    "        edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-protein'\n",
    "    if row['GeneForms'] == 'mRNA':\n",
    "        node_key = row['Ensembl_Transcript_IDs'].rstrip(); gene_form = row['GeneForms']\n",
    "        if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "        else: genomic_info_dict = None\n",
    "        edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-rna'\n",
    "    if gene_form is not None:\n",
    "        # add chebi metadata\n",
    "        if chebi in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][chebi].keys():\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalName'] |= {chemical_name}\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalID'] |= {chemical_id}\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_CasRN'] |= {casrn}\n",
    "            else:\n",
    "                master_metadata_dictionary['nodes'][chebi].update({\n",
    "                    url: {'CTD_ChemicalID': {chemical_id},\n",
    "                          'CTD_CasRN': {casrn},\n",
    "                          'CTD_ChemicalName': {chemical_name}}})\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'].update({chebi: {\n",
    "                url: {'CTD_ChemicalID': {chemical_id},\n",
    "                      'CTD_CasRN': {casrn},\n",
    "                      'CTD_ChemicalName': {chemical_name}}}})\n",
    "        \n",
    "        # add genomic information\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "            else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "        else:\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "            else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}})\n",
    "\n",
    "        # add relation data to dictionary\n",
    "        if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "            if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "                if 'CTD_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                    inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                    inital_ev = inital_ev['CTD_Evidence'] + evidence\n",
    "                    ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                    master_metadata_dictionary['edges'][edge_key][url]['CTD_Evidence'] = ev\n",
    "                else: master_metadata_dictionary['edges'][edge_key][url].update({'CTD_Evidence': evidence})\n",
    "            else: master_metadata_dictionary['edges'][edge_key].update({url: {'CTD_Evidence': evidence, 'Type': edge_type}})\n",
    "        else: master_metadata_dictionary['edges'].update({edge_key: {url: {'CTD_Evidence': evidence, 'Type': edge_type}}})\n",
    "            \n",
    "# delete unneeded data\n",
    "del ctd_gene_inx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `CTD_chem_go_enriched.tsv` <a class=\"anchor\" id=\"chemical-go\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Comparative Toxicogenomics Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#comparative-toxicogenomics-database)\n",
    "\n",
    "**Edges:**  \n",
    "- `chemical-gobp`  \n",
    "- `chemical-gocc`  \n",
    "- `chemical-gomf` \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "\n",
    "This chunk process the [`CTD_chem_go_enriched.tsv`](http://ctdbase.org/reports/CTD_chem_go_enriched.tsv.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Chemical_  \n",
    "  - `ChemicalID`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, MeSH Identifier. Variable is provided as a string without a prefix.    \n",
    "  - `CasRN`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, a string containing a CAS Registry Number, if available.    \n",
    "  - `ChemicalName`: A string containing the concept's synonym. If derived from an ontology, the string will be prefixed by the synonym type. If not, a string containing the name of the chemical.  \n",
    "  \n",
    "  _GO Biological Process, Cellular Component, Molecular Function_  \n",
    "  - `GOTermName`: A string containing the concept's synonym. \n",
    "  - `Ontology`: A string naming the GO Ontology subset.  \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `HighestGOLevel`: The highest level to which the GO term is assigned within the GO hierarchical ontology. Many GO terms are located at multiple levels within the ontology; only the highest level is displayed. Level 1 constitutes “children” of the most general Biological Process, Cellular Component, and Molecular Function terms. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.   \n",
    "  - `Pvalue`: Raw P-value. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.   \n",
    "  - `CorrectedPValue`:The corrected p-value calculated using the Bonferroni multiple testing adjustment. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.  \n",
    "  - `TargetMatchQty`: The count of matches to the target. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.  \n",
    "  - `TargetTotalQty`: The total matches to the target. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.\n",
    "  - `BackgroundMatchQty`: The count of matches to the genome. Source: http://ctdbase.org/help/chemGODetailHelp.jsp.\n",
    "  - `BackgroundTotalQty`: The total matches to the genome. Source: http://ctdbase.org/help/chemGODetailHelp.jsp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://ctdbase.org/reports/CTD_chem_go_enriched.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'CTD_chem_go_enriched.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location, 'CTD_chem_go_enriched.tsv')\n",
    "\n",
    "# load data\n",
    "ctd_chem_go = pandas.read_csv(unprocessed_data_location + 'CTD_chem_go_enriched.tsv', header=0, delimiter='\\t', skiprows=27)\n",
    "ctd_chem_go = ctd_chem_go[ctd_chem_go['# ChemicalName'] != '#']\n",
    "ctd_chem_go.fillna('None', inplace=True)\n",
    "# update prefix\n",
    "ctd_chem_go['ChemicalID'] = 'MESH:' + ctd_chem_go['ChemicalID']\n",
    "ctd_chem_go['GOTermID'] = ctd_chem_go['GOTermID'].str.replace(':', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge identifier maps\n",
    "ctd_chem_go = ctd_chem_go.merge(mesh_chebi_map, left_on='ChemicalID', right_on='MESH_ID')\n",
    "\n",
    "# visualize data\n",
    "ctd_chem_go.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'chemical-gobp': {}, 'chemical-gocc': {}, 'chemical-gomf': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(ctd_chem_go.iterrows(), total=ctd_chem_go.shape[0]):\n",
    "    chebi = row['CHEBI_ID'].rstrip(); node_key = row['GOTermID']\n",
    "    chemical_name = row['# ChemicalName']; chemical_id = row['ChemicalID'].rstrip(); casrn = row['CasRN']\n",
    "    ontology = row['Ontology']; go_name = row['GOTermName']\n",
    "    evidence = [{'CTD_Pvalue': row['PValue'],\n",
    "                 'CTD_CorrectedPValue': row['CorrectedPValue'],\n",
    "                 'CTD_TargetMatchQty': row['TargetMatchQty'],\n",
    "                 'CTD_TargetTotalQty': row['TargetTotalQty'],\n",
    "                 'CTD_BackgroundMatchQty': row['BackgroundMatchQty'],\n",
    "                 'CTD_BackgroundTotalQty': row['BackgroundTotalQty'],\n",
    "                 'CTD_HighestGOLevel': row['HighestGOLevel']}]\n",
    "    # specify edge type, which is related to the ontology aspect\n",
    "    if ontology == 'Biological Process': edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-gobp'\n",
    "    if ontology == 'Cellular Component': edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-gocc'\n",
    "    if ontology == 'Molecular Function': edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-gomf'    \n",
    "    \n",
    "    # add chebi metadata\n",
    "    if chebi in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][chebi].keys():\n",
    "            master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalName'] |= {chemical_name}\n",
    "            master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalID'] |= {chemical_id}\n",
    "            master_metadata_dictionary['nodes'][chebi][url]['CTD_CasRN'] |= {casrn}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][chebi].update({\n",
    "                url: {'CTD_ChemicalID': {chemical_id},\n",
    "                      'CTD_CasRN': {casrn},\n",
    "                      'CTD_ChemicalName': {chemical_name}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({chebi: {\n",
    "                url: {'CTD_ChemicalID': {chemical_id},\n",
    "                      'CTD_CasRN': {casrn},\n",
    "                      'CTD_ChemicalName': {chemical_name}}}})\n",
    "    \n",
    "    # add go information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['CTD_Ontology'] |= {ontology}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['CTD_GOTermName'] |= {go_name}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({\n",
    "                url: {'CTD_Ontology': {ontology},\n",
    "                      'CTD_GOTermName': {go_name}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key: {\n",
    "            url: {'CTD_Ontology': {ontology},\n",
    "                  'CTD_GOTermName': {go_name}}}})\n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'CTD_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['CTD_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['CTD_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'CTD_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'CTD_Evidence': evidence, 'Type': edge_type}}) \n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'CTD_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del ctd_chem_go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `CTD_chemicals_diseases.tsv` <a class=\"anchor\" id=\"chemical-disease\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Comparative Toxicogenomics Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#comparative-toxicogenomics-database)\n",
    "\n",
    "**Edges:**  \n",
    "- `chemical-disease`  \n",
    "- `chemical-phenotype`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Chemicals: [MESH_CHEBI_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/MESH_CHEBI_MAP.txt)  \n",
    "- Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt) \n",
    "- Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)   \n",
    "\n",
    "This chunk process the [`CTD_chemicals_diseases.tsv`](http://ctdbase.org/reports/CTD_chemicals_diseases.tsv.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Chemical_  \n",
    "  - `ChemicalID`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, MeSH Identifier. Variable is provided as a string without a prefix.    \n",
    "  - `CasRN`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. If not, a string containing a CAS Registry Number, if available.    \n",
    "  - `ChemicalName`: A string containing the concept's synonym. If derived from an ontology, the string will be prefixed by the synonym type. If not, a string containing the name of the chemical.    \n",
    "  \n",
    "  _Disease, Phenotype_  \n",
    "  - `DiseaseName`: A string containing the concept's synonym. \n",
    "  - `DiseaseID`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID.  \n",
    "  - `OmimIDs`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID.  \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `DirectEvidence`: '|'-delimited list of strings that include keywords.   \n",
    "  - `InferenceScore`: The inference score (float) reflects the degree of similarity between CTD chemical–gene–disease networks and a similar scale-free random network. The higher the score, the more likely the inference network has atypical connectivity.   \n",
    "  - `PubMedIDs`: |'-delimited list of PubMed identifiers that do not include a prefix.  \n",
    "  - `InferenceGeneSymbol`: A string containing the gene symbol. The genes on which the inferred association is based (i.e., genes that have curated interactions with the chemical and curated associations with the disease).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://ctdbase.org/reports/CTD_chemicals_diseases.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'CTD_chemicals_diseases.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location, 'CTD_chemicals_diseases.tsv')\n",
    "\n",
    "# load data\n",
    "ctd_chem_dis = pandas.read_csv(unprocessed_data_location + 'CTD_chemicals_diseases.tsv', header=0, delimiter='\\t', skiprows=27)\n",
    "ctd_chem_dis = ctd_chem_dis[ctd_chem_dis['# ChemicalName'] != '#']\n",
    "ctd_chem_dis = ctd_chem_dis[ctd_chem_dis['PubMedIDs'] != numpy.nan]\n",
    "ctd_chem_dis = ctd_chem_dis[ctd_chem_dis['DiseaseID'] != numpy.nan]\n",
    "# update prefix\n",
    "ctd_chem_dis['ChemicalID'] = 'MESH:' + ctd_chem_dis ['ChemicalID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctd_chem_dis = ctd_chem_dis.merge(mesh_chebi_map, left_on='ChemicalID', right_on='MESH_ID')\n",
    "ctd_chem_dis = ctd_chem_dis.merge(disease_maps, left_on='DiseaseID', right_on='Disease_IDs')\n",
    "ctd_chem_dis = ctd_chem_dis.merge(phenotype_maps, left_on='DiseaseID', right_on='Disease_IDs')\n",
    "\n",
    "# visualize data\n",
    "ctd_chem_dis.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'chemical-disease': {}, 'chemical-phenotype': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(ctd_chem_dis.iterrows(), total=ctd_chem_dis.shape[0]):\n",
    "    chebi = row['CHEBI_ID'].rstrip()\n",
    "    chemical_name = row['# ChemicalName']; chemical_id = row['ChemicalID'].rstrip(); casrn = row['CasRN']\n",
    "    dis_name = row['DiseaseName']; dis_id = row['DiseaseID']\n",
    "    omim = row['OmimIDs'] if not pandas.isna(row['OmimIDs']) else 'None'\n",
    "    evidence = [{'CTD_DirectEvidence': row['DirectEvidence'] if not pandas.isna(row['DirectEvidence']) else 'None',\n",
    "                 'CTD_InferenceScore': row['InferenceScore'] if not pandas.isna(row['InferenceScore']) else 'None',\n",
    "                 'CTD_PubMedIDs': row['PubMedIDs'],\n",
    "                 'CTD_InferenceGeneSymbol': row['InferenceGeneSymbol'] if not pandas.isna(row['InferenceGeneSymbol']) else 'None'}]\n",
    "    for node_key in [row['MONDO_IDs'], row['HP_IDs']]:\n",
    "        if not pandas.isna(node_key) and node_key.startswith('MONDO'):\n",
    "            edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-disease'\n",
    "        if not pandas.isna(node_key) and node_key.startswith('HP'):\n",
    "            edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-phenotype'\n",
    "        \n",
    "        # add chebi metadata\n",
    "        if chebi in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][chebi].keys():\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalName'] |= {chemical_name}\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_ChemicalID'] |= {chemical_id}\n",
    "                master_metadata_dictionary['nodes'][chebi][url]['CTD_CasRN'] |= {casrn}\n",
    "            else:\n",
    "                master_metadata_dictionary['nodes'][chebi].update({\n",
    "                url: {'CTD_ChemicalID': {chemical_id},\n",
    "                      'CTD_CasRN': {casrn},\n",
    "                      'CTD_ChemicalName': {chemical_name}}})\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'].update({chebi: {\n",
    "                url: {'CTD_ChemicalID': {chemical_id},\n",
    "                      'CTD_CasRN': {casrn},\n",
    "                      'CTD_ChemicalName': {chemical_name}}}})\n",
    "        \n",
    "        # add disease information\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][node_key]:\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['CTD_DiseaseName'] |= {dis_name}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['CTD_DiseaseID'] |= {dis_id}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['CTD_OmimIDs'] |= {omim}\n",
    "            else:\n",
    "                master_metadata_dictionary['nodes'][node_key].update({\n",
    "                    url: {'CTD_DiseaseName': {dis_name},\n",
    "                          'CTD_DiseaseID': {dis_id},\n",
    "                          'CTD_OmimIDs': {omim}}})\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {\n",
    "                    url: {'CTD_DiseaseName': {dis_name},\n",
    "                          'CTD_DiseaseID': {dis_id},\n",
    "                          'CTD_OmimIDs': {omim}}}})\n",
    "        \n",
    "        # add relation data to dictionary\n",
    "        if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "            if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "                if 'CTD_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                    inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                    inital_ev = inital_ev['CTD_Evidence'] + evidence\n",
    "                    ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                    master_metadata_dictionary['edges'][edge_key][url]['CTD_Evidence'] = ev\n",
    "                else: master_metadata_dictionary['edges'][edge_key][url].update({'CTD_Evidence': evidence})\n",
    "            else: master_metadata_dictionary['edges'][edge_key].update({url: {'CTD_Evidence': evidence, 'Type': edge_type}})\n",
    "        else: master_metadata_dictionary['edges'].update({edge_key: {url: {'CTD_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del ctd_chem_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `ChEBI2Reactome_All_Levels.txt` <a class=\"anchor\" id=\"gene-pathway\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Reactome Pathway Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#reactome-pathway-database)     \n",
    "\n",
    "\n",
    "**Edges:**  \n",
    "- `chemical-pathway`  \n",
    "\n",
    "This chunk process the [`ChEBI2Reactome_All_Levels.txt`](https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt) file and obtains the following node metadata:  \n",
    "- **Nodes:**  \n",
    "  _Pathway_  \n",
    "  - `DBReference`: A string containing the concept's database cross-reference, which is formatted as prefix:ID.       \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `EvidenceID`: A string containing an evidence code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/ChEBI2Reactome_All_Levels.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'ChEBI2Reactome_All_Levels.txt')\n",
    "\n",
    "# load data\n",
    "rtm_chem_path = pandas.read_csv(unprocessed_data_location + 'ChEBI2Reactome_All_Levels.txt', header=None, delimiter='\\t', skiprows=0)\n",
    "rtm_chem_path = rtm_chem_path[rtm_chem_path[5] == 'Homo sapiens']\n",
    "rtm_chem_path.fillna('None', inplace=True)\n",
    "# update prefix\n",
    "rtm_chem_path[0] = 'CHEBI_' + rtm_chem_path[0].astype('str')\n",
    "rtm_chem_path[1] = 'reactome_' + rtm_chem_path[1]\n",
    "\n",
    "# visualize data\n",
    "rtm_chem_path.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'chemical-pathway': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(rtm_chem_path.iterrows(), total=rtm_chem_path.shape[0]):\n",
    "    chebi = row[0].rstrip(); node_key = row[1]; path_name = row[3]\n",
    "    evidence = [{'CTD_EvidenceID': row[4]}]   \n",
    "    edge_key = '{}-{}'.format(chebi, node_key); edge_type = 'chemical-pathway'   \n",
    "    \n",
    "    # add reactome information \n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['Reactome_PathwayName'] |= {path_name}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({\n",
    "                url: {'Reactome_PathwayName': {path_name}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key: {\n",
    "            url: {'Reactome_PathwayName': {path_name}}}})\n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'Reactome_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['Reactome_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['Reactome_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'Reactome_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'Reactome_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'Reactome_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del rtm_chem_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `goa_human.gaf` <a class=\"anchor\" id=\"goa\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Gene Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#gene-ontology)   \n",
    "\n",
    "\n",
    "**Edges:**  \n",
    "- `protein-gobp`  \n",
    "- `protein-gocc`  \n",
    "- `protein-gomf`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt)  \n",
    "This chunk process the [`goa_human.gaf`](http://current.geneontology.org/annotations/goa_human.gaf.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "_GO Biological Process, Cellular Component, and Molecular Function_     \n",
    "  - `DB_Object_Name`: A string containing the concept's synonym.    \n",
    "  - `DB_Object_Synonym`: A string containing the concept's synonym.      \n",
    "  - `DB_Object_Symbol`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID. \n",
    "  - `With_Or_From`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID.  \n",
    "  - `DB_Object_Type`: A string indicating the type of object that has been annotated.  \n",
    "  \n",
    "  _Protein_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Qualifier`: Some annotations are modified by qualifiers, which have specific usage rules and meanings within GO. \n",
    "  - `DB_Reference`: One or more unique identifiers for a single source cited as an authority for the attribution of the GO ID to the DB Object ID. This may be a literature reference or a database record. The syntax is DB:accession_number.    \n",
    "  - `EvidenceCode`: Each annotation includes an evidence code to indicate how the annotation to a particular term is supported\n",
    "    - Inferred from Experiment (EXP)\n",
    "    - Inferred from Direct Assay (IDA)\n",
    "    - Inferred from Physical Interaction (IPI)\n",
    "    - Inferred from Mutant Phenotype (IMP)\n",
    "    - Inferred from Genetic Interaction (IGI)\n",
    "    - Inferred from Expression Pattern (IEP)\n",
    "    - Inferred from High Throughput Experiment (HTP)\n",
    "    - Inferred from High Throughput Direct Assay (HDA)\n",
    "    - Inferred from High Throughput Mutant Phenotype (HMP)\n",
    "    - Inferred from High Throughput Genetic Interaction (HGI)\n",
    "    - Inferred from High Throughput Expression Pattern (HEP)\n",
    "    - Inferred from Biological aspect of Ancestor (IBA)\n",
    "    - Inferred from Biological aspect of Descendant (IBD)\n",
    "    - Inferred from Key Residues (IKR)\n",
    "    - Inferred from Rapid Divergence (IRD)\n",
    "    - Inferred from Sequence or structural Similarity (ISS)\n",
    "    - Inferred from Sequence Orthology (ISO)\n",
    "    - Inferred from Sequence Alignment (ISA)\n",
    "    - Inferred from Sequence Model (ISM)\n",
    "    - Inferred from Genomic Context (IGC)\n",
    "    - Inferred from Reviewed Computational Analysis (RCA)\n",
    "    - Traceable Author Statement (TAS)\n",
    "    - Non-traceable Author Statement (NAS)\n",
    "    - Inferred by Curator (IC)\n",
    "    - No biological Data available (ND)\n",
    "    - Inferred from Electronic Annotation (IEA)  \n",
    "  - `AssignedBy`: A string indicating who assigned the association.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://current.geneontology.org/annotations/goa_human.gaf.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'goa_human.gaf'):\n",
    "    data_downloader(url, unprocessed_data_location, 'goa_human.gaf')\n",
    "\n",
    "# load data\n",
    "goa_gene = pandas.read_csv(unprocessed_data_location + 'goa_human.gaf', header=None, delimiter='\\t', skiprows=41, low_memory=False)\n",
    "goa_gene = goa_gene[goa_gene[12] == 'taxon:9606']\n",
    "goa_gene = goa_gene[goa_gene[3] != 'NOT']\n",
    "goa_gene = goa_gene[goa_gene[11] == 'protein']\n",
    "# fix prefix\n",
    "goa_gene[4] = goa_gene[4].str.replace(':', '_')\n",
    "goa_gene.fillna('None', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goa_gene = goa_gene.merge(uniprot_pro_map, left_on=1, right_on='Uniprot_Accession_IDs')\n",
    "\n",
    "# visualize data\n",
    "goa_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-gobp': {}, 'protein-gocc': {}, 'protein-gomf': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(goa_gene.iterrows(), total=goa_gene.shape[0]):\n",
    "    pr = row['Protein_Ontology_IDs'].rstrip(); node_key = row[4]\n",
    "    pr_db = row[9]; pr_syn = row[10]; pr_symb = row[2]; aspect = row[8]; db_with = row[7]\n",
    "    evidence = [{'GOA_Qualifier': row[3], 'GOA_DB_Reference': row[5],\n",
    "                 'GOA_EvidenceCode': row[6], 'GOA_AssignedBy': row[14]}]\n",
    "    if aspect == 'P': edge_key = '{}-{}'.format(pr, node_key); edge_type = 'protein-gobp'\n",
    "    if aspect == 'C': edge_key = '{}-{}'.format(pr, node_key); edge_type = 'protein-gocc'\n",
    "    if aspect == 'F': edge_key = '{}-{}'.format(pr, node_key); edge_type = 'protein-gomf'    \n",
    "    if pr in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[pr]\n",
    "    else: genomic_info_dict = None\n",
    "    \n",
    "    # add pr information\n",
    "    if pr in master_metadata_dictionary['nodes'].keys():      \n",
    "        if url in master_metadata_dictionary['nodes'][pr].keys():\n",
    "            master_metadata_dictionary['nodes'][pr][url]['GOA_DB_Object_Name'] |= {pr_db}\n",
    "            master_metadata_dictionary['nodes'][pr][url]['GOA_DB_Object_Synonym'] |= {pr_syn}\n",
    "            master_metadata_dictionary['nodes'][pr][url]['GOA_DB_Object_Symbol'] |= {pr_symb}\n",
    "            master_metadata_dictionary['nodes'][pr][url]['GOA_With_Or_From'] |= {db_with}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][pr].update({\n",
    "                url: {'GOA_DB_Object_Name': {pr_db},\n",
    "                      'GOA_DB_Object_Synonym': {pr_syn},\n",
    "                      'GOA_DB_Object_Symbol': {pr_symb},\n",
    "                      'GOA_With_Or_From': {db_with}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({pr: {\n",
    "            url: {'GOA_DB_Object_Name': {pr_db},\n",
    "                  'GOA_DB_Object_Synonym': {pr_syn},\n",
    "                  'GOA_DB_Object_Symbol': {pr_symb},\n",
    "                  'GOA_With_Or_From': {db_with}}}})\n",
    "    \n",
    "    # add genomic information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'GOA_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['GOA_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['GOA_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'GOA_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'GOA_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'GOA_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del goa_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt` <a class=\"anchor\" id=\"gene-gene\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [GeneMANIA](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#genemania)      \n",
    "\n",
    "**Edges:**  \n",
    "- `gene-gene`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Genes: [UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_ENTREZ_GENE_MAP.txt) \n",
    "\n",
    "This chunk process the [`COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt`](http://genemania.org/data/current/Homo_sapiens.COMBINED/COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt) file and obtains the following edge metadata:    \n",
    "- **Nodes:**  \n",
    "  _Genes_    \n",
    "  - `GenomicInformation`: A dictionary of gene identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "- **Edges:**  \n",
    "  - `Weight`: Assumes the input gene list is related through GO biological processes. The score will vary depending on the type of network, but in general is a number ranging from zero (no interaction) to 1 (strong interaction). See `PMID:25254104` for more detail.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://genemania.org/data/current/Homo_sapiens.COMBINED/COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt')\n",
    "\n",
    "# load data\n",
    "gm_gene_gene = pandas.read_csv(unprocessed_data_location + 'COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt', header=0, delimiter='\\t', skiprows=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_gene_gene = gm_gene_gene.merge(uniprot_entrez_data, left_on='Gene_A', right_on='Uniprot_Accession_IDs')\n",
    "gm_gene_gene.rename(columns={'Entrez_Gene_IDs': 'Entrez_Gene_A'}, inplace=True)\n",
    "gm_gene_gene = gm_gene_gene.merge(uniprot_entrez_data, left_on='Gene_B', right_on='Uniprot_Accession_IDs')\n",
    "gm_gene_gene.rename(columns={'Entrez_Gene_IDs': 'Entrez_Gene_B'}, inplace=True)\n",
    "\n",
    "# visualize data\n",
    "gm_gene_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'gene-gene': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(gm_gene_gene.iterrows(), total=gm_gene_gene.shape[0]):\n",
    "    genes = [row['Entrez_Gene_A'], row['Entrez_Gene_B']]; weight = row['Weight']; gene_info = []\n",
    "    edge_key = '{}-{}'.format(row['Entrez_Gene_A'], row['Entrez_Gene_B']); edge_type = 'gene-gene' \n",
    "    \n",
    "    for node_key in genes:\n",
    "        if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "        else: genomic_info_dict = None\n",
    "        \n",
    "        # add genomic information\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "            else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "        else:\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "            else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}}) \n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'GeneMania_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['GeneMania_Evidence'] = weight\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'GeneMania_Evidence': weight})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'GeneMania_Evidence': weight, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'GeneMania_Evidence': weight, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del gm_gene_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `phenotype.hpoa` <a class=\"anchor\" id=\"phenotype-disease\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Human Phenotype Ontology](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-phenotype-ontology)      \n",
    "\n",
    "**Edges:**  \n",
    "- `disease-phenotype`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt) \n",
    "\n",
    "This chunk process the [`phenotype.hpoa`](http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Disease, Phenotype_  \n",
    "  - `DiseaseName`: A string containing the concept's synonym.  \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Reference`: This required field indicates the source of the information used for the annotation. This may be the clinical experience of the annotator or may be taken from an article as indicated by a PubMed id. Each collaborating center of the Human Phenotype Ontology consortium is assigned a HPO:Ref id. In addition, if appropriate, a PubMed id for an article describing the clinical abnormality may be used.    \n",
    "  - `Evidence`: This required field indicates the level of evidence supporting the annotation. Annotations that have been extracted by parsing the Clinical Features sections of the omim.txt file are assigned the evidence code IEA. Other codes include PCS for published clinical study. This should be used for information extracted from articles in the medical literature. ICE can be used for annotations based on individual clinical experience. This may be appropriate for disorders with a limited amount of published data. This must be accompanied by an entry in the DB:Reference field denoting the individual or center performing the annotation together with an identifier. For instance, GH:007 might be used to refer to the seventh such annotation made by a specialist from Gotham Hospital (assuming the prefix GH has been registered with the HPO). Finally we have TAS, which stands for “traceable author statement”, usually reviews or disease entries (e.g. OMIM) that only refers to the original publication..    \n",
    "  - `Frequency`: A term-id from the HPO-sub-ontology below the term Frequency.\n",
    "     There are three allowed options for this field.\n",
    "      1. A term-id from the HPO-sub-ontology below the term Frequency.\n",
    "      2. A count of patients affected within a cohort. For instance, 7/13 would indicate that 7 of the 13 patients with the specified disease were found to have the phenotypic abnormality referred to by the HPO term in question in the study referred to by the DB_Reference\n",
    "      3. A percentage value such as 17%, again referring to the percentage of patients found to have the phenotypic abnormality referred to by the HPO term in question in the study referred to by the DB_Reference. If possible, the 7/13 format is preferred over the percentage format if the exact data is available..    \n",
    "  - `Sex`: This field contains the strings MALE or FEMALE if the annotation in question is limited to males or females. This field refers to the phenotypic (and not the chromosomal) sex, and does not intend to capture the further complexities of sex determination. If a phenotype is limited to one or the other sex, then the corresponding term from the Clinical modifier subontology should also be used in the Modifier field.\n",
    "  - `Modifier`: A term from the Clinical modifier subontology.  \n",
    "  - `Aspect`: One of P (Phenotypic abnormality), I (inheritance), C (onset and clinical course). This field is mandatory; cardinality 1.  \n",
    "  - `Biocuration`: This refers to the center or user making the annotation and the date on which the annotation was made; format is YYYY-MM-DD this field is mandatory. Multiple entries can be separated by a semicolon if an annotation was revised, e.g., HPO:skoehler[2010-04-21];HPO:lcarmody[2019-06-02]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa'\n",
    "if not os.path.exists(unprocessed_data_location + 'phenotype.hpoa'):\n",
    "    data_downloader(url, unprocessed_data_location, 'phenotype.hpoa')\n",
    "\n",
    "# load data\n",
    "hpo_dis_phe = pandas.read_csv(unprocessed_data_location + 'phenotype.hpoa', header=0, delimiter='\\t', skiprows=4, low_memory=False)\n",
    "hpo_dis_phe = hpo_dis_phe[hpo_dis_phe['Qualifier'] != 'NOT']\n",
    "hpo_dis_phe.fillna('None', inplace=True)\n",
    "\n",
    "# fix prefix\n",
    "hpo_dis_phe['HPO_ID'] = hpo_dis_phe['HPO_ID'].str.replace(':', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_dis_phe = hpo_dis_phe.merge(disease_maps, left_on='#DatabaseID', right_on='Disease_IDs')\n",
    "\n",
    "# visualize data\n",
    "hpo_dis_phe.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'disease-phenotype': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(hpo_dis_phe.iterrows(), total=hpo_dis_phe.shape[0]):\n",
    "    concepts = [row['MONDO_IDs'], row['HPO_ID']]\n",
    "    disease_names = {row['MONDO_IDs']: {row['DiseaseName']}, row['HPO_ID']: {'None'}}\n",
    "    edge_key = '{}-{}'.format(row['MONDO_IDs'], row['HPO_ID']); edge_type = 'disease-phenotype'\n",
    "    evidence = [{'HPO_Reference': row['Reference'],\n",
    "                 'HPO_EvidenceCode': row['Evidence'],\n",
    "                 'HPO_Frequency': row['Frequency'],\n",
    "                 'HPO_Sex': row['Sex'],\n",
    "                 'HPO_Modifier': row['Modifier'],\n",
    "                 'HPO_Aspect': row['Aspect'],\n",
    "                 'HPO_Biocuration': row['Biocuration']}]\n",
    "    for node_key in concepts:\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['HPO_DiseaseName'] |= disease_names[node_key]\n",
    "            else: master_metadata_dictionary['nodes'][node_key].update({url: {'HPO_DiseaseName': disease_names[node_key]}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key: {url: {'HPO_DiseaseName': disease_names[node_key]}}})\n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'HPO_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['HPO_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['HPO_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'HPO_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'HPO_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'HPO_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del hpo_dis_phe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `gene_association.reactome` <a class=\"anchor\" id=\"reactome-go\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Reactome Pathway Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#reactome-pathway-database)    \n",
    "\n",
    "**Edges:**  \n",
    "- `gobp-pathway`     \n",
    "- `pathway-gocc`   \n",
    "- `pathway-gomf`  \n",
    "\n",
    "This chunk process the [`gene_association.reactome.tsv`](https://reactome.org/download/current/gene_association.reactome.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Pathway_  \n",
    "  - `DBReference`: A string containing the concept's database cross-reference, which is formatted as Prefix:ID.    \n",
    "  \n",
    "  _Gene Ontology Biological Proces, Cellular Component, Molecular Function_\n",
    "  - `Aspect`: A variable indicating what GO subontology is being used.    \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `EvidenceCode`: Each annotation includes an evidence code to indicate how the annotation to a particular term is supported\n",
    "    - Inferred from Experiment (EXP)\n",
    "    - Inferred from Direct Assay (IDA)\n",
    "    - Inferred from Physical Interaction (IPI)\n",
    "    - Inferred from Mutant Phenotype (IMP)\n",
    "    - Inferred from Genetic Interaction (IGI)\n",
    "    - Inferred from Expression Pattern (IEP)\n",
    "    - Inferred from High Throughput Experiment (HTP)\n",
    "    - Inferred from High Throughput Direct Assay (HDA)\n",
    "    - Inferred from High Throughput Mutant Phenotype (HMP)\n",
    "    - Inferred from High Throughput Genetic Interaction (HGI)\n",
    "    - Inferred from High Throughput Expression Pattern (HEP)\n",
    "    - Inferred from Biological aspect of Ancestor (IBA)\n",
    "    - Inferred from Biological aspect of Descendant (IBD)\n",
    "    - Inferred from Key Residues (IKR)\n",
    "    - Inferred from Rapid Divergence (IRD)\n",
    "    - Inferred from Sequence or structural Similarity (ISS)\n",
    "    - Inferred from Sequence Orthology (ISO)\n",
    "    - Inferred from Sequence Alignment (ISA)\n",
    "    - Inferred from Sequence Model (ISM)\n",
    "    - Inferred from Genomic Context (IGC)\n",
    "    - Inferred from Reviewed Computational Analysis (RCA)\n",
    "    - Traceable Author Statement (TAS)\n",
    "    - Non-traceable Author Statement (NAS)\n",
    "    - Inferred by Curator (IC)\n",
    "    - No biological Data available (ND)\n",
    "    - Inferred from Electronic Annotation (IEA)   \n",
    "  - `AssignedBy`: A string indicating who assigned the association.   \n",
    "  - `Qualifier`: Some annotations are modified by qualifiers, which have specific usage rules and meanings within GO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/gene_association.reactome.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'gene_association.reactome'):\n",
    "    data_downloader(url, unprocessed_data_location, 'gene_association.reactome')\n",
    "\n",
    "# load data\n",
    "rce_go_ptw = pandas.read_csv(unprocessed_data_location + 'gene_association.reactome', header=None, delimiter='\\t', skiprows=4)\n",
    "rce_go_ptw.fillna('None', inplace=True)\n",
    "rce_go_ptw = rce_go_ptw[rce_go_ptw[12] == 'taxon:9606']\n",
    "rce_go_ptw = rce_go_ptw[[x.startswith('REACTOME') for x in rce_go_ptw[5]]]\n",
    "\n",
    "# fix variable prefixing\n",
    "rce_go_ptw[4] = rce_go_ptw[4].str.replace(':', '_')\n",
    "rce_go_ptw[5] = rce_go_ptw[5].str.replace('REACTOME:', 'reactome_')\n",
    "\n",
    "# visualize data\n",
    "rce_go_ptw.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'gobp-pathway': {}, 'pathway-gocc': {}, 'pathway-gomf': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(rce_go_ptw.iterrows(), total=rce_go_ptw.shape[0]):\n",
    "    react = row[5].rstrip(); node_key = row[4]; pathway_db = row[0]; aspect = row[8]\n",
    "    evidence = [{'Reactome_EvidenceCode': row[6],\n",
    "                 'Reactome_AssignedBy': row[14],\n",
    "                 'Reactome_Qualifier': row[3]}]\n",
    "    # specify edge type, which is related to the ontology aspect\n",
    "    if aspect == 'P': edge_key = '{}-{}'.format(node_key, react); edge_type = 'gobp-pathway'\n",
    "    if aspect == 'C': edge_key = '{}-{}'.format(react, node_key); edge_type = 'pathway-gocc'\n",
    "    if aspect == 'F': edge_key = '{}-{}'.format(react, node_key); edge_type = 'pathway-gomf'    \n",
    "    \n",
    "    # add reactome metadata\n",
    "    if react in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][react].keys():\n",
    "            master_metadata_dictionary['nodes'][react][url]['Reactome_DBReference'] |= {pathway_db}\n",
    "        else: master_metadata_dictionary['nodes'][react].update({url: {'Reactome_DBReference': {pathway_db}}})\n",
    "    else: master_metadata_dictionary['nodes'].update({react: {url: {'Reactome_DBReference': {pathway_db}}}})\n",
    "    \n",
    "    # add go information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['Reactome_Aspect'] |= {aspect}\n",
    "        else: master_metadata_dictionary['nodes'][node_key].update({url: {'Reactome_Aspect': {aspect}}})\n",
    "    else: master_metadata_dictionary['nodes'].update({node_key: {url: {'Reactome_Aspect': {aspect}}}})\n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'Reactome_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['Reactome_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['Reactome_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'Reactome_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'Reactome_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'Reactome_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del rce_go_ptw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `UniProt2Reactome_All_Levels.txt` <a class=\"anchor\" id=\"uniprot-react\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Reactome Pathway Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#reactome-pathway-database)    \n",
    "\n",
    "**Edges:**  \n",
    "- `protein-pathway`   \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt)   \n",
    "This chunk process the [`UniProt2Reactome_All_Levels.txt`](https://reactome.org/download/current/UniProt2Reactome_All_Levels.txt) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**   \n",
    "  _Pathway_  \n",
    "  - `PathwayName`: A string containing the concept's label.   \n",
    "    \n",
    "  _Protein_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `EvidenceID`: Each annotation includes an evidence code to indicate how the annotation to a particular term is supported\n",
    "    - Inferred from Experiment (EXP)\n",
    "    - Inferred from Direct Assay (IDA)\n",
    "    - Inferred from Physical Interaction (IPI)\n",
    "    - Inferred from Mutant Phenotype (IMP)\n",
    "    - Inferred from Genetic Interaction (IGI)\n",
    "    - Inferred from Expression Pattern (IEP)\n",
    "    - Inferred from High Throughput Experiment (HTP)\n",
    "    - Inferred from High Throughput Direct Assay (HDA)\n",
    "    - Inferred from High Throughput Mutant Phenotype (HMP)\n",
    "    - Inferred from High Throughput Genetic Interaction (HGI)\n",
    "    - Inferred from High Throughput Expression Pattern (HEP)\n",
    "    - Inferred from Biological aspect of Ancestor (IBA)\n",
    "    - Inferred from Biological aspect of Descendant (IBD)\n",
    "    - Inferred from Key Residues (IKR)\n",
    "    - Inferred from Rapid Divergence (IRD)\n",
    "    - Inferred from Sequence or structural Similarity (ISS)\n",
    "    - Inferred from Sequence Orthology (ISO)\n",
    "    - Inferred from Sequence Alignment (ISA)\n",
    "    - Inferred from Sequence Model (ISM)\n",
    "    - Inferred from Genomic Context (IGC)\n",
    "    - Inferred from Reviewed Computational Analysis (RCA)\n",
    "    - Traceable Author Statement (TAS)\n",
    "    - Non-traceable Author Statement (NAS)\n",
    "    - Inferred by Curator (IC)\n",
    "    - No biological Data available (ND)\n",
    "    - Inferred from Electronic Annotation (IEA)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://reactome.org/download/current/UniProt2Reactome_All_Levels.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'UniProt2Reactome_All_Levels.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'UniProt2Reactome_All_Levels.txt')\n",
    "\n",
    "# load data\n",
    "rce_prot_pth = pandas.read_csv(unprocessed_data_location + 'UniProt2Reactome_All_Levels.txt', header=None, delimiter='\\t', skiprows=0)\n",
    "rce_prot_pth = rce_prot_pth[rce_prot_pth[5] == 'Homo sapiens']\n",
    "# fix prefixes\n",
    "rce_prot_pth[1] = 'reactome_' + rce_prot_pth[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rce_prot_pth = rce_prot_pth.merge(uniprot_pro_map, left_on=0, right_on='Uniprot_Accession_IDs')\n",
    "\n",
    "# visualize data\n",
    "rce_prot_pth.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-pathway': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(rce_prot_pth.iterrows(), total=rce_prot_pth.shape[0]):\n",
    "    pr = row['Protein_Ontology_IDs'].rstrip(); node_key = row[1]; react_name = row[3]\n",
    "    evidence = [{'Reactome_EvidenceID': row[4]}]\n",
    "    edge_key = '{}-{}'.format(pr, node_key); edge_type = 'protein-pathway'\n",
    "    if pr in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[pr]\n",
    "    else: genomic_info_dict = None\n",
    "    \n",
    "    # add reactome information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['Reactome_PathwayName'] |= {react_name}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({url: {'Reactome_PathwayName': {react_name}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key: {url: {'Reactome_PathwayName': {react_name}}}})\n",
    "    \n",
    "    # add genomic information\n",
    "    if pr in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][pr].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][pr].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({pr: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({pr: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'Reactome_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['Reactome_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['Reactome_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'Reactome_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'Reactome_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'Reactome_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del rce_prot_pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt` <a class=\"anchor\" id=\"variant-disease\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#clinvar)  \n",
    "\n",
    "**Edges:**  \n",
    "- `variant-disease`  \n",
    "- `variant-phenotype`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt)\n",
    "- Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)    \n",
    "\n",
    "This chunk process the [`CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Disease, Phenotype_  \n",
    "  - `Phenotype`: A string containing a disease identifier and prefix. Sources are OMIM, MedGen (UMLS), and Orphanet.  \n",
    "  \n",
    "  _Variant_  \n",
    "  - `VariantName`: A string containing the name of the variant.  \n",
    "  - `rs_id`: An integer that represents a dbSNP identifier.    \n",
    "  - `AlleleID`: An integer that represents an Allele identifier.    \n",
    "  - `RCVaccession`: An integer that represents an RCV accession identifier. \n",
    "  - `Type`: Character, the type of variant represented by the AlleleID.  \n",
    "  - `Assembly`: A list of dictionaries, stored as a string, that contains information related to the assembly (i.e., Assembly, ChromosomeAccession, Chromosome, Start, Stop, ReferenceAlel, AlernateAllel, Cytogenetic, and PositionVCF).   \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `OtherIDs`: A \"|\"-delimited list of other identifiers associated with the variant edge. Note that each identifier included also includes a prefix.    \n",
    "  - `GeneID`: An identifier for the gene associated with each variant (wherever possible).  \n",
    "  - `Guidelines`: Character, ACMG only right now.  \n",
    "  - `TestedInGTR`: Character, Y/N for Yes/No if there is a test registered as specific to this variant in the NIH Genetic Testing Registry (GTR).  \n",
    "  - `LastEvaluated`: Date, the latest date any submitter reported clinical significance.  \n",
    "  - `ReviewStatus`: Character, highest review status for reporting this measure.  \n",
    "  - `ClinicalSignificance`: Character, comma-separated list of aggregate values of clinical significance calculated for this variant.  \n",
    "  - `ClinSigSimple`: Integer,  \n",
    "    0 = no current value of Likely pathogenic or Pathogenic;  \n",
    "    1 = at least one current record submitted with an interpretation of Likely pathogenic or Pathogenic (independent of whether that record includes assertion criteria and evidence).    \n",
    "   -1 = no values for clinical significance at all for this variant or set of variants; used for the \"included\" variants that are only in ClinVar because they are included in a haplotype or genotype with an interpretation.  \n",
    "  - `Origin`: Character, list of all allelic origins for this variant.  \n",
    "  - `OriginSimple`: Character, processed from Origin to make it easier to distinguish between germline and somatic.  \n",
    "  - `SubmitterCategories`: Coded value to indicate whether data were submitted by another resource (1), any other type of source (2), both (3), or none (4).  \n",
    "  - `NumberSubmitters`: Integer, number of submitters describing this variant  \n",
    "  - `Citation`: A \"|\"-delimited list of evidence supporting the variant association. Sources are either PubMed, PubMedCentral, or the NCBI Bookshelf.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt')\n",
    "\n",
    "# load data\n",
    "clv_var_dis = pandas.read_csv(unprocessed_data_location + 'CLINVAR_VARIANT_DISEASE_PHENOTYPE_EDGES.txt', header=0, delimiter='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_var_dis = clv_var_dis.merge(disease_maps, left_on='Phenotype', right_on='Disease_IDs')\n",
    "clv_var_dis = clv_var_dis.merge(phenotype_maps, left_on='Phenotype', right_on='Disease_IDs')\n",
    "\n",
    "# visualize data\n",
    "clv_var_dis.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'variant-disease': {}, 'variant-phenotype': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(clv_var_dis.iterrows(), total=clv_var_dis.shape[0]):\n",
    "    node_key = row['VariationID']; rcv = row['RCVaccession']; var_type = row['Type']\n",
    "    pheno = row['Phenotype']; rs_id = row['RS# (dbSNP)']; allele_id = row['AlleleID']\n",
    "    assembly = row['Assembly']; var_name = row['VariantName']\n",
    "    evidence = [{'ClinVar_GeneID': row['GeneID'],\n",
    "                 'ClinVar_OtherIDs': row['OtherIDs'],\n",
    "                 'ClinVar_Guidelines': row['Guidelines'],\n",
    "                 'ClinVar_TestedInGTR': row['TestedInGTR'],\n",
    "                 'ClinVar_LastEvaluated': row['LastEvaluated'],\n",
    "                 'ClinVar_ReviewStatus': row['ReviewStatus'],\n",
    "                 'ClinVar_ClinicalSignificance': row['ClinicalSignificance'],\n",
    "                 'ClinVar_ClinSigSimple': row['ClinSigSimple'],\n",
    "                 'ClinVar_Origin': row['Origin'],\n",
    "                 'ClinVar_OriginSimple': row['OriginSimple'],\n",
    "                 'ClinVar_SubmitterCategories': row['SubmitterCategories'],\n",
    "                 'ClinVar_NumberSubmitters': row['NumberSubmitters'],\n",
    "                 'ClinVar_Citation': row['Citation']}]   \n",
    "    for idx in [row['MONDO_IDs'].rstrip(), row['HP_IDs'].rstrip()]:\n",
    "        if idx.startswith('MONDO'): edge_key = '{}-{}'.format(node_key, idx); edge_type = 'variant-disease'\n",
    "        else: edge_key = '{}-{}'.format(node_key, idx); edge_type = 'variant-phenotype'\n",
    "    \n",
    "        # add disease/phenotype metadata\n",
    "        if idx in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][idx].keys():\n",
    "                master_metadata_dictionary['nodes'][idx][url]['ClinVar_Phenotype'] |= {pheno}\n",
    "            else: master_metadata_dictionary['nodes'][idx].update({url: {'ClinVar_Phenotype': {pheno}}})\n",
    "        else: master_metadata_dictionary['nodes'].update({idx: {url: {'ClinVar_Phenotype': {pheno}}}})\n",
    "\n",
    "        # add variant information\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_VariantName'] |= {var_name}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_rs_id'] |= {rs_id}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_AlleleID'] |= {allele_id}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_RCVaccession'] |= {rcv}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_Type'] |= {var_type}\n",
    "                master_metadata_dictionary['nodes'][node_key][url]['ClinVar_Assembly'] |= {assembly}\n",
    "            else:\n",
    "                master_metadata_dictionary['nodes'][node_key].update({\n",
    "                    url: {'ClinVar_rs_id': {rs_id},\n",
    "                          'ClinVar_VariantName': {var_name},\n",
    "                          'ClinVar_AlleleID': {allele_id},\n",
    "                          'ClinVar_RCVaccession': {rcv},\n",
    "                          'ClinVar_Type': {var_type},\n",
    "                          'ClinVar_Assembly': {assembly}\n",
    "                         }})\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {\n",
    "                url: {'ClinVar_rs_id': {rs_id},\n",
    "                      'ClinVar_VariantName': {var_name},\n",
    "                      'ClinVar_AlleleID': {allele_id},\n",
    "                      'ClinVar_RCVaccession': {rcv},\n",
    "                      'ClinVar_Type': {var_type},\n",
    "                      'ClinVar_Assembly': {assembly}}}})\n",
    "\n",
    "        # add relation data to dictionary\n",
    "        if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "            if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "                if 'ClinVar_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                    inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                    inital_ev = inital_ev['ClinVar_Evidence'] + evidence\n",
    "                    ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                    master_metadata_dictionary['edges'][edge_key][url]['ClinVar_Evidence'] = ev\n",
    "                else: master_metadata_dictionary['edges'][edge_key][url].update({'ClinVar_Evidence': evidence})\n",
    "            else: master_metadata_dictionary['edges'][edge_key].update({url: {'ClinVar_Evidence': evidence, 'Type': edge_type}})\n",
    "        else: master_metadata_dictionary['edges'].update({edge_key: {url: {'ClinVar_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del clv_var_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `CLINVAR_VARIANT_GENE_EDGES.txt` <a class=\"anchor\" id=\"variant-gene\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [ClinVar](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#clinvar)  \n",
    "\n",
    "**Edges:**  \n",
    "- `variant-gene`     \n",
    "\n",
    "This chunk process the [`CLINVAR_VARIANT_GENE_EDGES.txt`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_GENE_EDGES.txt) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Variant_  \n",
    "  - `VariantName`: A string containing the name of the variant.  \n",
    "  - `rs_id`: An integer that represents a dbSNP identifier.    \n",
    "  - `AlleleID`: An integer that represents an Allele identifier.    \n",
    "  - `RCVaccession`: An integer that represents an RCV accession identifier. \n",
    "  - `Type`: Character, the type of variant represented by the AlleleID.  \n",
    "  - `Assembly`: A list of dictionaries, stored as a string, that contains information related to the assembly (i.e., Assembly, ChromosomeAccession, Chromosome, Start, Stop, ReferenceAlel, AlernateAllel, Cytogenetic, and PositionVCF).  \n",
    "  - `GenesPerAlleleID`: An integer that represents the count of genes that are found in the allele which corresponds to the variant.  \n",
    "  - `Category`: The type of allele-gene relationship. The values for category are:\n",
    "      - Asserted, but not computed: Submitted as related to a gene, but not within the location of that gene on the genome\n",
    "      - Genes overlapped by variant: The gene and variant overlap\n",
    "      - Near gene, downstream: Outside the location of the gene on the genome, within 5 kb\n",
    "      -Near gene, upstream: Outside the location of the gene on the genome, within 5 kb\n",
    "      - Within multiple genes by overlap: The variant is within genes that overlap on the genome. Includes introns\n",
    "      - Within single gene: The variant is in only one gene. Includes introns\n",
    "      \n",
    "  _Gene_    \n",
    "  - `GenomicInformation`: A dictionary of gene identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `OtherIDs`: A \"|\"-delimited list of other identifiers associated with the variant edge. Note that each identifier included also includes a prefix.    \n",
    "  - `Guidelines`: Character, ACMG only right now.  \n",
    "  - `TestedInGTR`: Character, Y/N for Yes/No if there is a test registered as specific to this variant in the NIH Genetic Testing Registry (GTR).  \n",
    "  - `LastEvaluated`: Date, the latest date any submitter reported clinical significance.  \n",
    "  - `ReviewStatus`: Character, highest review status for reporting this measure.  \n",
    "  - `ClinicalSignificance`: Character, comma-separated list of aggregate values of clinical significance calculated for this variant.  \n",
    "  - `ClinSigSimple`: Integer,  \n",
    "    0 = no current value of Likely pathogenic or Pathogenic;  \n",
    "    1 = at least one current record submitted with an interpretation of Likely pathogenic or Pathogenic (independent of whether that record includes assertion criteria and evidence).    \n",
    "   -1 = no values for clinical significance at all for this variant or set of variants; used for the \"included\" variants that are only in ClinVar because they are included in a haplotype or genotype with an interpretation.  \n",
    "  - `Origin`: Character, list of all allelic origins for this variant.  \n",
    "  - `OriginSimple`: Character, processed from Origin to make it easier to distinguish between germline and somatic.  \n",
    "  - `SubmitterCategories`: Coded value to indicate whether data were submitted by another resource (1), any other type of source (2), both (3), or none (4).  \n",
    "  - `NumberSubmitters`: Integer, number of submitters describing this variant  \n",
    "  - `Citation`: A \"|\"-delimited list of evidence supporting the variant association. Sources are either PubMed, PubMedCentral, or the NCBI Bookshelf.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/CLINVAR_VARIANT_GENE_EDGES.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'CLINVAR_VARIANT_GENE_EDGES.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'CLINVAR_VARIANT_GENE_EDGES.txt')\n",
    "\n",
    "# load data\n",
    "clv_var_gene = pandas.read_csv(unprocessed_data_location + 'CLINVAR_VARIANT_GENE_EDGES.txt', header=0, delimiter='\\t', low_memory=False)\n",
    "\n",
    "# visualize data\n",
    "clv_var_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'variant-gene': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(clv_var_gene.iterrows(), total=clv_var_gene.shape[0]):\n",
    "    node_key = row['VariationID']; rcv = row['RCVaccession']; var_type = row['Type']\n",
    "    gene = row['GeneID']; var_name = row['VariantName']\n",
    "    rs_id = row['RS# (dbSNP)']; allele_id = row['AlleleID']\n",
    "    assembly = row['Assembly']; gpa = row['GenesPerAlleleID']; category = row['Category']\n",
    "    evidence = [{'ClinVar_OtherIDs': row['OtherIDs'],\n",
    "                 'ClinVar_Guidelines': row['Guidelines'],\n",
    "                 'ClinVar_TestedInGTR': row['TestedInGTR'],\n",
    "                 'ClinVar_LastEvaluated': row['LastEvaluated'],\n",
    "                 'ClinVar_ReviewStatus': row['ReviewStatus'],\n",
    "                 'ClinVar_ClinicalSignificance': row['ClinicalSignificance'],\n",
    "                 'ClinVar_ClinSigSimple': row['ClinSigSimple'],\n",
    "                 'ClinVar_Origin': row['Origin'],\n",
    "                 'ClinVar_OriginSimple': row['OriginSimple'],\n",
    "                 'ClinVar_SubmitterCategories': row['SubmitterCategories'],\n",
    "                 'ClinVar_NumberSubmitters': row['NumberSubmitters'],\n",
    "                 'ClinVar_Citation': row['Citation']}]   \n",
    "    edge_key = '{}-{}'.format(node_key, gene); edge_type = 'variant-gene'\n",
    "\n",
    "    # add genomic information\n",
    "    if gene in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[gene]\n",
    "    else: genomic_info_dict = None\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][gene].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][gene].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({gene: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({gene: {'genomic_data': 'None'}})\n",
    "    \n",
    "    # add variant information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_VariantName'] |= {var_name}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_rs_id'] |= {rs_id}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_AlleleID'] |= {allele_id}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_RCVaccession'] |= {rcv}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_Type'] |= {var_type}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_Assembly'] |= {assembly}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_GenesPerAlleleID'] |= {gpa}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['ClinVar_Category'] |= {category}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({\n",
    "                url: {'ClinVar_rs_id': {rs_id},\n",
    "                      'ClinVar_VariantName': {var_name},\n",
    "                      'ClinVar_AlleleID': {allele_id},\n",
    "                      'ClinVar_RCVaccession': {rcv},\n",
    "                      'ClinVar_Type': {var_type},\n",
    "                      'ClinVar_Assembly': {assembly},\n",
    "                      'ClinVar_GenesPerAlleleID': {gpa},\n",
    "                      'ClinVar_Category': {category}\n",
    "                     }})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key: {\n",
    "            url: {'ClinVar_rs_id': {rs_id},\n",
    "                  'ClinVar_VariantName': {var_name},\n",
    "                  'ClinVar_AlleleID': {allele_id},\n",
    "                  'ClinVar_RCVaccession': {rcv},\n",
    "                  'ClinVar_Type': {var_type},\n",
    "                  'ClinVar_Assembly': {assembly},\n",
    "                  'ClinVar_GenesPerAlleleID': {gpa},\n",
    "                  'ClinVar_Category': {category}}}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'ClinVar_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['ClinVar_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['ClinVar_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'ClinVar_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'ClinVar_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'ClinVar_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del clv_var_gene        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt` <a class=\"anchor\" id=\"hpa\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:**  \n",
    "- [Genotype-Tissue Expression Project](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#genotype-tissue-expression-project)  \n",
    "- [Human Protein Atlas](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#human-protein-atlas)  \n",
    "\n",
    "**Edges:**  \n",
    "- `protein-anatomy`  \n",
    "- `protein-cell`  \n",
    "- `rna-anatomy`  \n",
    "- `rna-cell`  \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Proteins: [UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_ACCESSION_PRO_ONTOLOGY_MAP.txt)    \n",
    "- Anatomy: [HPA_GTEx_TISSUE_CELL_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEx_TISSUE_CELL_MAP.txt)\n",
    "- Cells: [HPA_GTEx_TISSUE_CELL_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEx_TISSUE_CELL_MAP.txt)  \n",
    "- RNA: [GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/GENE_SYMBOL_ENSEMBL_TRANSCRIPT_MAP.txt)  \n",
    "\n",
    "This chunk process the [`HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Anatomy, Cell_ \n",
    "  - `Anatomy`: A string containing the concept's synonym. If derived from an ontology, the string will be prefixed by the synonym type.    \n",
    "  - `Anatomy_Type`: A string indicating the type of annotation.    \n",
    "  - `Subcellular_Location`: A string containing a subcellular compartment.  \n",
    "  \n",
    "  _Protein, RNA_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Expression_Value`: The expression value derived from the experiments.    \n",
    "  - `Source`: A string indicating the source of the data (i.e., Human Protein Atlas or the Genotype-Tissue Expression project).  \n",
    "  - `Evidence`: A string indicating if the evidence is at the transcript or protein level.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt')\n",
    "\n",
    "# load data\n",
    "hpa_gen_ant = pandas.read_csv(unprocessed_data_location + 'HPA_GTEX_RNA_GENE_PROTEIN_EDGES.txt', header=None, delimiter='\\t', skiprows=0)\n",
    "\n",
    "# filter data\n",
    "hpa_gen_ant = hpa_gen_ant[hpa_gen_ant[3] != 'No human protein/transcript evidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_gen_ant = hpa_gen_ant.merge(uniprot_pro_map, left_on=2, right_on='Uniprot_Accession_IDs')\n",
    "hpa_gen_ant = hpa_gen_ant.merge(anatomy_maps, left_on=6, right_on='anatomy_ids')\n",
    "hpa_gen_ant = hpa_gen_ant.merge(symbol_transcript_map, left_on=1, right_on='Gene_Symbols')\n",
    "\n",
    "# visualize data\n",
    "hpa_gen_ant.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-anatomy': {}, 'protein-cell': {}, 'rna-anatomy': {}, 'rna-cell': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(hpa_gen_ant.iterrows(), total=hpa_gen_ant.shape[0]):\n",
    "    node_key = row['ontolgoy_ids']; anatomy = row[6]; anatomy_type = row[4]; subcell = row[5]\n",
    "    evidence = [{'HPA_GTEx_Expression_Value': row[7], 'HPA_GTEx_Source': row[8], 'HPA_GTEx_Evidence': row[3]}]   \n",
    "    protein = row['Protein_Ontology_IDs'].rstrip(); rna = row['Ensembl_Transcript_IDs']\n",
    "    if row[3] == 'Evidence at protein level' and row[4] == 'anatomy':\n",
    "        node_key2 = protein; edge_key = '{}-{}'.format(node_key2, node_key); edge_type = 'protein-anatomy'\n",
    "    elif row[3] == 'Evidence at protein level' and row[4] != 'anatomy':\n",
    "        node_key2 = protein; edge_key = '{}-{}'.format(node_key2, node_key); edge_type = 'protein-cell'\n",
    "    elif row[3] == 'Evidence at transcript level' and row[4] == 'anatomy':\n",
    "        node_key2 = rna; edge_key = '{}-{}'.format(node_key2, node_key); edge_type = 'rna-anatomy'\n",
    "    elif row[3] == 'Evidence at transcript level' and row[4] != 'anatomy':\n",
    "        node_key2 = rna; edge_key = '{}-{}'.format(node_key2, node_key); edge_type = 'protein-cell'\n",
    "    else: pass\n",
    "    \n",
    "    # add anatomical information\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['HPA_GTEx_Anatomy'] |= {anatomy}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['HPA_GTEx_Anatomy_Type'] |= {anatomy_type}\n",
    "            master_metadata_dictionary['nodes'][node_key][url]['HPA_GTEx_Subcellular_Location'] |= {subcell}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({url: {\n",
    "                'HPA_GTEx_Anatomy': {anatomy},\n",
    "                'HPA_GTEx_Anatomy_Type': {anatomy_type},\n",
    "                'HPA_GTEx_Subcellular_Location': {subcell}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key: {url: {\n",
    "            'HPA_GTEx_Anatomy': {anatomy},\n",
    "            'HPA_GTEx_Anatomy_Type': {anatomy_type},\n",
    "            'HPA_GTEx_Subcellular_Location': {subcell}}}})\n",
    "\n",
    "    # add genomic information\n",
    "    if node_key2 in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key2]\n",
    "    if node_key2 in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][node_key2].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][node_key2].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({node_key2: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key2: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'HPA_GTEx_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['HPA_GTEx_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['HPA_GTEx_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'HPA_GTEx_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'HPA_GTEx_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'HPA_GTEx__Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del hpa_gen_ant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `UNIPROT_PROTEIN_CATALYST.txt` <a class=\"anchor\" id=\"uniprot-catalyst\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Universal Protein Resource Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#universal-protein-resource-knowledgebase)  \n",
    "\n",
    "**Edges:**  \n",
    "- `protein-catalyst`  \n",
    "\n",
    "This chunk process the [`UNIPROT_PROTEIN_CATALYST.txt`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_CATALYST.txt) file and obtains the following node and edge metadata:  \n",
    " \n",
    "- **Nodes:**  \n",
    "  _Protein_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Status`: A string to indicate the status of the entry in Uniprot.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_CATALYST.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'UNIPROT_PROTEIN_CATALYST.txt')\n",
    "\n",
    "# load data\n",
    "upt_prot_cat = pandas.read_csv(unprocessed_data_location + 'UNIPROT_PROTEIN_CATALYST.txt', header=None, delimiter='\\t', skiprows=0)\n",
    "\n",
    "# visualize data\n",
    "upt_prot_cat.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-catalyst': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(upt_prot_cat.iterrows(), total=upt_prot_cat.shape[0]):\n",
    "    node_key = row[0]; chebi = row[1]; evidence = [{'Uniprot_Status': row[2]}]   \n",
    "    edge_key = '{}-{}'.format(node_key, chebi); edge_type = 'protein-catalyst'\n",
    "    \n",
    "    # add catalyst information\n",
    "    if chebi in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][chebi].keys():\n",
    "            master_metadata_dictionary['nodes'][chebi][url]['Uniprot_CHEBI'] |= {chebi}\n",
    "        else: master_metadata_dictionary['nodes'][chebi].update({url: {'Uniprot_CHEBI': {chebi}}})\n",
    "    else: master_metadata_dictionary['nodes'].update({chebi: {url: {'Uniprot_CHEBI': {chebi}}}})\n",
    "    \n",
    "    # add genomic information\n",
    "    if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'Uniprot_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['Uniprot_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['Uniprot_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'Uniprot_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'Uniprot_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'Uniprot_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del upt_prot_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `UNIPROT_PROTEIN_COFACTOR.txt` <a class=\"anchor\" id=\"uniprot-cofactor\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Universal Protein Resource Knowledgebase](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#universal-protein-resource-knowledgebase)  \n",
    "\n",
    "**Edges:**  \n",
    "- `protein-cofactor`  \n",
    "\n",
    "This chunk process the [`UNIPROT_PROTEIN_COFACTOR.txt`](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_COFACTOR.txt) file and obtains the following node and edge metadata:  \n",
    " \n",
    "- **Nodes:**  \n",
    "  _Protein_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `Status`: A string to indicate the status of the entry in Uniprot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/UNIPROT_PROTEIN_COFACTOR.txt'\n",
    "if not os.path.exists(unprocessed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, 'UNIPROT_PROTEIN_COFACTOR.txt')\n",
    "\n",
    "# load data\n",
    "upt_prot_cof = pandas.read_csv(unprocessed_data_location + 'UNIPROT_PROTEIN_COFACTOR.txt', header=None, delimiter='\\t', skiprows=0)\n",
    "\n",
    "# visualize data\n",
    "upt_prot_cof.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-cofactor': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(upt_prot_cof.iterrows(), total=upt_prot_cof.shape[0]):\n",
    "    node_key = row[0]; chebi = row[1]; evidence = [{'Uniprot_Status': row[2]}]   \n",
    "    edge_key = '{}-{}'.format(node_key, chebi); edge_type = 'protein-cofactor'\n",
    "    \n",
    "    # add catalyst information\n",
    "    if chebi in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][chebi].keys():\n",
    "            master_metadata_dictionary['nodes'][chebi][url]['Uniprot_CHEBI'] |= {chebi}\n",
    "        else: master_metadata_dictionary['nodes'][chebi].update({url: {'Uniprot_CHEBI': {chebi}}})\n",
    "    else: master_metadata_dictionary['nodes'].update({chebi: {url: {'Uniprot_CHEBI': {chebi}}}})\n",
    "    \n",
    "    # add genomic information\n",
    "    if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'Uniprot_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['Uniprot_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['Uniprot_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'Uniprot_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'Uniprot_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'Uniprot_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del upt_prot_cof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `9606.protein.links.v11.0.txt.gz` <a class=\"anchor\" id=\"protein-protein\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [Search Tool for Recurring Instances of Neighbouring Genes Database](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#search-tool-for-recurring-instances-of-neighbouring-genes-database) \n",
    "\n",
    "**Edges:**  \n",
    "- `protein-protein`   \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Proteins: [STRING_PRO_ONTOLOGY_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/STRING_PRO_ONTOLOGY_MAP.txt)  \n",
    "\n",
    "This chunk process the [`9606.protein.links.v11.0.txt.gz`](https://stringdb-static.org/download/protein.links.v11.0/9606.protein.links.v11.0.txt.gz) file and obtains the following node and edge metadata:  \n",
    " \n",
    "- **Nodes:**  \n",
    "  _Protein_    \n",
    "  - `GenomicInformation`: A dictionary of protein identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `combined_score`: The combined score is computed by combining the probabilities from the different evidence channels and corrected for the probability of randomly observing an interaction. Scores range from 0-1000. For a more detailed description please see PMID:15608232.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://stringdb-static.org/download/protein.links.v11.0/9606.protein.links.v11.0.txt.gz'\n",
    "if not os.path.exists(unprocessed_data_location + '9606.protein.links.v11.0.txt'):\n",
    "    data_downloader(url, unprocessed_data_location, '9606.protein.links.v11.0.txt')\n",
    "\n",
    "# load data\n",
    "stg_prot_prot = pandas.read_csv(unprocessed_data_location + '9606.protein.links.v11.0.txt', header=0, delimiter=' ', skiprows=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_prot_prot = stg_prot_prot.merge(string_pro_map, left_on='protein1', right_on='STRING_IDs')\n",
    "stg_prot_prot = stg_prot_prot.merge(string_pro_map, left_on='protein2', right_on='STRING_IDs')\n",
    "\n",
    "# visualize data\n",
    "stg_prot_prot.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'protein-protein': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(stg_prot_prot.iterrows(), total=stg_prot_prot.shape[0]):\n",
    "    proteins = [row['Protein_Ontology_IDs_x'], row['Protein_Ontology_IDs_y']]; score = row['combined_score']; gene_info = []\n",
    "    edge_key = '{}-{}'.format(row['Protein_Ontology_IDs_x'], row['Protein_Ontology_IDs_y']); edge_type = 'protein-protein' \n",
    "    \n",
    "    for node_key in proteins:\n",
    "        if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "        else: genomic_info_dict = None\n",
    "        \n",
    "        # add genomic information\n",
    "        if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "            else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "        else:\n",
    "            if genomic_info_dict is not None:\n",
    "                master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "            else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}}) \n",
    "    \n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'String_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['String_Evidence'] = score\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'String_Evidence': score})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'String_Evidence': score, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'String_Evidence': score, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del stg_prot_prot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### `curated_gene_disease_associations.tsv` <a class=\"anchor\" id=\"gene-phen\"></a>  \n",
    "\n",
    "**Data Source Wiki Page:** [DisGeNET](https://github.com/callahantiff/PheKnowLator/wiki/v4-Data-Sources#disgenet)\n",
    "\n",
    "**Edges:**  \n",
    "- `gene-disease`   \n",
    "- `gene-phenotype`    \n",
    "\n",
    "**Identifier Maps:**    \n",
    "- Diseases: [DISEASE_MONDO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/DISEASE_MONDO_MAP.txt)\n",
    "- Phenotypes: [PHENOTYPE_HPO_MAP.txt](https://storage.googleapis.com/pheknowlator/current_build/data/processed_data/PHENOTYPE_HPO_MAP.txt)   \n",
    "\n",
    "This chunk process the [curated_gene_disease_associations.tsv](https://www.disgenet.org/static/disgenet_ap1/files/downloads/curated_gene_disease_associations.tsv.gz) file and obtains the following node and edge metadata:  \n",
    "- **Nodes:**  \n",
    "  _Disease, Phenotype_  \n",
    "  - `diseaseId`: A string containing the concept's database cross-reference, which is formatted as DB:ID. If not, a MeSH or OMIM identifier. Variable is provided as a string with the \"MESH\" or \"OMIM\" prefix in all caps.    \n",
    "  - `diseaseName`: A string containing the concept's synonym. If derived from an ontology, the string will be prefixed by the synonym type.    \n",
    "  - `diseaseSematicType`: A string containing a high-level grouper or typing variable for the disease.    \n",
    "  - `diseaseClass`: A \";\"-delimnited list of ICD codes that can be used to classify the disease.  \n",
    " \n",
    "  _Gene_    \n",
    "  - `GenomicInformation`: A dictionary of gene identifier information. See the [Genomic Entity Metadata](#genomicinfo) code chunk for more details. \n",
    "\n",
    "\n",
    "- **Edges:**  \n",
    "  - `DSI`: The Disease Similarity Index ranges from from 0.25 to 1. It is calculated as: DSI = log2(# diseases assoc with gene/total # of diseases in DisGeNET) / log2(1/total # of diseases in DisGeNET)  \n",
    "  - `DPI`: The Disease Pleiotropy Index ranges from 0 to 1. it is calculated as: DPI = (# of MeSH disease classes of disease assoc with gene/total # of MeSH disease classes)*100.   \n",
    "  - `score`: The score range from 0 to 1, and take into account the number and type of sources (level of curation, model organisms), and the number of publications supporting the association.  \n",
    "  - `EI`: The Evidence Index (EL) is a metric developed by ClinGen that measures the strength of evidence of a gene-disease relationship that correlates to a qualitative classification: \"Definitive\", \"Strong\", \"Moderate\", \"Limited\", \"Disputed\" ([PMID:28552198](https://www.ncbi.nlm.nih.gov/pubmed/28552198)). EI=1 indicates that all the publications support the GDA or the VDA, while EI<1 indicates that there are publications that assert that there is no association between the gene/variants and the disease. If the gene/variant has no EI value, it indicates that the index has not been computed for this association. It is calculated as: EI = (# positive pubs/total # of pubs).  \n",
    "  - `YearInitial`: First time that the association was reported.  \n",
    "  - `YearFinal`: Last time that the association was reported.  \n",
    "  - `NofPmids`: Count of associated Pubmed IDs.  \n",
    "  - `NofSnps`: Count of associated SNPs.  \n",
    "  - `Source`: The original source reporting the Gene-Disease Association.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://www.disgenet.org/static/disgenet_ap1/files/downloads/curated_gene_disease_associations.tsv.gz'\n",
    "if not os.path.exists(unprocessed_data_location + 'curated_gene_disease_associations.tsv'):\n",
    "    data_downloader(url, unprocessed_data_location, 'curated_gene_disease_associations.tsv')\n",
    "\n",
    "# load data\n",
    "dgt_dis_gene = pandas.read_csv(unprocessed_data_location + 'curated_gene_disease_associations.tsv', header=0, delimiter='\\t', skiprows=0)\n",
    "dgt_dis_gene = dgt_dis_gene[dgt_dis_gene['diseaseType'] != 'group']\n",
    "\n",
    "# fix variable typing\n",
    "dgt_dis_gene['YearInitial'] = dgt_dis_gene['YearInitial'].astype('float').astype('Int64')\n",
    "dgt_dis_gene['YearFinal'] = dgt_dis_gene['YearFinal'].astype('float').astype('Int64')\n",
    "\n",
    "# fix prefix\n",
    "dgt_dis_gene['geneId'] = 'NCBIGene_' + dgt_dis_gene['geneId'].astype('str')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge Identifier Maps*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgt_dis_gene = dgt_dis_gene.merge(disease_maps, left_on='diseaseId', right_on='Disease_IDs')\n",
    "dgt_dis_gene = dgt_dis_gene.merge(phenotype_maps, left_on='diseaseId', right_on='Disease_IDs')\n",
    "\n",
    "# visualize data\n",
    "dgt_dis_gene.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create Metadata Dictionary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_metadata_dictionary['edges'].update({'gene-disease': {}, 'gene-phenotype': {}})\n",
    "\n",
    "# create dictionary\n",
    "for idx, row in tqdm(dgt_dis_gene.iterrows(), total=dgt_dis_gene.shape[0]):\n",
    "    node_key = row['geneId']; dis_id = row['diseaseId']; dis_name = row['diseaseName']\n",
    "    sem_type = row['diseaseSemanticType']; dis_cls = row['diseaseClass']\n",
    "    evidence = [{'DisGeNET_DSI': row['DSI'] if not pandas.isna(row['DSI']) else 'None',\n",
    "                 'DisGeNET_DPI': row['DPI'] if not pandas.isna(row['DPI']) else 'None',\n",
    "                 'DisGeNET_score': row['score'] if not pandas.isna(row['score']) else 'None',\n",
    "                 'DisGeNET_EI': row['EI'] if not pandas.isna(row['EI']) else 'None',\n",
    "                 'DisGeNET_YearInitial': row['YearInitial'] if not pandas.isna(row['YearInitial']) else 'None',\n",
    "                 'DisGeNET_YearFinal': row['YearFinal'] if not pandas.isna(row['YearFinal']) else 'None',\n",
    "                 'DisGeNET_NofPmids': row['NofPmids'],\n",
    "                 'DisGeNET_NofSnps': row['NofSnps']}]   \n",
    "    if row['diseaseType'] == 'disease':\n",
    "        node_key2 = row['MONDO_IDs']; edge_key = '{}-{}'.format(node_key, node_key2); edge_type = 'gene-disease'\n",
    "    else:\n",
    "        node_key2 = row['HP_IDs']; edge_key = '{}-{}'.format(node_key, node_key2); edge_type = 'gene-phenotype'\n",
    "    \n",
    "    # add disease/phenotype information\n",
    "    if node_key2 in master_metadata_dictionary['nodes'].keys():\n",
    "        if url in master_metadata_dictionary['nodes'][node_key2].keys():\n",
    "            master_metadata_dictionary['nodes'][node_key2][url]['DisGeNET_diseaseId'] |= {dis_id}\n",
    "            master_metadata_dictionary['nodes'][node_key2][url]['DisGeNET_diseaseName'] |= {dis_name}\n",
    "            master_metadata_dictionary['nodes'][node_key2][url]['DisGeNET_diseaseSemanticType'] |= {sem_type}\n",
    "            master_metadata_dictionary['nodes'][node_key2][url]['DisGeNET_diseaseClass'] |= {dis_cls}\n",
    "        else:\n",
    "            master_metadata_dictionary['nodes'][node_key2].update({url: {\n",
    "                'DisGeNET_diseaseId': {dis_id},\n",
    "                'DisGeNET_diseaseName': {dis_name},\n",
    "                'DisGeNET_diseaseSemanticType': {sem_type},\n",
    "                'DisGeNET_diseaseClass': {dis_cls}}})\n",
    "    else:\n",
    "        master_metadata_dictionary['nodes'].update({node_key2: {url: {\n",
    "            'DisGeNET_diseaseId': {dis_id},\n",
    "            'DisGeNET_diseaseName': {dis_name},\n",
    "            'DisGeNET_diseaseSemanticType': {sem_type},\n",
    "            'DisGeNET_diseaseClass': {dis_cls}}}})\n",
    "\n",
    "    # add genomic information\n",
    "    if node_key in genomic_metadata.keys(): genomic_info_dict = genomic_metadata[node_key]\n",
    "    if node_key in master_metadata_dictionary['nodes'].keys():\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'][node_key].update({'genomic_data': genomic_info_dict})\n",
    "        else: master_metadata_dictionary['nodes'][node_key].update({'genomic_data': 'None'})\n",
    "    else:\n",
    "        if genomic_info_dict is not None:\n",
    "            master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': genomic_info_dict}})\n",
    "        else: master_metadata_dictionary['nodes'].update({node_key: {'genomic_data': 'None'}})\n",
    "\n",
    "    # add relation data to dictionary\n",
    "    if edge_key in master_metadata_dictionary['edges'].keys():\n",
    "        if url in master_metadata_dictionary['edges'][edge_key].keys():\n",
    "            if 'DisGeNET_Evidence' in master_metadata_dictionary['edges'][edge_key][url].keys():\n",
    "                inital_ev = master_metadata_dictionary['edges'][edge_key][url]\n",
    "                inital_ev = inital_ev['DisGeNET_Evidence'] + evidence\n",
    "                ev = [json.loads(i) for i in set(json.dumps(item, sort_keys=True) for item in inital_ev)]\n",
    "                master_metadata_dictionary['edges'][edge_key][url]['DisGeNET_Evidence'] = ev\n",
    "            else: master_metadata_dictionary['edges'][edge_key][url].update({'DisGeNET_Evidence': evidence})\n",
    "        else: master_metadata_dictionary['edges'][edge_key].update({url: {'DisGeNET_Evidence': evidence, 'Type': edge_type}})\n",
    "    else: master_metadata_dictionary['edges'].update({edge_key: {url: {'DisGeNET_Evidence': evidence, 'Type': edge_type}}})\n",
    "\n",
    "# delete unneeded data\n",
    "del dgt_dis_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***\n",
    "\n",
    "#### Save Metadata Dictionary\n",
    "Write the metadata dictionary to a file named `entity_metadata_dict.pkl` and located in the `resources/metadata/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Node Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create list of dictionaries\n",
    "# print('Creating Node List ...')\n",
    "# node_list = [{k: v} for k, v in tqdm(master_metadata_dictionary['nodes'].items())]\n",
    "# master_metadata_dictionary['nodes'] = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_temp = {}\n",
    "for k, v in tqdm(master_metadata_dictionary['nodes'].items()):\n",
    "    file_loc = metadata_location + 'temp/nodes/' + k  + '.json'\n",
    "    # write data to temp directory\n",
    "    dump_jsonl([v[k]], file_loc)\n",
    "    # add dictionary entry with file path\n",
    "    node_temp[k] = file_loc\n",
    "    # delete entry\n",
    "    del master_metadata_dictionary['nodes'][k]\n",
    "\n",
    "# update nodes entry\n",
    "master_metadata_dictionary['nodes'] = node_temp\n",
    "    \n",
    "# remove unneeded data\n",
    "del node_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Relation Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nCreating Relations List ...')\n",
    "# relation_list = [{k: v} for k, v in tqdm(master_metadata_dictionary['relations'].items())]\n",
    "# master_metadata_dictionary['relations'] = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_temp = {}\n",
    "for k, v in tqdm(master_metadata_dictionary['relations'].items()):\n",
    "    file_loc = metadata_location + 'temp/relations/' + k  + '.json'\n",
    "    # write data to temp directory\n",
    "    dump_jsonl([v[k]], file_loc)\n",
    "    # add dictionary entry with file path\n",
    "    relations_temp[k] = file_loc\n",
    "    # delete entry\n",
    "    del master_metadata_dictionary['relations'][k]\n",
    "\n",
    "# update nodes entry\n",
    "master_metadata_dictionary['relations'] = relations_temp\n",
    "    \n",
    "# remove unneeded data\n",
    "del relation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Edge Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('\\nCreating Edge List ...')\n",
    "# # edge_list = [{k: v} for k, v in tqdm(master_metadata_dictionary['edges'].items())]\n",
    "# master_metadata_dictionary['edges'] = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_temp = {}\n",
    "for k, v in tqdm(master_metadata_dictionary['edges'].items()):\n",
    "    file_loc = metadata_location + 'temp/edges/' + k  + '.json'\n",
    "    # write data to temp directory\n",
    "    dump_jsonl([v[k]], file_loc)\n",
    "    # add dictionary entry with file path\n",
    "    edges_temp[k] = file_loc\n",
    "    # delete entry\n",
    "    del master_metadata_dictionary['edges'][k]\n",
    "    \n",
    "\n",
    "# update nodes entry\n",
    "master_metadata_dictionary['edges'] = edges_temp\n",
    "    \n",
    "# remove unneeded data\n",
    "del edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save Updated Metadata Dictionary to Metadata Location*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of the dictionary\n",
    "# output > 4GB requires special approach: https://stackoverflow.com/questions/42653386/does-pickle-randomly-fail-with-oserror-on-large-files\n",
    "filepath = metadata_location + 'entity_metadata_dict.pkl'\n",
    "\n",
    "# defensive way to write pickle.write, allowing for very large files on all platforms\n",
    "max_bytes, bytes_out = 2**31 - 1, pickle.dumps(master_metadata_dictionary)\n",
    "n_bytes = sys.getsizeof(bytes_out)\n",
    "\n",
    "with open(filepath, 'wb') as f_out:\n",
    "    for idx in range(0, n_bytes, max_bytes):\n",
    "        f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "```\n",
    "@misc{callahan_tj_2019_3401437,\n",
    "  author       = {Callahan, TJ},\n",
    "  title        = {PheKnowLator},\n",
    "  month        = mar,\n",
    "  year         = 2019,\n",
    "  doi          = {10.5281/zenodo.3401437},\n",
    "  url          = {https://doi.org/10.5281/zenodo.3401437}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
